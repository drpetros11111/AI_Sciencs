{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drpetros11111/AI_Sciencs/blob/CNN/CNN_convolution_and_pooling_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKTL1cpjkuDx"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LS2jsgSkuD_"
      },
      "outputs": [],
      "source": [
        "def f_padd(I,p):\n",
        "    numRows = I.shape[0]\n",
        "    numCols = I.shape[1]\n",
        "    zeroRows = np.zeros((p,numCols))\n",
        "    I = np.vstack((zeroRows,I))\n",
        "    I = np.vstack((I,zeroRows))\n",
        "    zeroCols = np.zeros((numRows+2*p,p))\n",
        "    I = np.hstack((zeroCols,I))\n",
        "    I = np.hstack((I,zeroCols))\n",
        "    return I"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Add Padding Function\n",
        "This function, f_padd, adds padding to an image I with a specified padding width p.\n",
        "\n",
        "Padding is often used in image processing to maintain the dimensions of an image after applying operations like convolution.\n",
        "\n",
        "Let's break down the function step by step:\n",
        "\n",
        "\n",
        "---\n",
        "## Function Breakdown\n",
        "Function Definition and Input Parameters\n",
        "\n",
        "    def f_padd(I, p):\n",
        "\n",
        "##I\n",
        "\n",
        "The input image (a 2D array).\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "##p\n",
        "\n",
        "The padding width (number of rows/columns of zeros to add around the image).\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "##Get the Dimensions of the Input Image\n",
        "\n",
        "    numRows = I.shape[0]\n",
        "    numCols = I.shape[1]\n",
        "\n",
        "##numRows\n",
        "\n",
        "Number of rows in the input image.\n",
        "\n",
        "##numCols\n",
        "\n",
        "Number of columns in the input image.\n",
        "Create Rows of Zeros for Padding\n",
        "\n",
        "    zeroRows = np.zeros((p, numCols))\n",
        "\n",
        "---\n",
        "##zeroRows\n",
        "\n",
        "A 2D array of zeros with p rows and the same number of columns as the input image.\n",
        "\n",
        "This will be used to pad the top and bottom of the image.\n",
        "\n",
        "Add Zero Rows to the Top and Bottom of the Image\n",
        "\n",
        "    I = np.vstack((zeroRows, I))\n",
        "    I = np.vstack((I, zeroRows))\n",
        "\n",
        "---\n",
        "##np.vstack\n",
        "\n",
        "Stacks arrays vertically (row-wise).\n",
        "\n",
        "The first np.vstack adds zeroRows to the top of the image.\n",
        "\n",
        "The second np.vstack adds zeroRows to the bottom of the image.\n",
        "\n",
        "----\n",
        "##Create Columns of Zeros for Padding\n",
        "\n",
        "    zeroCols = np.zeros((numRows + 2 * p, p))\n",
        "\n",
        "\n",
        "##Shape of zeroCols\n",
        "The shape of the zeroCols array is determined by the tuple (numRows + 2 * p, p).\n",
        "\n",
        "    numRows + 2 * p\n",
        "\n",
        "numRows is the number of rows in the original image.\n",
        "\n",
        "    2 * p\n",
        "\n",
        "accounts for the padding added to the top and bottom of the image.\n",
        "\n",
        "Therefore, numRows + 2 * p is the total number of rows in the image after adding the top and bottom padding.\n",
        "\n",
        "    p\n",
        "\n",
        "##p\n",
        "\n",
        "is the number of columns of zeros to be added on each side of the image.\n",
        "\n",
        "Therefore, p is the width of the padding columns to be added to the left and right sides.\n",
        "\n",
        "##Creating the Array of Zeros\n",
        "\n",
        "    zeroCols = np.zeros((numRows + 2 * p, p))\n",
        "\n",
        "np.zeros creates an array filled with zeros.\n",
        "\n",
        "The shape of this array is\n",
        "\n",
        "    (numRows + 2 * p, p)\n",
        "\n",
        "meaning it has numRows + 2 * p rows and p columns.\n",
        "\n",
        "##Visual Example\n",
        "Suppose the original image I has dimensions 3 x 3 (i.e., numRows = 3 and numCols = 3), and we want to add a padding width p = 1.\n",
        "\n",
        "The number of rows in zeroCols will be 3 + 2 * 1 = 5.\n",
        "\n",
        "The number of columns in zeroCols will be 1.\n",
        "\n",
        "So, zeroCols will look like this:\n",
        "\n",
        "    zeroCols = [[0],\n",
        "               [0],\n",
        "               [0],\n",
        "               [0],\n",
        "               [0]]\n",
        "\n",
        "##Applying zeroCols to the Image\n",
        "After creating zeroCols, it will be added to the left and right sides of the padded image (which already has top and bottom padding):\n",
        "\n",
        "    I = np.hstack((zeroCols, I))  # Adds `zeroCols` to the left side\n",
        "    I = np.hstack((I, zeroCols))  # Adds `zeroCols` to the right side\n",
        "\n",
        "##Here's how it works step-by-step\n",
        "\n",
        "Original Image with Top and Bottom\n",
        "##Padding:\n",
        "\n",
        "    [[0, 0, 0],\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 6],\n",
        "    [7, 8, 9],\n",
        "    [0, 0, 0]]\n",
        "\n",
        "##After Adding Left Padding:\n",
        "\n",
        "    [[0, 0, 0, 0],\n",
        "    [0, 1, 2, 3],\n",
        "    [0, 4, 5, 6],\n",
        "    [0, 7, 8, 9],\n",
        "    [0, 0, 0, 0]]\n",
        "\n",
        "##After Adding Right Padding:\n",
        "\n",
        "    [[0, 0, 0, 0, 0],\n",
        "    [0, 1, 2, 3, 0],\n",
        "    [0, 4, 5, 6, 0],\n",
        "    [0, 7, 8, 9, 0],\n",
        "    [0, 0, 0, 0, 0]]\n",
        "\n",
        "##Summary\n",
        "\n",
        "The line\n",
        "\n",
        "    zeroCols = np.zeros((numRows + 2 * p, p))\n",
        "\n",
        "creates a vertical strip of zeros that matches the height of the image after top and bottom padding and has a width of p columns.\n",
        "\n",
        "This allows us to pad the left and right sides of the image, completing the symmetrical padding on all four sides.\n",
        "\n",
        "-----\n",
        "##zeroCols\n",
        "\n",
        "A 2D array of zeros with numRows + 2 * p rows (the height of the image after adding the zero rows) and p columns. This will be used to pad the left and right of the image.\n",
        "\n",
        "Add Zero Columns to the Left and Right of the Image\n",
        "\n",
        "\n",
        "    I = np.hstack((zeroCols, I))\n",
        "    I = np.hstack((I, zeroCols))\n",
        "\n",
        "----\n",
        "##np.hstack: Stacks arrays horizontally (column-wise)\n",
        "\n",
        "The first\n",
        "\n",
        "    np.hstack #adds zeroCols to the left of the image.\n",
        "\n",
        "The second\n",
        "\n",
        "    np.hstack #adds zeroCols to the right of the image.\n",
        "\n",
        "----\n",
        "###Return the Padded Image\n",
        "\n",
        "    return I\n",
        "\n",
        "-----\n",
        "##Summary\n",
        "\n",
        "The function f_padd effectively adds a border of zeros around the input image I with a width specified by p.\n",
        "\n",
        "The result is an image with additional rows and columns of zeros, which can be useful for various image processing tasks. Here's an example to illustrate:\n",
        "\n",
        "---\n",
        "##Example\n",
        "\n",
        "Given an input image I:\n",
        "\n",
        "    I = [[1, 2, 3],\n",
        "        [4, 5, 6],\n",
        "        [7, 8, 9]]\n",
        "\n",
        "If p = 1, the function adds a border of zeros around this image:\n",
        "\n",
        "    I = [[0, 0, 0, 0, 0],\n",
        "        [0, 1, 2, 3, 0],\n",
        "        [0, 4, 5, 6, 0],\n",
        "        [0, 7, 8, 9, 0],\n",
        "        [0, 0, 0, 0, 0]]\n",
        "\n",
        "This padded image has a border of zeros of width p added to the top, bottom, left, and right of the original image."
      ],
      "metadata": {
        "id": "pKctwlNuKBb6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhF8WB20kuED"
      },
      "outputs": [],
      "source": [
        "def f_conv2d(I,K,p):\n",
        "    fSize = K.shape[0]\n",
        "    I2 = f_padd(I,p)\n",
        "    numRows = I2.shape[0]\n",
        "    numCols = I2.shape[1]\n",
        "\n",
        "    C = np.zeros((numRows-2*p,numCols-2*p))\n",
        "\n",
        "    for i in range(numRows-fSize+1):\n",
        "        for j in range(numCols-fSize+1):\n",
        "            A = I2[i:i+fSize,j:j+fSize]\n",
        "            C[i,j] = (A.flatten()*K.flatten()).sum()\n",
        "    return C\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolution Function\n",
        "This snippet defines a function f_conv2d that performs a 2D convolution operation on an input image I using a kernel K with padding p.\n",
        "\n",
        "The function outputs the result of the convolution.\n",
        "\n",
        "Here's a detailed explanation of each part of the code:\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "## Function Definition\n",
        "\n",
        "    def f_conv2d(I, K, p):\n",
        "\n",
        "This line defines the function f_conv2d which takes three parameters:\n",
        "\n",
        "###I\n",
        "\n",
        "the input image (a 2D numpy array).\n",
        "\n",
        "###K\n",
        "\n",
        "the kernel (a smaller 2D numpy array).\n",
        "\n",
        "###p\n",
        "\n",
        "the padding size (an integer).\n",
        "\n",
        "----\n",
        "## Filter Size and Padded Image\n",
        "\n",
        "    fSize = K.shape[0]\n",
        "    I2 = f_padd(I, p)\n",
        "\n",
        "###fSize = K.shape[0]\n",
        "\n",
        "This line gets the size of the kernel (assuming it's a square matrix) by accessing the number of rows (or columns, since it's square).\n",
        "\n",
        "###I2 = f_padd(I, p)\n",
        "\n",
        "This line calls the f_padd function (assumed to be defined elsewhere) to pad the input image I with p rows and columns of zeros around the border, resulting in a padded image I2.\n",
        "\n",
        "----\n",
        "###Shape of the Padded Image\n",
        "\n",
        "    numRows = I2.shape[0]\n",
        "    numCols = I2.shape[1]\n",
        "\n",
        "numRows and numCols store the dimensions of the padded image I2.\n",
        "\n",
        "----\n",
        "##Initialize Output Matrix\n",
        "\n",
        "    C = np.zeros((numRows-2*p, numCols-2*p))\n",
        "\n",
        "###Explanation\n",
        "\n",
        "    C = np.zeros((numRows-2*p, numCols-2*p))\n",
        "\n",
        "\n",
        "###C = np.zeros((numRows-2*p, numCols-2*p))\n",
        "\n",
        "The line initializes the output matrix C, which will store the results of the convolution operation.\n",
        "\n",
        "####Shape Calculation\n",
        "\n",
        "numRows and numCols are the dimensions of the padded image I2.\n",
        "\n",
        "2*p accounts for the padding added to both sides (top and bottom for rows, left and right for columns).\n",
        "\n",
        "    numRows - 2*p and numCols - 2*p\n",
        "    \n",
        "give the dimensions of the original image I before padding.\n",
        "\n",
        "##Why Subtract 2*p?\n",
        "When you pad an image with p rows/columns of zeros, you add p zeros to each side:\n",
        "\n",
        "The total number of additional rows is\n",
        "\n",
        "    2*p (i.e., p rows at the top + p rows at the bottom).\n",
        "\n",
        "The total number of additional columns is 2*p (i.e., p columns on the left + p columns on the right).\n",
        "\n",
        "\n",
        "###Ensuring the Correct Output Size\n",
        "The original image I has dimensions (originalNumRows, originalNumCols).\n",
        "\n",
        "\n",
        "After padding, the padded image I2 has dimensions (originalNumRows + 2*p, originalNumCols + 2*p).\n",
        "\n",
        "\n",
        "During convolution, the valid output size should match the original image size because the padding ensures the kernel fits within the image boundaries, producing an output of the same dimensions as the input.\n",
        "\n",
        "Thus, the output matrix C is initialized to have the same dimensions as the original image, which is   \n",
        "\n",
        "    (numRows - 2*p, numCols - 2*p).\n",
        "\n",
        "##Visual Example\n",
        "Let's say your original image I is 3x3, and you pad it with p = 1.\n",
        "\n",
        "The padded image I2 will be 5x5 (3 + 2*1).\n",
        "\n",
        "To store the convolution results, you need a matrix C of the same size as the original image (3x3), hence C = np.zeros((5-2*1, 5-2*1)) becomes C = np.zeros((3, 3)).\n",
        "\n",
        "The line\n",
        "\n",
        "    C = np.zeros((numRows-2*p, numCols-2*p))\n",
        "\n",
        "ensures that the output matrix C has the correct dimensions to store the results of the convolution operation, excluding the padded borders.\n",
        "\n",
        "This way, C will have the same dimensions as the original input image I.\n",
        "###C\n",
        "\n",
        "the output matrix (or convolved image) is initialized with zeros. Its size is reduced by 2*p in both dimensions because the padding doesn't contribute to the valid convolution region.\n",
        "\n",
        "----\n",
        "##Perform Convolution\n",
        "\n",
        "    for i in range(numRows - fSize + 1):\n",
        "        for j in range(numCols - fSize + 1):\n",
        "            A = I2[i:i+fSize, j:j+fSize]\n",
        "            C[i, j] = (A.flatten() * K.flatten()).sum()\n",
        "\n",
        "----\n",
        "##The outer for loop\n",
        "\n",
        "iterates over the rows of the padded image I2, stopping at\n",
        "\n",
        "    numRows - fSize + 1\n",
        "\n",
        "to ensure the kernel K doesn't go out of bounds.\n",
        "\n",
        "###Why Add 1?\n",
        "The +1 in numRows - fSize + 1 accounts for the starting position being inclusive.\n",
        "\n",
        "Without +1, the filter would not cover all valid starting positions.\n",
        "\n",
        " For a 2x2 filter on a 5x5 image, it can start from position 0, 1, 2, or 3, which is 4 valid positions (5 - 2 + 1 = 4).\n",
        "\n",
        " numRows - fSize + 1 ensures the loop covers all valid positions from top-left to bottom-right.\n",
        "\n",
        "The kernel slides across every possible position where it fits entirely within the padded image.\n",
        "\n",
        "The padding ensures the kernel can also slide over the boundary regions of the original image.\n",
        "\n",
        "Adding +1 in numRows - fSize + 1 ensures that we include the last valid position, considering the inclusive nature of Python's range function.\n",
        "\n",
        "----\n",
        "###The inner for loop\n",
        "\n",
        "iterates over the columns, similarly stopping at numCols - fSize + 1.\n",
        "\n",
        "###A = I2[i:i+fSize, j:j+fSize]\n",
        "\n",
        "This line extracts a submatrix A from the padded image I2 starting at (i, j) with the same size as the kernel K.\n",
        "---\n",
        "\n",
        "    C[i, j] = (A.flatten() * K.flatten()).sum()\n",
        "\n",
        "This line performs element-wise multiplication of the flattened versions of A and K, then sums the result to get a single scalar value, which is assigned to the corresponding position (i, j) in the output matrix C.\n",
        "\n",
        "----\n",
        "##Return the Convolution Result\n",
        "\n",
        "    return C\n",
        "\n",
        "The function returns the convolved image C.\n",
        "\n",
        "-----\n",
        "##Summary\n",
        "The function f_conv2d performs the following steps:\n",
        "\n",
        "Pads the input image I with p zeros on all sides.\n",
        "\n",
        "Initializes an output matrix C to store the results of the convolution.\n",
        "\n",
        "Iterates over the padded image to perform the convolution operation, extracting submatrices of the same size as the kernel, performing element-wise multiplication, summing the results, and storing them in the corresponding positions in C.\n",
        "\n",
        "Returns the convolved image C.\n",
        "\n",
        "----\n",
        "##Example Usage\n",
        "\n",
        "\n",
        "    import numpy as np\n",
        "\n",
        "# Define a simple input image and kernel\n",
        "    I = np.array([[1, 2, 3],\n",
        "                 [4, 5, 6],\n",
        "                 [7, 8, 9]])\n",
        "\n",
        "    K = np.array([[1, 0],\n",
        "                 [0, -1]])\n",
        "\n",
        "# Define padding size\n",
        "    p = 1\n",
        "\n",
        "# Perform the convolution\n",
        "    result = f_conv2d(I, K, p)\n",
        "\n",
        "    print(result)\n",
        "\n",
        "This code snippet should help you understand how the convolution operation is performed using the f_conv2d function."
      ],
      "metadata": {
        "id": "4CK6jLtXbIrL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqMTr-k-kuEF"
      },
      "outputs": [],
      "source": [
        "def f_ReLU(C):\n",
        "    C[C<0] = 0\n",
        "    return C"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RELU Function Definition\n",
        "\n",
        "    def f_ReLU(C):\n",
        "       C[C < 0] = 0\n",
        "       return C\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Purpose of the Function\n",
        "This function implements the ReLU (Rectified Linear Unit) activation function.\n",
        "\n",
        "The ReLU function is commonly used in neural networks, particularly in deep learning, because it introduces non-linearity to the model and helps mitigate the vanishing gradient problem.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## How the Function Works\n",
        "\n",
        "###Input:\n",
        "\n",
        "The function takes a single input parameter C, which is typically a NumPy array.\n",
        "\n",
        "This array represents the output of a layer in a neural network before applying the activation function.\n",
        "\n",
        "----\n",
        "##ReLU Activation\n",
        "\n",
        "The ReLU function sets all negative values in the input array C to zero. This is achieved using the line C[C < 0] = 0.\n",
        "\n",
        "C < 0 creates a boolean array where each element is True if the corresponding element in C is less than zero, and False otherwise.\n",
        "\n",
        "C[C < 0] = 0 uses this boolean array to index into C and set all elements where the condition is True to zero.\n",
        "\n",
        "----\n",
        "##Return\n",
        "\n",
        "The modified array C, with all negative values replaced by zeros, is then returned.\n",
        "\n",
        "----\n",
        "##Example\n",
        "Let's look at a simple example to see how it works:\n",
        "\n",
        "    import numpy as np\n",
        "\n",
        "# Example input array\n",
        "    C = np.array([[1, -2, 3], [-4, 5, -6]])\n",
        "\n",
        "# Applying the ReLU function\n",
        "    C_relu = f_ReLU(C)\n",
        "\n",
        "    print(C_relu)\n",
        "\n",
        "----\n",
        "##Output\n",
        "\n",
        "    [[1 0 3]\n",
        "    [0 5 0]]\n",
        "\n",
        "-----\n",
        "##Explanation of the Example\n",
        "###Input Array C:\n",
        "\n",
        "    [[ 1, -2,  3],\n",
        "    [-4,  5, -6]]\n",
        "\n",
        "-----\n",
        "##Applying ReLU\n",
        "\n",
        "Positive values remain unchanged.\n",
        "\n",
        "Negative values are set to zero.\n",
        "\n",
        "-----\n",
        "##Output Array C_relu\n",
        "\n",
        "    [[1, 0, 3],\n",
        "    [0, 5, 0]]\n",
        "\n",
        "-----\n",
        "##Summary\n",
        "ReLU Activation Function: The function f_ReLU is a straightforward implementation of the ReLU activation function.\n",
        "\n",
        "It modifies the input array C in-place by setting all negative values to zero and returns the modified array.\n",
        "\n",
        "Use in Neural Networks: ReLU is widely used in neural networks because it helps to introduce non-linearity, which is crucial for learning complex patterns, and it mitigates the vanishing gradient problem by ensuring that gradients do not become too small during backpropagation.\n",
        "\n",
        "This function is essential in the context of neural networks and deep learning, where activation functions play a critical role in the performance and convergence of the model."
      ],
      "metadata": {
        "id": "gskiBAa8Hg8O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iy0oBac-kuEI"
      },
      "outputs": [],
      "source": [
        "def f_sigmoid(f,w,bf):\n",
        "    x = w.dot(f)+bf\n",
        "    y_hat = 1/(1+np.exp(-x))\n",
        "    return y_hat"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sigmoid Function Definition\n",
        "\n",
        "    def f_sigmoid(f, w, bf):\n",
        "       x = w.dot(f) + bf\n",
        "       y_hat = 1 / (1 + np.exp(-x))\n",
        "       return y_hat\n",
        "\n",
        "------\n",
        "## Purpose of the Function\n",
        "This function calculates the output of a single-layer neural network using the sigmoid activation function.\n",
        "\n",
        "The sigmoid function maps any real-valued number into the range (0, 1), making it useful for binary classification problems.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Parameters\n",
        "###f\n",
        "\n",
        "This is typically a vector representing the input features.\n",
        "\n",
        "###w\n",
        "\n",
        "This is a weight vector that is used to scale the input features.\n",
        "\n",
        "###bf\n",
        "\n",
        "This is a bias term that is added after the weighted sum of the input features.\n",
        "\n",
        "-----\n",
        "##Steps in the Function\n",
        "###Weighted Sum with Bias\n",
        "\n",
        "    x = w.dot(f) + bf\n",
        "\n",
        "###w.dot(f)\n",
        "\n",
        "This performs the dot product between the weight vector w and the input feature vector f. The dot product is a single number that represents the weighted sum of the input features.\n",
        "\n",
        "###+ bf\n",
        "\n",
        "This adds the bias term bf to the weighted sum. The bias helps in adjusting the output along with the weights.\n",
        "\n",
        "-----\n",
        "##Sigmoid Activation\n",
        "\n",
        "    y_hat = 1 / (1 + np.exp(-x))\n",
        "\n",
        "###np.exp(-x)\n",
        "\n",
        "This calculates the exponential of -x.\n",
        "\n",
        "###1 / (1 + np.exp(-x))\n",
        "\n",
        "This applies the sigmoid function to x. The sigmoid function squashes the input x into a value between 0 and 1.\n",
        "\n",
        "----\n",
        "##Return\n",
        "\n",
        "    return y_hat\n",
        "\n",
        "The function returns y_hat, which is the output of the sigmoid activation function.\n",
        "\n",
        "This value is interpreted as the probability or confidence level of the input belonging to a certain class (e.g., class 1 in binary classification).\n",
        "\n",
        "-----\n",
        "##Example\n",
        "Let's look at a simple example to see how it works:\n",
        "\n",
        "    import numpy as np\n",
        "\n",
        "# Example input\n",
        "    f = np.array([0.5, 1.5])  # Input feature vector\n",
        "    w = np.array([0.3, 0.7])  # Weight vector\n",
        "    bf = 0.1                  # Bias term\n",
        "\n",
        "# Applying the sigmoid function\n",
        "    y_hat = f_sigmoid(f, w, bf)\n",
        "\n",
        "    print(y_hat)\n",
        "\n",
        "#Output\n",
        "\n",
        "0.7858349830425586\n",
        "\n",
        "#Explanation of the Example\n",
        "Input Feature Vector f: [0.5, 1.5]\n",
        "\n",
        "Weight Vector w: [0.3, 0.7]\n",
        "\n",
        "Bias Term bf: 0.1\n",
        "\n",
        "Weighted Sum Calculation:\n",
        "\n",
        "w.dot(f) = 0.3*0.5 + 0.7*1.5 = 0.15 + 1.05 = 1.2\n",
        "\n",
        "Adding bias: x = 1.2 + 0.1 = 1.3\n",
        "\n",
        "Sigmoid Activation Calculation:\n",
        "\n",
        "np.exp(-1.3) ≈ 0.27253179\n",
        "\n",
        "y_hat = 1 / (1 + 0.27253179) ≈ 0.7858349830425586\n",
        "\n",
        "Output y_hat: 0.7858349830425586\n",
        "\n",
        "This value indicates a high confidence that the input belongs to the positive class (close to 1).\n",
        "\n",
        "----\n",
        "##Summary\n",
        "Function Purpose: The f_sigmoid function calculates the output of a single-layer neural network using the sigmoid activation function.\n",
        "\n",
        "Weighted Sum with Bias: It first computes the weighted sum of the input features and adds the bias term.\n",
        "\n",
        "Sigmoid Activation: It then applies the sigmoid activation function to squash the result into a range between 0 and 1.\n",
        "\n",
        "Output: The output is the activated value, representing the predicted probability or confidence level for the input."
      ],
      "metadata": {
        "id": "a3q7liHrJ0K1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwf-54rYkuEK"
      },
      "outputs": [],
      "source": [
        "def f_forwardPass(I,K,b,w,bf):\n",
        "    p = int(K.shape[0]/2)\n",
        "    C = f_conv2d(I,K,p)\n",
        "    C = C+b\n",
        "    C = f_ReLU(C)\n",
        "    S = f_pool(C)\n",
        "    f = S.flatten()\n",
        "    y_hat = f_sigmoid(f,w,bf)\n",
        "    return C,f,y_hat"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Function Definition\n",
        "\n",
        "This function performs a forward pass through a convolutional neural network (CNN) layer, followed by a fully connected layer.\n",
        "\n",
        "The forward pass involves convolution, adding bias, applying the ReLU activation function, pooling, flattening the result, and finally using a sigmoid activation to produce the output.\n",
        "\n",
        "----\n",
        "##Parameters\n",
        "\n",
        "I: The input image or feature map.\n",
        "\n",
        "K: The convolutional kernel (filter).\n",
        "\n",
        "b: The bias term for the convolutional layer.\n",
        "\n",
        "w: The weight vector for the fully connected layer.\n",
        "\n",
        "bf: The bias term for the fully connected layer.\n",
        "\n",
        "-----\n",
        "##Steps in the Function\n",
        "###Padding Calculation:\n",
        "\n",
        "    p = int(K.shape[0] / 2)\n",
        "\n",
        "This calculates the amount of padding needed. For a kernel of size fSize, padding of fSize/2 ensures that the output feature map has the same spatial dimensions as the input.\n",
        "\n",
        "----\n",
        "----\n",
        "Let's break down and explain the purpose of this line of code:\n",
        "\n",
        "    p = int(K.shape[0] / 2)\n",
        "\n",
        "##Purpose of the Line\n",
        "This line calculates the amount of padding required to ensure that the output feature map has the same spatial dimensions (height and width) as the input image after the convolution operation.\n",
        "\n",
        "##Explanation\n",
        "###Kernel Shape\n",
        "\n",
        "    K.shape[0]\n",
        "\n",
        "K is the convolutional kernel (or filter), and K.shape[0] gives the height (or number of rows) of the kernel.\n",
        "\n",
        "Assuming the kernel is square, K.shape[0] is equal to K.shape[1].\n",
        "\n",
        "###Padding Calculation:\n",
        "\n",
        "    K.shape[0] / 2\n",
        "\n",
        "This expression calculates half the height of the kernel. For example, if the kernel is of size 3x3, K.shape[0] is 3, and 3 / 2 gives 1.5.\n",
        "Integer Conversion:\n",
        "\n",
        "    int(K.shape[0] / 2)\n",
        "\n",
        "The result of the division is converted to an integer using int(). This rounds down the result to the nearest whole number. So, if the kernel size is 3, 1.5 becomes 1.\n",
        "\n",
        "----\n",
        "-----\n",
        "\n",
        "##Example\n",
        "Consider a few different kernel sizes to see how padding is calculated:\n",
        "\n",
        "###Kernel Size 3x3\n",
        "    K.shape[0] = 3\n",
        "    K.shape[0] / 2 = 1.5\n",
        "    int(3 / 2) = 1\n",
        "    p = 1\n",
        "\n",
        "##Kernel Size 5x5\n",
        "    K.shape[0] = 5\n",
        "    K.shape[0] / 2 = 2.5\n",
        "    int(5 / 2) = 2\n",
        "    p = 2\n",
        "\n",
        "----\n",
        "##Purpose of Padding\n",
        "Padding is used to control the spatial dimensions of the output feature map after applying the convolution operation.\n",
        "\n",
        "By padding the input image appropriately, you ensure that the output feature map has the same height and width as the input image.\n",
        "\n",
        "----\n",
        "##Why Padding?\n",
        "Let's take an example to understand why padding is necessary:\n",
        "\n",
        "###Without Padding\n",
        "Assume you have a 5x5 input image and a 3x3 kernel.\n",
        "\n",
        "If you perform a convolution without padding, the output feature map will be smaller than the input image:\n",
        "\n",
        "###Input Image: 5x5\n",
        "###Kernel: 3x3\n",
        "\n",
        "    Output Feature Map: (5 - 3 + 1) x (5 - 3 + 1) = 3x3\n",
        "\n",
        "This reduction in size occurs because the kernel cannot be applied to the borders of the input image without going out of bounds.\n",
        "\n",
        "##With Padding\n",
        "To maintain the original size of the input image, you can add padding around the borders:\n",
        "\n",
        "    Padding: 1 (for a 3x3 kernel)\n",
        "    Input Image after Padding: 7x7 (5 original + 2 padding)\n",
        "    Kernel: 3x3\n",
        "\n",
        "Output Feature Map: (7 - 3 + 1) x (7 - 3 + 1) = 5x5\n",
        "\n",
        "By adding 1 pixel of padding around the borders, the output feature map retains the same height and width as the input image.\n",
        "----\n",
        "##Conclusion\n",
        "The line p = int(K.shape[0] / 2) calculates the amount of padding needed to ensure that the convolution operation does not reduce the spatial dimensions of the input image.\n",
        "\n",
        "This is particularly useful in convolutional neural networks (CNNs) where maintaining the input size across layers is often desirable.\n",
        "\n",
        "----\n",
        "-----\n",
        "##Convolution\n",
        "\n",
        "    C = f_conv2d(I, K, p)\n",
        "\n",
        "f_conv2d is called to perform a 2D convolution of the input image I with the kernel K, using padding p.\n",
        "\n",
        "##Adding Bias\n",
        "\n",
        "    C = C + b\n",
        "\n",
        "The bias term b is added to the result of the convolution.\n",
        "\n",
        "##ReLU Activation\n",
        "\n",
        "    C = f_ReLU(C)\n",
        "\n",
        "The ReLU activation function is applied to introduce non-linearity by setting all negative values in C to zero.\n",
        "\n",
        "##Pooling\n",
        "\n",
        "    S = f_pool(C)\n",
        "    \n",
        "f_pool is called to perform a pooling operation on the activated feature map C, reducing its spatial dimensions.\n",
        "\n",
        "##Flattening\n",
        "\n",
        "    f = S.flatten()\n",
        "\n",
        "The pooled feature map S is flattened into a one-dimensional vector f, preparing it for the fully connected layer.\n",
        "\n",
        "##Fully Connected Layer and Sigmoid Activation:\n",
        "\n",
        "    y_hat = f_sigmoid(f, w, bf)\n",
        "\n",
        "The sigmoid activation function is applied to the output of the fully connected layer.\n",
        "\n",
        "This layer computes a weighted sum of the input vector f with weights w and adds the bias term bf.\n",
        "\n",
        "----\n",
        "##Return Values\n",
        "\n",
        "    return C, f, y_hat\n",
        "\n",
        "The function returns three values:\n",
        "\n",
        "C: The feature map after convolution, bias addition, and ReLU activation.\n",
        "\n",
        "f: The flattened feature vector after pooling.\n",
        "\n",
        "y_hat: The final output of the forward pass after applying the sigmoid activation function.\n",
        "\n",
        "-----\n",
        "##Example\n",
        "Let's walk through an example to see how it works:\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Define dummy inputs and parameters\n",
        "    I = np.random.rand(5, 5)  # Example input image\n",
        "    K = np.random.rand(3, 3)  # Example kernel\n",
        "    b = 1.0  # Bias term for convolution\n",
        "    w = np.random.rand(9)  # Weights for fully connected layer (assuming 3x3 pooling result flattened)\n",
        "    bf = 0.5  # Bias term for fully connected layer\n",
        "\n",
        "# Define the necessary functions (as assumed to exist in the code)\n",
        "    def f_conv2d(I, K, p):\n",
        "       # Simplified convolution function\n",
        "       I2 = np.pad(I, ((p, p), (p, p)), mode='constant')\n",
        "       C = np.zeros_like(I)\n",
        "\n",
        "       for i in range(I.shape[0]):\n",
        "          for j in range(I.shape[1]):\n",
        "            C[i, j] = np.sum(I2[i:i+K.shape[0], j:j+K.shape[1]] * K)\n",
        "            return C\n",
        "\n",
        "-----\n",
        "    def f_ReLU(C):\n",
        "       C[C < 0] = 0\n",
        "       return C\n",
        "\n",
        "----\n",
        "    def f_pool(C):\n",
        "       # Simplified pooling function (2x2 max pooling)\n",
        "       S = C[::2, ::2]  # Assume downsampling by factor of 2\n",
        "       return S\n",
        "\n",
        "    def f_sigmoid(f, w, bf):\n",
        "       x = w.dot(f) + bf\n",
        "       y_hat = 1 / (1 + np.exp(-x))\n",
        "       return y_hat\n",
        "\n",
        "# Perform the forward pass\n",
        "    C, f, y_hat = f_forwardPass(I, K, b, w, bf)\n",
        "    print(\"Convolved feature map (C):\", C)\n",
        "    print(\"Flattened feature vector (f):\", f)\n",
        "    print(\"Output (y_hat):\", y_hat)\n",
        "\n",
        "##Summary\n",
        "Padding Calculation: Determines how much padding is needed to maintain the input size after convolution.\n",
        "\n",
        "###Convolution\n",
        "\n",
        "Applies the kernel to the input image, adding padding to maintain the spatial dimensions.\n",
        "\n",
        "###Adding Bias\n",
        "\n",
        "Adds a bias term to each element of the convolved feature map.\n",
        "\n",
        "###ReLU Activation\n",
        "\n",
        "Applies the ReLU function to introduce non-linearity.\n",
        "\n",
        "###Pooling\n",
        "\n",
        "Reduces the spatial dimensions of the feature map.\n",
        "\n",
        "###Flattening\n",
        "\n",
        "Converts the pooled feature map into a one-dimensional vector.\n",
        "\n",
        "###Fully Connected Layer and Sigmoid Activation\n",
        "\n",
        "Applies a weighted sum and sigmoid function to produce the final output.\n",
        "\n",
        "###Return Values\n",
        "\n",
        "Provides the convolved feature map, flattened feature vector, and final output.\n",
        "\n",
        "This function demonstrates the key steps involved in a forward pass through a simple CNN and fully connected layer."
      ],
      "metadata": {
        "id": "d8Gaj72TMr3w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDGXUmuMkuEL"
      },
      "outputs": [],
      "source": [
        "def f_getGradient_w(y_hat,y,f):\n",
        "    Dw = np.squeeze(np.zeros((1,len(f))))\n",
        "    a = (y_hat-y)*y_hat*(1-y_hat)\n",
        "    for i in range(len(f)):\n",
        "        Dw[i] = a*f[i]\n",
        "    return Dw"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1st Backpropagation Function -Calculates the Gradient (Single Neuron Backpropagation)\n",
        "\n",
        "This function f_getGradient_w calculates the gradient of the weights (denoted as Dw) for a single neuron in a neural network during the backpropagation process.\n",
        "\n",
        "The gradient is used to update the weights in order to minimize the loss function.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#Parameters\n",
        "\n",
        "##y_hat\n",
        "\n",
        "The predicted output of the neuron (a single value).\n",
        "\n",
        "##y\n",
        "\n",
        "The true label or target value (a single value).\n",
        "\n",
        "##f\n",
        "\n",
        "The flattened feature vector (input to the neuron).\n",
        "\n",
        "----\n",
        "#Steps in the Function\n",
        "\n",
        "##Initialize the Gradient Array\n",
        "\n",
        "    Dw = np.squeeze(np.zeros((1, len(f))))\n",
        "    np.zeros((1, len(f)))\n",
        "    \n",
        "creates a 1xN array filled with zeros, where N is the length of the feature vector f.\n",
        "\n",
        "=========================================\n",
        "##np.squeeze\n",
        "\n",
        "removes single-dimensional entries from the shape of the array, converting it to a 1D array of length N.\n",
        "\n",
        "Let's create an array with a shape that includes a single-dimensional entry:\n",
        "\n",
        "    import numpy as np\n",
        "    array = np.zeros((1, 5))\n",
        "    print(array.shape)  # Output: (1, 5)\n",
        "\n",
        "This creates a 2D array with one row and five columns.\n",
        "\n",
        "The shape of the array is (1, 5), meaning it has one row and five columns.\n",
        "\n",
        "##Applying np.squeeze:\n",
        "\n",
        "Now, let's apply np.squeeze to this array:\n",
        "\n",
        "    squeezed_array = np.squeeze(array)\n",
        "    print(squeezed_array.shape)  # Output: (5,)\n",
        "    \n",
        "np.squeeze removes the single-dimensional entry from the shape.\n",
        "\n",
        "The shape of the array after squeezing is (5,), meaning it's now a 1D array with five elements.\n",
        "\n",
        "##Explanation\n",
        "The original array array has a shape of (1, 5).\n",
        "\n",
        "This means it has one row and five columns.\n",
        "\n",
        "When we apply np.squeeze, it removes the dimension of size 1 (the single row), resulting in a 1D array with the shape (5,).\n",
        "\n",
        "##Applying to the Code Snippet\n",
        "In the context of your code snippet:\n",
        "\n",
        "    Dw = np.squeeze(np.zeros((1, len(f))))\n",
        "\n",
        "##Creating a Zero Array\n",
        "\n",
        "###np.zeros((1, len(f)))\n",
        "\n",
        "creates a 2D array with shape (1, len(f)), where len(f) is the number of features.\n",
        "\n",
        "This means it creates a single row with len(f) columns, all initialized to zero.\n",
        "\n",
        "##Applying np.squeeze\n",
        "\n",
        "np.squeeze removes the single-dimensional entry (the single row), converting it to a 1D array.\n",
        "\n",
        "After squeezing, the shape of Dw is (len(f),), which is a 1D array with len(f) elements.\n",
        "\n",
        "##Why Use np.squeeze?\n",
        "\n",
        "In many machine learning and numerical computing tasks, we often need to ensure that arrays have the correct dimensions.\n",
        "\n",
        "np.squeeze is used here to convert a 2D array with one row into a 1D array, which simplifies further calculations and operations.\n",
        "\n",
        "##Conclusion\n",
        "np.squeeze is a useful function in NumPy to remove single-dimensional entries from the shape of an array.\n",
        "\n",
        "In the provided code, it ensures that the gradient array Dw has the correct shape (1D array with length equal to the number of features) for further processing.\n",
        "\n",
        "-----\n",
        "##Compute the Error Term\n",
        "\n",
        "    a = (y_hat - y) * y_hat * (1 - y_hat)\n",
        "\n",
        "This term a represents the error gradient with respect to the neuron's output.\n",
        "\n",
        "y_hat - y is the difference between the predicted output and the true output.\n",
        "\n",
        "y_hat * (1 - y_hat) is the derivative of the sigmoid activation function (which is commonly used in neural networks).\n",
        "\n",
        "The product of these terms gives the gradient of the loss with respect to the neuron's output.\n",
        "\n",
        "-----\n",
        "##Calculate the Gradient for Each Weight\n",
        "\n",
        "    for i in range(len(f)):\n",
        "       Dw[i] = a * f[i]\n",
        "\n",
        "Iterate over each element in the feature vector f.\n",
        "\n",
        "For each feature f[i], calculate the gradient Dw[i] by multiplying the error term a with the corresponding feature value f[i].\n",
        "\n",
        "This follows the chain rule in calculus, where the gradient of the weight is the product of the gradient of the output and the input feature.\n",
        "\n",
        "----\n",
        "##Return the Gradient\n",
        "\n",
        "    return Dw\n",
        "\n",
        "The function returns the calculated gradient array Dw.\n",
        "\n",
        "------\n",
        "#Example\n",
        "Suppose y_hat = 0.8, y = 1, and f = [0.5, 0.3, 0.2].\n",
        "\n",
        "##Initialize the Gradient Array\n",
        "\n",
        "    Dw = np.squeeze(np.zeros((1, 3)))  # Dw = [0.0, 0.0, 0.0]\n",
        "\n",
        "##Compute the Error Term\n",
        "\n",
        "    a = (0.8 - 1) * 0.8 * (1 - 0.8)  # a = -0.16\n",
        "\n",
        "##Calculate the Gradient for Each Weight\n",
        "\n",
        "    Dw[0] = -0.16 * 0.5  # Dw[0] = -0.08\n",
        "    Dw[1] = -0.16 * 0.3  # Dw[1] = -0.048\n",
        "    Dw[2] = -0.16 * 0.2  # Dw[2] = -0.032\n",
        "\n",
        "##Return the Gradient\n",
        "\n",
        "    return Dw  # Dw = [-0.08, -0.048, -0.032]\n",
        "\n",
        "----   \n",
        "#Conclusion\n",
        "This function computes the gradient of the loss function with respect to the weights of a neuron.\n",
        "\n",
        "The gradients are essential for updating the weights during the training process to minimize the loss function and improve the model's predictions."
      ],
      "metadata": {
        "id": "GupTY2onK57A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjR5w2UhkuEM"
      },
      "outputs": [],
      "source": [
        "def f_getGradient_f(y_hat,y,w):\n",
        "    Df = np.squeeze(np.zeros((1,len(w))))\n",
        "    a = (y_hat-y)*y_hat*(1-y_hat)\n",
        "    for i in range(len(w)):\n",
        "        Df[i] = a*w[i]\n",
        "    return Df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2nd Backpropagation Function -Calculates the gradient of the loss with respect to the features\n",
        "\n",
        "both calculate gradients, but they differ in the parameters they use to compute these gradients and their intended use in the context of training a neural network.\n",
        "\n",
        "--------------\n",
        "    f_getGradient_f:\n",
        "\n",
        "##Inputs\n",
        "\n",
        "    y_hat (predicted value), y (true value), w (weight vector)\n",
        "\n",
        "##Outputs: Df (gradient with respect to features)\n",
        "\n",
        "    def f_getGradient_f(y_hat, y, w):\n",
        "      # Initialize gradient vector with zeros\n",
        "     Df = np.squeeze(np.zeros((1, len(w))))\n",
        "    \n",
        "     # Calculate the error term 'a'\n",
        "     a = (y_hat - y) * y_hat * (1 - y_hat)\n",
        "    \n",
        "     # Compute the gradient for each weight\n",
        "     for i in range(len(w)):\n",
        "        Df[i] = a * w[i]\n",
        "    \n",
        "     return Df\n",
        "\n",
        "This function computes the gradient of the loss with respect to each feature by considering the contribution of each weight. Again, the error term a adjusts the gradient based on the difference between the predicted and true values.\n",
        "\n",
        "-----\n",
        "#Context in Neural Network Training\n",
        "f_getGradient_w: This function is used during the backpropagation step to update the weights of the network.\n",
        "\n",
        "The gradients calculated here indicate how much each weight should be adjusted to minimize the loss.\n",
        "\n",
        "    f_getGradient_f:\n",
        "\n",
        "This function calculates how much the input features (or activations from a previous layer) influence the loss, which can be used to update the features in certain types of neural networks or for analysis purposes.\n",
        "\n",
        "----\n",
        "#Example:\n",
        "Assume we have:\n",
        "\n",
        "Predicted value y_hat = 0.8\n",
        "\n",
        "True value y = 1\n",
        "\n",
        "Feature vector f = [0.5, 1.2, -0.7]\n",
        "\n",
        "Weight vector w = [0.3, -0.8, 0.5]\n",
        "\n",
        "    Using f_getGradient_w:\n",
        "    gradients_w = f_getGradient_w(0.8, 1, [0.5, 1.2, -0.7])\n",
        "    print(\"Gradients with respect to weights:\", gradients_w)\n",
        "\n",
        "##Using f_getGradient_f:\n",
        "\n",
        "    gradients_f = f_getGradient_f(0.8, 1, [0.3, -0.8, 0.5])\n",
        "    print(\"Gradients with respect to features:\", gradients_f)\n",
        "\n",
        "Both functions will provide different gradient vectors, reflecting their respective influences on the loss function.\n",
        "\n",
        "----\n",
        "#Conclusion\n",
        "The main difference between the two functions lies in their targets for gradient computation:\n",
        "\n",
        "f_getGradient_w focuses on the gradients of the weights using the feature vector.\n",
        "\n",
        "f_getGradient_f focuses on the gradients of the features using the weight vector.\n",
        "\n",
        "In summary, f_getGradient_w and f_getGradient_f are integral steps in the backpropagation process. They calculate the necessary gradients for updating weights and features to minimize the loss function, thereby improving the model's performance during training.\n",
        "\n",
        "\n",
        "The first function in the backpropagation process is\n",
        "\n",
        "##1. f_getGradient_w.\n",
        "\n",
        "This is followed by\n",
        "\n",
        "##f_getGradient_f.\n",
        "\n",
        "By computing the gradients in this order, you ensure that the error term is propagated correctly from the output layer back through the network, allowing for proper weight updates during the training process.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LT5vJC8lWem5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ooLYTUZkuEO"
      },
      "outputs": [],
      "source": [
        "def f_getGradient_bf(y_hat,y):\n",
        "    Dbf = (y_hat-y)*y_hat*(1-y_hat)\n",
        "    return Dbf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3rd Backpropagation Function - Calculates the gradient of the loss with respect to the bias term in the output layer\n",
        "\n",
        "This function calculates the gradient of the loss with respect to the bias term in the output layer. In the context of a neural network, the bias term affects the activation function and needs to be adjusted to minimize the loss.\n",
        "\n",
        "----\n",
        "## Calculation\n",
        "\n",
        "It uses the error term, computed as the difference between the predicted output (y_hat) and the actual output (y), multiplied by the derivative of the sigmoid activation function (since y_hat is the output of a sigmoid function).\n",
        "\n",
        "This function calculates the gradient of the loss with respect to the bias term in the output layer. In the context of a neural network, the bias term affects the activation function and needs to be adjusted to minimize the loss.\n",
        "\n",
        "Calculation: It uses the error term, computed as the difference between the predicted output (y_hat) and the actual output (y), multiplied by the derivative of the sigmoid activation function (since y_hat is the output of a sigmoid function).\n",
        "\n"
      ],
      "metadata": {
        "id": "dI0Y_sVokvpq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uJP-8lRkuEQ"
      },
      "outputs": [],
      "source": [
        "def f_getGradient_S(Df):\n",
        "    n = int(len(Df)**0.5)\n",
        "    DS = Df.reshape((n,n))\n",
        "    return DS"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4th Backpropagation Function -Df is a 1D array (vector) that represents the gradient of some feature or activation, which was flattened from its original 2D shape\n",
        "\n",
        "---\n",
        "##Input Gradient Vector (Df):\n",
        "\n",
        "Df is assumed to be a 1D array (vector) that represents the gradient of some feature or activation, which was flattened from its original 2D shape.\n",
        "Calculate the Dimension (n):\n",
        "\n",
        "    n = int(len(Df) ** 0.5)\n",
        "\n",
        "This calculates the size of one dimension of the original 2D shape. It assumes that Df was flattened from a square matrix, so the number of elements in Df should be a perfect square.\n",
        "\n",
        "len(Df) gives the total number of elements in the 1D gradient vector.\n",
        "\n",
        "Taking the square root of len(Df) gives the dimension of the original 2D matrix, n.\n",
        "\n",
        "int() converts the result to an integer. This is necessary because the square root could result in a floating-point number.\n",
        "\n",
        "-----\n",
        "##Reshape the Vector (DS):\n",
        "\n",
        "    DS = Df.reshape((n, n))\n",
        "\n",
        "This reshapes the 1D gradient vector Df into a 2D matrix with shape (n, n).\n",
        "The reshape method transforms the vector back into the 2D shape it was before being flattened.\n",
        "\n",
        "----\n",
        "##Return the Reshaped Matrix\n",
        "\n",
        "The function returns the reshaped matrix DS, which now matches the original 2D dimensions from which Df was flattened.\n",
        "\n",
        "-----\n",
        "Example\n",
        "Assume Df is a gradient vector with 16 elements. This suggests that the original matrix was of size 4x4. Here’s what happens:\n",
        "\n",
        "    Df = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16])\n",
        "    n = int(len(Df) ** 0.5)  # n = int(16 ** 0.5) = 4\n",
        "    DS = Df.reshape((n, n))\n",
        "\n",
        "The reshaped matrix DS would be:\n",
        "\n",
        "[[ 1,  2,  3,  4],\n",
        " [ 5,  6,  7,  8],\n",
        " [ 9, 10, 11, 12],\n",
        " [13, 14, 15, 16]]\n",
        "\n",
        " -----\n",
        "##Summary\n",
        "The function f_getGradient_S is used to convert a flattened 1D gradient vector back into its original 2D form.\n",
        "\n",
        "This is useful in neural networks for visualizing or updating the gradients associated with a particular feature or layer in its spatial dimensions."
      ],
      "metadata": {
        "id": "hBtTkX8hi-BK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TE7E_fXHkuER"
      },
      "outputs": [],
      "source": [
        "def f_getGradient_C(DS,C):\n",
        "    r = C.shape[0]\n",
        "    c = C.shape[1]\n",
        "    DC = np.zeros((r,c))\n",
        "    for i in range(0,r,2):\n",
        "        for j in range(0,c,2):\n",
        "            C_block = C[i:i+2,j:j+2]\n",
        "            ind = np.unravel_index(np.argmax(C_block,axis=None),C_block.shape)\n",
        "            DC[i+ind[0],j+ind[1]] = DS[int(i/2),int(j/2)]\n",
        "    return DC"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Function computes the gradient with respect to the convolutional layer's output (C)\n",
        "\n",
        "The function f_getGradient_C computes the gradient with respect to the convolutional layer's output (C) using the gradient from the pooling layer (DS).\n",
        "\n",
        "This function backpropagates the gradients through a max-pooling layer.\n",
        "\n",
        "-----\n",
        "##Function Explanation\n",
        "\n",
        "    def f_getGradient_C(DS, C):\n",
        "       r = C.shape[0]\n",
        "       c = C.shape[1]\n",
        "       DC = np.zeros((r, c))\n",
        "       for i in range(0, r, 2):\n",
        "         for j in range(0, c, 2):\n",
        "            C_block = C[i:i+2, j:j+2]\n",
        "            ind = np.unravel_index(np.argmax(C_block, axis=None), C_block.shape)\n",
        "            DC[i+ind[0], j+ind[1]] = DS[int(i/2), int(j/2)]\n",
        "        return DC\n",
        "\n",
        "----\n",
        "##Detailed Explanation\n",
        "###Input and Initialization\n",
        "\n",
        "DS: This is the gradient coming from the subsequent layer (the pooling layer in this case). It has the same shape as the output of the pooling layer.\n",
        "\n",
        "C: This is the output of the convolutional layer before pooling.\n",
        "\n",
        "r and c: These are the number of rows and columns of the matrix C.\n",
        "\n",
        "DC: This is the gradient of the loss with respect to the output of the convolutional layer, initialized to a zero matrix of the same shape as C.\n",
        "\n",
        "###Loop Over the Convolution Output\n",
        "\n",
        "The function iterates over the convolutional output C in steps of 2 (since it's assumed that the pooling window is 2x2 and stride is 2).\n",
        "\n",
        "###Pooling Window and Gradient Assignment\n",
        "\n",
        "C_block: This extracts a 2x2 block from the matrix C.\n",
        "\n",
        "ind: This finds the index of the maximum value in the C_block. The max-pooling operation records which index within the 2x2 block had the maximum value.\n",
        "\n",
        "The gradient DS for the corresponding pooled region is then assigned to the location in DC that corresponds to the maximum value in C_block.\n",
        "\n",
        "###Returning the Gradient:\n",
        "\n",
        "The function returns DC, which is the gradient with respect to the convolutional layer's output.\n",
        "\n",
        "-----\n",
        "#Example to Illustrate\n",
        "Let's assume C is the output from a convolutional layer, and DS is the gradient from the pooling layer:\n",
        "\n",
        "    import numpy as np\n",
        "\n",
        "-----\n",
        "# Example convolutional layer output (4x4 matrix)\n",
        "    C = np.array([\n",
        "      [1, 3, 2, 4],\n",
        "      [5, 6, 7, 8],\n",
        "      [9, 2, 4, 1],\n",
        "      [3, 7, 5, 6]\n",
        "    ])\n",
        "\n",
        "-----\n",
        "# Example pooling layer gradient (2x2 matrix)\n",
        "    DS = np.array([\n",
        "      [1, 2],\n",
        "      [3, 4]\n",
        "    ])\n",
        "\n",
        "    def f_getGradient_C(DS, C):\n",
        "       r = C.shape[0]\n",
        "       c = C.shape[1]\n",
        "       DC = np.zeros((r, c))\n",
        "       for i in range(0, r, 2):\n",
        "          for j in range(0, c, 2):\n",
        "             C_block = C[i:i+2, j:j+2]\n",
        "             ind = np.unravel_index(np.argmax(C_block, axis=None), C_block.shape)\n",
        "             DC[i+ind[0], j+ind[1]] = DS[int(i/2), int(j/2)]\n",
        "      return DC\n",
        "\n",
        "###Explanation\n",
        "Looping Over Blocks in the Convolutional Output:\n",
        "    for i in range(0, r, 2):\n",
        "       for j in range(0, c, 2):\n",
        "\n",
        "These loops iterate over the convolutional output matrix C in steps of 2.\n",
        "r and c are the dimensions (number of rows and columns) of the matrix C.\n",
        "\n",
        "The step of 2 is used because the max-pooling operation typically uses a 2x2 window with a stride of 2.\n",
        "\n",
        "###Extracting a 2x2 Block:\n",
        "\n",
        "    C_block = C[i:i+2, j:j+2]\n",
        "\n",
        "This line extracts a 2x2 block from C, starting at position (i, j).\n",
        "\n",
        "C_block is a small 2x2 matrix containing elements from C.\n",
        "\n",
        "###Finding the Index of the Maximum Value in the Block:\n",
        "\n",
        "    ind = np.unravel_index(np.argmax(C_block, axis=None), C_block.shape)\n",
        "    np.argmax(C_block, axis=None)\n",
        "    \n",
        "finds the index of the maximum value in C_block as if C_block were a flattened array.\n",
        "\n",
        "    np.unravel_index\n",
        "\n",
        "then converts this flat index back into a tuple of coordinates within the 2x2 block.\n",
        "\n",
        "ind is a tuple representing the row and column indices of the maximum value within the C_block.\n",
        "\n",
        "###Assigning the Gradient to the Corresponding Position in DC:\n",
        "\n",
        "    DC[i+ind[0], j+ind[1]] = DS[int(i/2), int(j/2)]\n",
        "\n",
        "This line assigns the gradient value from DS to the corresponding position in DC.\n",
        "\n",
        "    DS[int(i/2), int(j/2)]\n",
        "\n",
        "retrieves the gradient from the down-sampled gradient matrix DS. The indices int(i/2) and int(j/2) map the current 2x2 block position to the corresponding position in the smaller DS matrix.\n",
        "\n",
        "    DC[i+ind[0], j+ind[1]]\n",
        "\n",
        "places the retrieved gradient value into DC at the position corresponding to the maximum value within the 2x2 block.\n",
        "\n",
        "-----\n",
        "#Summary\n",
        "The loop iterates over each 2x2 block in the convolutional output matrix C.\n",
        "For each block, it identifies the position of the maximum value (which was selected during max-pooling).\n",
        "\n",
        "It then assigns the corresponding gradient value from the pooled gradient matrix DS back to the original position in DC.\n",
        "\n",
        "-----\n",
        "#Example\n",
        "To clarify, let's consider a small example:\n",
        "\n",
        "##Convolutional Output C:\n",
        "\n",
        "   [[1, 3, 2, 4],\n",
        "    [5, 6, 7, 8],\n",
        "    [9, 2, 4, 1],\n",
        "    [3, 7, 5, 6]]\n",
        "\n",
        "##Pooled Gradient DS:\n",
        "\n",
        "    [[1, 2],\n",
        "    [3, 4]]\n",
        "\n",
        "The function will perform the following steps for each 2x2 block:\n",
        "\n",
        "For the block at (0,0):\n",
        "\n",
        "    [[1, 3],\n",
        "    [5, 6]]\n",
        "\n",
        "Maximum value is 6 at (1,1).\n",
        "\n",
        "Assign DS[0,0] (which is 1) to DC[0+1, 0+1], so DC[1,1] = 1.\n",
        "\n",
        "For the block at (0,2):\n",
        "\n",
        "    [[2, 4],\n",
        "    [7, 8]]\n",
        "\n",
        "Maximum value is 8 at (1,1).\n",
        "\n",
        "Assign DS[0,1] (which is 2) to DC[1,3], so DC[1,3] = 2.\n",
        "\n",
        "For the block at (2,0):\n",
        "\n",
        "    [[9, 2],\n",
        "    [3, 7]]\n",
        "\n",
        "Maximum value is 9 at (0,0).\n",
        "\n",
        "Assign DS[1,0] (which is 3) to DC[2,0], so DC[2,0] = 3.\n",
        "\n",
        "For the block at (2,2):\n",
        "\n",
        "    [[4, 1],\n",
        "    [5, 6]]\n",
        "Maximum value is 6 at (1,1).\n",
        "\n",
        "Assign DS[1,1] (which is 4) to DC[3,3], so DC[3,3] = 4.\n",
        "\n",
        "Resulting DC:\n",
        "\n",
        "   [[0, 0, 0, 0],\n",
        "    [0, 1, 0, 2],\n",
        "    [3, 0, 0, 0],\n",
        "    [0, 0, 0, 4]]\n",
        "\n",
        "This shows how the gradients are backpropagated from the pooled output to the original convolutional output, assigning them to the positions of the maximum values within each pooling block.\n",
        "------\n",
        "# Call the function\n",
        "    DC = f_getGradient_C(DS, C)\n",
        "    print(DC)\n",
        "\n",
        "------\n",
        "#Result\n",
        "\n",
        "[[0. 0. 0. 2.]\n",
        " [0. 1. 0. 0.]\n",
        " [3. 0. 0. 0.]\n",
        " [0. 0. 0. 4.]]\n",
        "\n",
        "Here, the DC matrix shows the gradient assigned to the positions of the maximum values in each 2x2 block of C, as determined by the DS matrix.\n",
        "\n",
        "------------------\n",
        "#Summary\n",
        "The f_getGradient_C function essentially backpropagates the gradients through a max-pooling layer by distributing the gradient from the pooled output back to the positions of the maximum values in the convolutional output.\n",
        "\n",
        "This is an essential step in implementing backpropagation in convolutional neural networks (CNNs), ensuring the correct gradients are computed for the convolutional layer parameters."
      ],
      "metadata": {
        "id": "YpOHwTAadqsC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pio8_YRpkuES"
      },
      "outputs": [],
      "source": [
        "def f_getChainRuleGradients(C,DC,I,u,v):\n",
        "    DKuv = 0\n",
        "    for i in range(C.shape[0]):\n",
        "        for j in range(C.shape[1]):\n",
        "            if C[i,j]>0 and i-u>=0 and j-v>=0 and i-u<C.shape[0] and j-v<C.shape[1]:\n",
        "                DKuv = DKuv + (I[i-u,j-v]*DC[i,j])\n",
        "    return DKuv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The gradient of the loss with respect to 𝐶\n",
        "###C\n",
        "\n",
        "The output of the convolutional layer after applying the activation function (in this case, ReLU).\n",
        "\n",
        "###DC\n",
        "\n",
        "The gradient of the loss with respect to\n",
        "$$𝐶$$\n",
        "which is propagated back from the pooling layer.\n",
        "\n",
        "I: The original input image to the convolutional layer.\n",
        "\n",
        "u, v: The coordinates of the element in the convolutional kernel\n",
        "$$𝐾$$\n",
        "for which we are calculating the gradient.\n",
        "\n",
        "------\n",
        "##Function Logic\n",
        "\n",
        "###Initialization:\n",
        "\n",
        "    DKuv = 0\n",
        "This initializes the gradient\n",
        "\n",
        "$$𝐷\n",
        "𝐾\n",
        "𝑢\n",
        "𝑣$$\n",
        "\n",
        "DKuv of the kernel element at position\n",
        "$$(\n",
        "𝑢\n",
        ",\n",
        "𝑣\n",
        ")$$\n",
        "\n",
        "(u,v) to zero.\n",
        "\n",
        "Loop Over Each Element in\n",
        "$$𝐶$$\n",
        "\n",
        "    for i in range(C.shape[0]):\n",
        "       for j in range(C.shape[1]):\n",
        "\n",
        "This loops over each element in the convolutional output 𝐶.\n",
        "\n",
        "Conditions to Ensure Valid Indices:\n",
        "\n",
        "\n",
        "    if C[i,j] > 0 and i-u >= 0 and j-v >= 0 and i-u < C.shape[0] and j-v < C.shape[1]:\n",
        "\n",
        "C[i, j] > 0: This checks if the element\n",
        "𝐶\n",
        "[\n",
        "𝑖\n",
        ",\n",
        "𝑗\n",
        "]\n",
        "C[i,j]\n",
        "\n",
        "is part of the region that was activated by the ReLU function.\n",
        "\n",
        "Since ReLU sets negative values to zero, we only consider positive values.\n",
        "i-u >= 0 and j-v >= 0 and i-u < C.shape[0] and j-v < C.shape[1]: These conditions ensure that the indices\n",
        "$$(\n",
        "𝑖\n",
        "−\n",
        "𝑢\n",
        ",\n",
        "𝑗\n",
        "−\n",
        "𝑣\n",
        ")$$\n",
        "(i−u,j−v) are within the bounds of the input image\n",
        "𝐼\n",
        "\n",
        "-----\n",
        "##Accumulate Gradient\n",
        "\n",
        "    DKuv = DKuv + (I[i-u, j-v] * DC[i, j])\n",
        "If the conditions are satisfied, the gradient\n",
        "$$𝐷\n",
        "𝐾\n",
        "𝑢\n",
        "𝑣$$\n",
        "\n",
        "DKuv is updated by adding the product of:\n",
        "\n",
        "I[i-u, j-v]: The corresponding element in the input image.\n",
        "\n",
        "DC[i, j]: The gradient of the loss with respect to\n",
        "𝐶\n",
        "[\n",
        "𝑖\n",
        ",\n",
        "𝑗\n",
        "]\n",
        "C[i,j].\n",
        "Return the Gradient:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "return DKuv\n",
        "After looping through all elements, the function returns the calculated gradient\n",
        "𝐷\n",
        "𝐾\n",
        "𝑢\n",
        "𝑣\n",
        "DKuv.\n",
        "\n",
        "Example\n",
        "Let's consider a small example to illustrate the calculation:\n",
        "\n",
        "Input Matrices\n",
        "Input Image\n",
        "𝐼\n",
        "I:\n",
        "\n",
        "plaintext\n",
        "Copy code\n",
        "[[1, 2, 3],\n",
        " [4, 5, 6],\n",
        " [7, 8, 9]]\n",
        "Output of Convolution\n",
        "𝐶\n",
        "C:\n",
        "\n",
        "plaintext\n",
        "Copy code\n",
        "[[1, 3],\n",
        " [5, 6]]\n",
        "Gradient with Respect to\n",
        "𝐶\n",
        "C (DC):\n",
        "\n",
        "plaintext\n",
        "Copy code\n",
        "[[0.1, 0.2],\n",
        " [0.3, 0.4]]\n",
        "Kernel Element Position\n",
        "(\n",
        "𝑢\n",
        ",\n",
        "𝑣\n",
        ")\n",
        "=\n",
        "(\n",
        "0\n",
        ",\n",
        "0\n",
        ")\n",
        "(u,v)=(0,0)\n",
        "\n",
        "Calculation\n",
        "Initialize\n",
        "𝐷\n",
        "𝐾\n",
        "𝑢\n",
        "𝑣\n",
        "DKuv:\n",
        "\n",
        "plaintext\n",
        "Copy code\n",
        "DKuv = 0\n",
        "Loop through each element in\n",
        "𝐶\n",
        "C:\n",
        "\n",
        "For\n",
        "𝑖\n",
        "=\n",
        "0\n",
        ",\n",
        "𝑗\n",
        "=\n",
        "0\n",
        "i=0,j=0:\n",
        "\n",
        "plaintext\n",
        "Copy code\n",
        "DKuv += I[0-0, 0-0] * DC[0, 0]\n",
        "DKuv += 1 * 0.1\n",
        "DKuv = 0.1\n",
        "For\n",
        "𝑖\n",
        "=\n",
        "0\n",
        ",\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "i=0,j=1:\n",
        "\n",
        "plaintext\n",
        "Copy code\n",
        "DKuv += I[0-0, 1-0] * DC[0, 1]\n",
        "DKuv += 2 * 0.2\n",
        "DKuv = 0.5\n",
        "For\n",
        "𝑖\n",
        "=\n",
        "1\n",
        ",\n",
        "𝑗\n",
        "=\n",
        "0\n",
        "i=1,j=0:\n",
        "\n",
        "plaintext\n",
        "Copy code\n",
        "DKuv += I[1-0, 0-0] * DC[1, 0]\n",
        "DKuv += 4 * 0.3\n",
        "DKuv = 1.7\n",
        "For\n",
        "𝑖\n",
        "=\n",
        "1\n",
        ",\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "i=1,j=1:\n",
        "\n",
        "plaintext\n",
        "Copy code\n",
        "DKuv += I[1-0, 1-0] * DC[1, 1]\n",
        "DKuv += 5 * 0.4\n",
        "DKuv = 3.7\n",
        "Return\n",
        "𝐷\n",
        "𝐾\n",
        "𝑢\n",
        "𝑣\n",
        "DKuv:\n",
        "\n",
        "plaintext\n",
        "Copy code\n",
        "return 3.7\n",
        "The function f_getChainRuleGradients accumulates the gradient for the specific kernel element\n",
        "(\n",
        "𝑢\n",
        ",\n",
        "𝑣\n",
        ")\n",
        "(u,v) by considering the influence of each activated element in\n",
        "𝐶\n",
        "C and the corresponding elements in the input image\n",
        "𝐼\n",
        "I. This approach follows the chain rule to propagate the gradients back through the layers."
      ],
      "metadata": {
        "id": "kZxKSydL8tSG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ydf8JGOYkuET"
      },
      "outputs": [],
      "source": [
        "def f_getGradient_K(C,I,y_hat,y,w):\n",
        "    Df = f_getGradient_f(y_hat,y,w)\n",
        "    DS = f_getGradient_S(Df)\n",
        "    DC = f_getGradient_C(DS,C)\n",
        "    DK = np.zeros((5,5))\n",
        "    for u in range(-2,3):\n",
        "        for v in range(-2,3):\n",
        "            DK[u+2,v+2] = f_getChainRuleGradients(C,DC,I,u,v)\n",
        "    return DK,DC"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# f_getGradient_K function\n",
        "\n",
        "This function computes the gradient of the convolutional kernel\n",
        "$$𝐾$$\n",
        "during the backpropagation process in a convolutional neural network (CNN).\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Calculate Gradient with Respect to Flattened Output\n",
        "\n",
        "    Df = f_getGradient_f(y_hat, y, w)\n",
        "\n",
        "###f_getGradient_f(y_hat, y, w):\n",
        "\n",
        "This function calculates the gradient of the loss with respect to the flattened feature map output from the pooling layer.\n",
        "\n",
        "----\n",
        "##Parameters\n",
        "y_hat: The predicted output.\n",
        "\n",
        "y: The true label.\n",
        "\n",
        "w: The weights of the fully connected layer.\n",
        "\n",
        "------\n",
        "##Reshape Gradient to Match Pooled\n",
        " Output Dimensions\n",
        "\n",
        "    DS = f_getGradient_S(Df)\n",
        "\n",
        "###f_getGradient_S(Df)\n",
        "\n",
        "This function reshapes the gradient Df from a flattened vector back into a 2D shape that matches the pooled output dimensions.\n",
        "\n",
        "------\n",
        "##Calculate Gradient with Respect to Convolutional Output:\n",
        "\n",
        "    DC = f_getGradient_C(DS, C)\n",
        "\n",
        "-----\n",
        "\n",
        "    f_getGradient_C(DS, C)\n",
        "\n",
        "This function calculates the gradient of the loss with respect to the output of the convolutional layer (before pooling).\n",
        "\n",
        "\n",
        "###Parameters\n",
        "DS: The gradient with respect to the pooled output.\n",
        "\n",
        "C: The output of the convolutional layer.\n",
        "\n",
        "----\n",
        "\n",
        "Initialize Kernel Gradient Matrix:\n",
        "\n",
        "    DK = np.zeros((5,5))\n",
        "\n",
        "DK: This matrix will hold the gradients of the kernel 𝐾.\n",
        "\n",
        "Assuming a 5x5 kernel, it's initialized to zeros.\n",
        "\n",
        "------\n",
        "###Calculate Gradients for Each Element in the Kernel:\n",
        "\n",
        "    for u in range(-2, 3):\n",
        "       for v in range(-2, 3):\n",
        "          DK[u+2, v+2] = f_getChainRuleGradients(C, DC, I, u, v)\n",
        "\n",
        "This double loop iterates over the kernel's dimensions.\n",
        "\n",
        "For a 5x5 kernel, u and v range from -2 to 2.\n",
        "\n",
        "----\n",
        "###f_getChainRuleGradients(C, DC, I, u, v):\n",
        "\n",
        "This function calculates the gradient for a specific element in the kernel using the chain rule.\n",
        "\n",
        "###Parameters:\n",
        "C: The output of the convolutional layer.\n",
        "\n",
        "DC: The gradient with respect to the\n",
        "convolutional layer's output.\n",
        "\n",
        "I: The input image.\n",
        "\n",
        "u, v: The current indices in the kernel.\n",
        "\n",
        "Note: u+2 and v+2 adjust the indices to correctly map into the 5x5 kernel matrix DK.\n",
        "\n",
        "-----\n",
        "###Return the Gradient Matrices:\n",
        "\n",
        "    return DK, DC\n",
        "\n",
        "-----\n",
        "##Summary of the Gradient Calculation Process\n",
        "Gradient with Respect to Fully Connected Layer Output (Df):\n",
        "\n",
        "1. Calculate the gradient of the loss with respect to the output of the fully connected layer.\n",
        "\n",
        "2. Gradient with Respect to Pooled Output (DS):\n",
        "\n",
        "3. Reshape the gradient Df to match the dimensions of the pooled output.\n",
        "\n",
        "4. Gradient with Respect to Convolutional Layer Output (DC):\n",
        "\n",
        "5. Calculate the gradient of the loss with respect to the output of the convolutional layer.\n",
        "\n",
        "6. Gradient with Respect to Kernel (DK):\n",
        "\n",
        "Iterate over each element in the kernel and use the chain rule to compute its gradient.\n",
        "\n",
        "This ensures that gradients are propagated back correctly through each layer, allowing for the correct adjustment of parameters (weights and biases) during the training process of the neural network."
      ],
      "metadata": {
        "id": "ZC_BT9cYs3w7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYh-rYRCkuET"
      },
      "outputs": [],
      "source": [
        "def f_getGradient_b(C,DC):\n",
        "    Db = DC[C>0].sum()\n",
        "    return Db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuTD007EkuEU"
      },
      "outputs": [],
      "source": [
        "def f_backwardPass(I,C,f,w,y_hat,y):\n",
        "    Dw = f_getGradient_w(y_hat,y,f)\n",
        "    Dbf = f_getGradient_bf(y_hat,y)\n",
        "    DK,DC = f_getGradient_K(C,I,y_hat,y,w)\n",
        "    Db = f_getGradient_b(C,DC)\n",
        "    return DK,Db,Dw,Dbf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Lq2P0hOkuEU"
      },
      "outputs": [],
      "source": [
        "def f_initParams():\n",
        "    K = 0.01*np.random.randn(5,5)\n",
        "    b = 0.01*np.random.randn()\n",
        "    w = np.squeeze(0.01*np.random.randn(1,256))\n",
        "    bf = 0.01*np.random.randn()\n",
        "    return K,b,w,bf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEhPBI4bkuEV",
        "outputId": "56fa93bf-6df6-4661-f7ae-7bfa7c74e02c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.4920229785037398\n",
            "0.243209155495617\n",
            "0.15288964553136547\n",
            "0.11512223380183137\n",
            "0.09087306899243046\n",
            "0.07531031211479094\n",
            "0.06470764726937782\n",
            "0.05729752408381704\n",
            "0.051654478064740786\n",
            "0.04719946859083668\n",
            "0.04359662849889232\n",
            "0.04056794188721934\n",
            "0.03802546596762972\n",
            "0.03587733059268647\n",
            "0.03405203009207826\n",
            "0.03244757526677582\n",
            "0.031024403919923484\n",
            "0.029752018091007512\n",
            "0.028606511851509826\n",
            "0.027568868555206745\n",
            "0.026623761696283544\n",
            "0.025758693538737613\n",
            "0.02496336530069008\n",
            "0.02422920923141936\n",
            "0.023549035905569685\n",
            "0.022915914406668098\n",
            "0.022323590582331607\n",
            "0.021769937027563663\n",
            "0.021251060744915732\n",
            "0.0207635869665655\n",
            "0.020304574630617984\n",
            "0.019871447996215756\n",
            "0.01946194089732216\n",
            "0.019074050978987237\n",
            "0.018706001881194704\n",
            "0.018355699463212194\n",
            "0.018021189144092755\n",
            "0.01770243160950251\n",
            "0.017398265894446933\n",
            "0.01710764556369455\n",
            "0.0168296247186427\n",
            "0.016563346025295723\n",
            "0.016308030428783798\n",
            "0.01606296828194199\n",
            "0.015827511664852217\n",
            "0.01560106771175928\n",
            "0.015383092793555568\n",
            "0.015173087429734881\n",
            "0.014970591824611076\n",
            "0.0147751819396627\n"
          ]
        }
      ],
      "source": [
        "I = np.random.randint(1,255,(32,32))\n",
        "y = 0\n",
        "K,b,w,bf = f_initParams()\n",
        "for i in range(50):\n",
        "    C,f,y_hat = f_forwardPass(I,K,b,w,bf)\n",
        "    print(y_hat)\n",
        "    DK,Db,Dw,Dbf = f_backwardPass(I,C,f,w,y_hat,y)\n",
        "    alpha = 0.001\n",
        "    K = K - alpha*DK\n",
        "    b = b - alpha*Db\n",
        "    w = w - alpha*Dw\n",
        "    bf = bf - alpha*Dbf\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYwl0A0EkuEZ",
        "outputId": "25e9785b-5e1f-41fe-eeb7-fd2aaef6a62f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.895626736610168"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_hat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIKjwpi4kuEZ",
        "outputId": "a616a6dd-d631-425a-a047-b5441f23ea8d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "256"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZ2oMuMBkuEa",
        "outputId": "c0a88c1e-6565-4ab8-ddeb-e88e30c095d1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.4961399908819838"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_hat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXdJBSpxkuEb",
        "outputId": "e0dfb6cc-7591-4c62-b880-fcbc51d718b6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-2.97547376, -2.61208367, -1.25452159, -0.10679289, -1.12689902],\n",
              "       [-2.60374285, -2.91080737, -2.63396244, -3.35799435, -3.24738384],\n",
              "       [-3.56845218, -3.09257297, -1.02921786, -4.64212009, -2.74420624],\n",
              "       [-3.32659253, -2.67697302, -3.12026933, -3.79688228, -1.79560188],\n",
              "       [-2.45224328, -3.83832392, -1.55289401, -2.82055551, -2.72611557]])"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "DK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AS1G-OTkuEc",
        "outputId": "8cd3d052-4bd3-40b4-ab8f-c4e9f732ebdf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([-0.34803887, -0.81728514, -0.87373002, -1.14537418, -0.59310609,\n",
              "       -0.20351208, -0.72914047, -0.88524479, -0.41872099, -0.41498201,\n",
              "       -0.37638551, -0.73271683, -0.30267096, -0.51834231, -0.77523831,\n",
              "       -0.30602767, -0.47541978, -0.5287802 , -0.39389314, -0.2601277 ,\n",
              "       -1.15820767, -0.16973692, -0.11222836, -0.        , -0.46965639,\n",
              "       -0.75465536, -0.47732463, -0.34511674, -0.30446064, -0.49239171,\n",
              "       -0.40545826, -0.71765816, -0.23753064, -0.23499351, -1.42790353,\n",
              "       -0.17978169, -0.51620326, -1.13837197, -0.25103392, -0.66151841,\n",
              "       -1.21015924, -0.46574599, -0.43922686, -0.37903103, -0.18672564,\n",
              "       -0.43357395, -1.08880907, -0.        , -0.6861942 , -0.58266859,\n",
              "       -0.49492594, -0.40179354, -0.52538117, -0.50736274, -1.15901314,\n",
              "       -0.39998735, -0.22748738, -0.18657371, -0.68763762, -0.        ,\n",
              "       -0.54415166, -0.26445015, -0.66209606, -0.31885034, -0.18546286,\n",
              "       -0.31396223, -0.89413688, -0.55533751, -0.40425145, -0.93037689,\n",
              "       -0.79868628, -0.48441011, -0.35975779, -0.6919888 , -0.4964582 ,\n",
              "       -0.62925026, -1.08212084, -0.58229601, -0.08226466, -0.40732064,\n",
              "       -0.49967268, -0.50396907, -0.48534156, -0.32748795, -0.49214408,\n",
              "       -0.0388831 , -0.47729357, -0.423376  , -0.4703694 , -0.15264267,\n",
              "       -0.75282001, -0.88297674, -0.22543503, -0.60561293, -0.62865227,\n",
              "       -0.04793328, -0.34216996, -1.13817544, -0.56775143, -0.56024257,\n",
              "       -0.73312023, -0.33155966, -0.36223231, -0.7307187 , -0.96041906,\n",
              "       -0.23559464, -0.67419153, -0.75881377, -0.27572134, -0.66503538,\n",
              "       -0.89855134, -0.22327997, -0.19686894, -0.59053428, -0.        ,\n",
              "       -1.00811868, -0.78952826, -0.515134  , -0.46587462, -0.96329019,\n",
              "       -1.02510528, -0.52449331, -0.71891822, -0.19767294, -0.29167654,\n",
              "       -0.08972099, -0.59508678, -0.57014449, -0.73343748, -0.85330007,\n",
              "       -0.50942077, -0.44487296, -0.44616715, -0.53951918, -0.24028061,\n",
              "       -0.26998891, -0.24709156, -1.07668906, -0.29269902, -0.19029594,\n",
              "       -1.04705006, -0.60853525, -0.22220069, -0.41877886, -0.51752949,\n",
              "       -0.9588315 , -0.81089911, -0.7164498 , -0.71108204, -0.20518753,\n",
              "       -0.85248239, -0.214624  , -0.69954589, -0.76850704, -0.64107039,\n",
              "       -0.52414974, -0.19134866, -0.31759293, -0.66596614, -0.16377858,\n",
              "       -0.63648681, -0.17859177, -1.2117078 , -1.37772717, -0.53347928,\n",
              "       -0.75040356, -0.53495596, -0.61044522, -0.74508245, -1.07874703,\n",
              "       -0.75113656, -0.18402508, -0.66717564, -0.21747083, -0.67363453,\n",
              "       -0.77430225, -0.08561174, -0.3662409 , -0.20107899, -0.76480246,\n",
              "       -0.47707945, -0.26708987, -0.71377712, -0.62721932, -0.81756436,\n",
              "       -0.61106867, -0.71590925, -0.31782441, -0.19976453, -0.95697408,\n",
              "       -0.62843286, -0.48071353, -0.09375682, -0.69239711, -0.41839629,\n",
              "       -0.45638528, -0.32304649, -1.11465096, -0.2974555 , -0.21426432,\n",
              "       -0.53033058, -0.37012502, -0.55715697, -0.41565909, -0.28591755,\n",
              "       -0.83378255, -0.55600405, -0.09982022, -0.50985042, -0.83994837,\n",
              "       -0.36780444, -0.64577747, -0.20603587, -0.3443035 , -0.46436854,\n",
              "       -0.87636926, -0.98491447, -0.2983588 , -0.70240831, -0.6073161 ,\n",
              "       -0.66639739, -0.        , -0.        , -0.37948339, -0.23467308,\n",
              "       -0.49169168, -0.60721213, -0.61521733, -0.25792916, -0.50436017,\n",
              "       -0.39641599, -0.84554512, -0.96357743, -0.58247333, -0.97141872,\n",
              "       -0.50868668, -0.53447164, -0.69337092, -0.23469518, -0.62994825,\n",
              "       -0.08568491, -0.24542768, -0.20447445, -0.12108686, -0.14770146,\n",
              "       -0.66341001, -0.6729699 , -0.76349779, -0.51623553, -0.        ,\n",
              "       -0.01641897, -0.78097847, -0.46456612, -0.1752207 , -0.07035359,\n",
              "       -0.        ])"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Dw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3ifXK4ykuEc",
        "outputId": "e4dca400-178f-49b8-9c2c-23a3990d6c4d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-0.020351432784903194"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anNCXOLdkuEd",
        "outputId": "752df789-9133-494a-88d0-9a802c64a961"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-0.1060162056883096"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Dbf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary of the CNN code\n",
        "The provided code snippet is a simplified implementation of a Convolutional Neural Network (CNN) for training with backpropagation.\n",
        "\n",
        "It includes functions for forward propagation, backpropagation, and parameter updates.\n",
        "\n",
        "Here’s a detailed explanation of what each part of the code does:\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "#1. Function Definitions\n",
        "##1.1 f_padd(I, p)\n",
        "\n",
        "###Purpose\n",
        "\n",
        "Adds padding to the input image I.\n",
        "\n",
        "##Parameters\n",
        "I: Input image.\n",
        "\n",
        "p: Padding size.\n",
        "\n",
        "##Operation\n",
        "\n",
        "Adds p rows of zeros to the top and bottom.\n",
        "\n",
        "Adds p columns of zeros to the left and right.\n",
        "\n",
        "Returns the padded image.\n",
        "\n",
        "...............................................................................\n",
        "\n",
        "##1.2 f_conv2d(I, K, p)\n",
        "\n",
        "Performs 2D convolution on the input image I with kernel K and padding p.\n",
        "\n",
        "###Parameters\n",
        "I: Input image.\n",
        "\n",
        "K: Convolutional kernel.\n",
        "\n",
        "p: Padding size.\n",
        "\n",
        "###Operation\n",
        "Pads the input image I.\n",
        "\n",
        "Applies the convolution operation using kernel K.\n",
        "\n",
        "Returns the feature map C.\n",
        "\n",
        "------\n",
        "##1.3 f_ReLU(C)\n",
        "\n",
        "###Purpose\n",
        "\n",
        "Applies the ReLU activation function.\n",
        "\n",
        "###Parameters\n",
        "\n",
        "C: Input feature map.\n",
        "\n",
        "###Operation:\n",
        "Sets all negative values in C to zero.\n",
        "\n",
        "Returns the activated feature map.\n",
        "\n",
        "------\n",
        "##1.4 f_pool(C)\n",
        "\n",
        "###Purpose\n",
        "\n",
        "Performs max pooling on the feature map C.\n",
        "\n",
        "###Parameters\n",
        "\n",
        "C: Input feature map.\n",
        "\n",
        "###Operation\n",
        "Applies 2x2 max pooling.\n",
        "\n",
        "Reduces the dimensions of C by half.\n",
        "\n",
        "Returns the pooled feature map S.\n",
        "\n",
        "------\n",
        "##1.5 f_sigmoid(f, w, bf)\n",
        "\n",
        "###Purpose\n",
        "\n",
        "Computes the output of a sigmoid activation function.\n",
        "\n",
        "###Parameters\n",
        "\n",
        "f: Flattened pooled feature map.\n",
        "\n",
        "w: Weights.\n",
        "\n",
        "bf: Bias for the sigmoid function.\n",
        "\n",
        "###Operation\n",
        "Computes the weighted sum x = w.dot(f) + bf.\n",
        "\n",
        "Applies the sigmoid function to x.\n",
        "\n",
        "Returns the sigmoid output y_hat.\n",
        "\n",
        "------\n",
        "#1.6 f_forwardPass(I, K, b, w, bf)\n",
        "\n",
        "###Purpose\n",
        "\n",
        "Performs forward propagation through the network.\n",
        "\n",
        "###Parameters\n",
        "I: Input image.\n",
        "\n",
        "K: Convolutional kernel.\n",
        "\n",
        "b: Bias for the convolution layer.\n",
        "\n",
        "w: Weights for the sigmoid function.\n",
        "\n",
        "bf: Bias for the sigmoid function.\n",
        "\n",
        "####Operation\n",
        "Applies convolution, adds bias, applies ReLU, and performs pooling.\n",
        "\n",
        "Flattens the pooled feature map and computes the sigmoid activation.\n",
        "\n",
        "Returns the intermediate and final outputs.\n",
        "\n",
        "------\n",
        "##1.7 f_getGradient_w(y_hat, y, f)\n",
        "\n",
        "###Purpose\n",
        "\n",
        "Computes the gradient with respect to weights w.\n",
        "\n",
        "###Parameters\n",
        "y_hat: Predicted output.\n",
        "\n",
        "y: Actual target.\n",
        "\n",
        "f: Flattened pooled feature map.\n",
        "\n",
        "###Operation\n",
        "Calculates the gradient using the chain rule.\n",
        "\n",
        "Returns the gradient with respect to w.\n",
        "\n",
        "------\n",
        "##1.8 f_getGradient_f(y_hat, y, w)\n",
        "###Purpose\n",
        "\n",
        "Computes the gradient with respect to the flattened feature map f.\n",
        "\n",
        "###Parameters\n",
        "y_hat: Predicted output.\n",
        "\n",
        "y: Actual target.\n",
        "\n",
        "w: Weights.\n",
        "\n",
        "###Operation\n",
        "Calculates the gradient using the chain rule.\n",
        "Returns the gradient with respect to f.\n",
        "\n",
        "-----\n",
        "##1.9 f_getGradient_bf(y_hat, y)\n",
        "\n",
        "###Purpose\n",
        "\n",
        "Computes the gradient with respect to the bias bf in the sigmoid function.\n",
        "\n",
        "###Parameters\n",
        "y_hat: Predicted output.\n",
        "\n",
        "y: Actual target.\n",
        "\n",
        "####Operation\n",
        "Calculates the gradient with respect to bf.\n",
        "\n",
        "Returns the gradient.\n",
        "\n",
        "-----\n",
        "##1.10 f_getGradient_S(Df)\n",
        "\n",
        "###Purpose\n",
        "\n",
        "Reshapes the gradient vector Df into the shape of the pooled feature map S.\n",
        "\n",
        "###Parameters\n",
        "Df: Gradient vector.\n",
        "\n",
        "###Operation\n",
        "Reshapes Df into a square matrix.\n",
        "\n",
        "Returns the reshaped gradient matrix DS.\n",
        "\n",
        "-----\n",
        "##1.11 f_getGradient_C(DS, C)\n",
        "###Purpose\n",
        "\n",
        "Computes the gradient with respect to the convolutional layer C from the pooled gradients DS.\n",
        "\n",
        "###Parameters\n",
        "DS: Gradient matrix for the pooled feature map.\n",
        "\n",
        "C: Feature map before pooling.\n",
        "\n",
        "###Operation:\n",
        "Maps gradients from DS back to the locations in C that contributed to the pooling.\n",
        "\n",
        "Returns the gradient with respect to C.\n",
        "\n",
        "------\n",
        "##1.12 f_getChainRuleGradients(C, DC, I, u, v)\n",
        "###Purpose\n",
        "\n",
        "Computes the gradient for each kernel weight using the chain rule.\n",
        "\n",
        "###Parameters\n",
        "C: Feature map before pooling.\n",
        "\n",
        "DC: Gradient matrix for the pooled feature map.\n",
        "\n",
        "I: Input image.\n",
        "\n",
        "u, v: Offsets for the kernel position.\n",
        "\n",
        "###Operation\n",
        "Calculates the gradient contribution from each input pixel to each kernel weight.\n",
        "\n",
        "Returns the gradient for a particular kernel weight.\n",
        "\n",
        "------\n",
        "##1.13 f_getGradient_K(C, I, y_hat, y, w)\n",
        "###Purpose\n",
        "\n",
        "Computes the gradient with respect to the convolutional kernel K.\n",
        "\n",
        "###Parameters\n",
        "C: Feature map before pooling.\n",
        "\n",
        "I: Input image.\n",
        "\n",
        "y_hat: Predicted output.\n",
        "\n",
        "y: Actual target.\n",
        "\n",
        "w: Weights.\n",
        "\n",
        "###Operation\n",
        "Calculates gradients for the kernel using the chain rule.\n",
        "Returns the gradient with respect to K and the gradient matrix DC.\n",
        "\n",
        "-----\n",
        "##1.14 f_getGradient_b(C, DC)\n",
        "###Purpose\n",
        "\n",
        "Computes the gradient with respect to the bias b.\n",
        "\n",
        "###Parameters\n",
        "C: Feature map before pooling.\n",
        "\n",
        "DC: Gradient matrix for the convolutional layer.\n",
        "\n",
        "###Operation:\n",
        "Sums up gradients where C is positive.\n",
        "\n",
        "Returns the gradient with respect to b.\n",
        "\n",
        "------\n",
        "##1.15 f_backwardPass(I, C, f, w, y_hat, y)\n",
        "###Purpose\n",
        "\n",
        "Performs backpropagation to compute gradients for all parameters.\n",
        "\n",
        "###Parameters\n",
        "I: Input image.\n",
        "\n",
        "C: Feature map before pooling.\n",
        "\n",
        "f: Flattened pooled feature map.\n",
        "\n",
        "w: Weights.\n",
        "\n",
        "y_hat: Predicted output.\n",
        "\n",
        "y: Actual target.\n",
        "\n",
        "###Operation:\n",
        "Calls gradient functions to compute gradients for weights, biases, and kernel.\n",
        "\n",
        "Returns gradients for K, b, w, and bf.\n",
        "\n",
        "-------\n",
        "##1.16 f_initParams()\n",
        "###Purpose\n",
        "\n",
        "Initializes parameters for the convolutional layer.\n",
        "\n",
        "###Operation\n",
        "Initializes the convolution kernel K, bias b, weights w, and bias for sigmoid bf with small random values.\n",
        "\n",
        "Returns initialized parameters.\n",
        "\n",
        "-----\n",
        "-----\n",
        "#2. Main Code Execution\n",
        "###Initialization\n",
        "\n",
        "An input image I is randomly generated.\n",
        "\n",
        "Parameters K, b, w, and bf are initialized.\n",
        "\n",
        "###Training Loop\n",
        "\n",
        "Forward Pass: Computes the forward pass through the network, producing output y_hat.\n",
        "\n",
        "Backward Pass: Computes gradients for all parameters using the backward pass.\n",
        "\n",
        "Parameter Update: Updates parameters (K, b, w, bf) using gradient descent with a learning rate alpha.\n",
        "\n",
        "-----\n",
        "-----\n",
        "#Summary\n",
        "\n",
        "This program demonstrates a basic implementation of forward and backward propagation in a simple CNN-like structure. It includes padding, convolution, ReLU activation, pooling, and sigmoid activation.\n",
        "\n",
        "The backward propagation functions calculate gradients needed to update the weights and biases in the network, allowing the model to learn from the data over multiple iterations."
      ],
      "metadata": {
        "id": "--7Mt6PEthYm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7zVnJSdkuEe"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}