{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drpetros11111/AI_Sciencs/blob/CNN/Copy_of_CNN_convolution_and_pooling_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKTL1cpjkuDx"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LS2jsgSkuD_"
      },
      "outputs": [],
      "source": [
        "def f_padd(I,p):\n",
        "    numRows = I.shape[0]\n",
        "    numCols = I.shape[1]\n",
        "    zeroRows = np.zeros((p,numCols))\n",
        "    I = np.vstack((zeroRows,I))\n",
        "    I = np.vstack((I,zeroRows))\n",
        "    zeroCols = np.zeros((numRows+2*p,p))\n",
        "    I = np.hstack((zeroCols,I))\n",
        "    I = np.hstack((I,zeroCols))\n",
        "    return I"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Add Padding Function\n",
        "This function, f_padd, adds padding to an image I with a specified padding width p.\n",
        "\n",
        "p represents the number of rows of zeros that you want to create. Specifically, p determines the vertical extent of the padding. Here’s a detailed explanation:\n",
        "\n",
        "# p:\n",
        "\n",
        "In\n",
        "\n",
        "    zeroRows = np.zeros((p,numCols))\n",
        "\n",
        "IS an integer parameter that specifies\n",
        "\n",
        "#how many rows of zeros should be added as padding.\n",
        "\n",
        "It defines the height of the zero-padding on the top and bottom of the input matrix.\n",
        "\n",
        "#numCols\n",
        "\n",
        "This is the number of columns in the input matrix I, and it determines the width of the zero rows.\n",
        "\n",
        "Thus,\n",
        "\n",
        "    np.zeros((p, numCols))\n",
        "\n",
        "creates a 2D array of shape (p, numCols) filled with zeros, where p is the number of rows and numCols is the number of columns. This array will be used to pad the input matrix I vertically.\n",
        "\n",
        "In summary, p stands for the padding size (in terms of rows) that you want to add to the top and bottom of the input matrix.\n",
        "\n",
        "#Padding is often used in image processing to maintain the dimensions of an image after applying operations like convolution.\n",
        "\n",
        "Let's break down the function step by step:\n",
        "\n",
        "\n",
        "---\n",
        "## Function Breakdown\n",
        "Function Definition and Input Parameters\n",
        "\n",
        "    def f_padd(I, p):\n",
        "\n",
        "##I\n",
        "\n",
        "The input image (a 2D array).\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "##p\n",
        "\n",
        "The padding width (number of rows/columns of zeros to add around the image).\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "##Get the Dimensions of the Input Image\n",
        "\n",
        "    numRows = I.shape[0]\n",
        "    numCols = I.shape[1]\n",
        "\n",
        "##numRows\n",
        "\n",
        "Number of rows in the input image.\n",
        "\n",
        "##numCols\n",
        "\n",
        "Number of columns in the input image.\n",
        "Create Rows of Zeros for Padding\n",
        "\n",
        "    zeroRows = np.zeros((p, numCols))\n",
        "\n",
        "---\n",
        "##zeroRows\n",
        "\n",
        "A 2D array of zeros with p rows and the same number of columns as the input image.\n",
        "\n",
        "This will be used to pad the top and bottom of the image.\n",
        "\n",
        "Add Zero Rows to the Top and Bottom of the Image\n",
        "\n",
        "    I = np.vstack((zeroRows, I))\n",
        "    I = np.vstack((I, zeroRows))\n",
        "\n",
        "---\n",
        "##np.vstack\n",
        "\n",
        "Stacks arrays vertically (row-wise).\n",
        "\n",
        "The first np.vstack adds zeroRows to the top of the image.\n",
        "\n",
        "The second np.vstack adds zeroRows to the bottom of the image.\n",
        "\n",
        "----\n",
        "##Create Columns of Zeros for Padding\n",
        "\n",
        "    zeroCols = np.zeros((numRows + 2 * p, p))\n",
        "\n",
        "\n",
        "##Shape of zeroCols\n",
        "The shape of the zeroCols array is determined by the tuple (numRows + 2 * p, p).\n",
        "\n",
        "    numRows + 2 * p\n",
        "\n",
        "numRows is the number of rows in the original image.\n",
        "\n",
        "    2 * p\n",
        "\n",
        "accounts for the padding added to the top and bottom of the image.\n",
        "\n",
        "Therefore, numRows + 2 * p is the total number of rows in the image after adding the top and bottom padding.\n",
        "\n",
        "    p\n",
        "\n",
        "##p\n",
        "\n",
        "is the number of columns of zeros to be added on each side of the image.\n",
        "\n",
        "Therefore, p is the width of the padding columns to be added to the left and right sides.\n",
        "\n",
        "##Creating the Array of Zeros\n",
        "\n",
        "    zeroCols = np.zeros((numRows + 2 * p, p))\n",
        "\n",
        "np.zeros creates an array filled with zeros.\n",
        "\n",
        "The shape of this array is\n",
        "\n",
        "    (numRows + 2 * p, p)\n",
        "\n",
        "meaning it has numRows + 2 * p rows and p columns.\n",
        "\n",
        "zeroCols is a 2D array created with the np.zeros function. Here’s what each component represents:\n",
        "\n",
        "numRows: This is the number of rows in the original input matrix I.\n",
        "p: This is the padding size, specifying how many rows of zeros will be added to both the top and bottom of the original matrix.\n",
        "So, zeroCols has the following characteristics:\n",
        "\n",
        "Number of Rows: numRows + 2 * p\n",
        "\n",
        "This is the total number of rows in zeroCols. It accounts for the original number of rows in I plus p rows of padding on the top and p rows of padding on the bottom.\n",
        "\n",
        "------------\n",
        "------------\n",
        "#Number of Columns: p\n",
        "\n",
        "This specifies the width of the zero padding. The zero columns matrix has p columns, which will be added to the left and right sides of the original matrix.\n",
        "\n",
        "To summarize, zeroCols is a 2D array with:\n",
        "\n",
        "##Height: numRows + 2 * p (the original height plus padding rows on top and bottom).\n",
        "\n",
        "##Width: p (the width of the padding columns to be added on the left and right).\n",
        "\n",
        "Example\n",
        "If numRows is 3 and p is 2:\n",
        "\n",
        "The height of zeroCols will be 3 + 2 * 2 = 7.\n",
        "\n",
        "The width of zeroCols will be 2.\n",
        "\n",
        "-------------\n",
        "-------------\n",
        "\n",
        "##Visual Example\n",
        "Suppose the original image I has dimensions 3 x 3 (i.e., numRows = 3 and numCols = 3), and we want to add a padding width p = 1.\n",
        "\n",
        "The number of rows in zeroCols will be 3 + 2 * 1 = 5.\n",
        "\n",
        "The number of columns in zeroCols will be 1.\n",
        "\n",
        "So, zeroCols will look like this:\n",
        "\n",
        "    zeroCols = [[0],\n",
        "               [0],\n",
        "               [0],\n",
        "               [0],\n",
        "               [0]]\n",
        "\n",
        "##Applying zeroCols to the Image\n",
        "After creating zeroCols, it will be added to the left and right sides of the padded image (which already has top and bottom padding):\n",
        "\n",
        "    I = np.hstack((zeroCols, I))  # Adds `zeroCols` to the left side\n",
        "    I = np.hstack((I, zeroCols))  # Adds `zeroCols` to the right side\n",
        "\n",
        "##Here's how it works step-by-step\n",
        "\n",
        "Original Image with Top and Bottom\n",
        "##Padding:\n",
        "\n",
        "    [[0, 0, 0],\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 6],\n",
        "    [7, 8, 9],\n",
        "    [0, 0, 0]]\n",
        "\n",
        "##After Adding Left Padding:\n",
        "\n",
        "    [[0, 0, 0, 0],\n",
        "    [0, 1, 2, 3],\n",
        "    [0, 4, 5, 6],\n",
        "    [0, 7, 8, 9],\n",
        "    [0, 0, 0, 0]]\n",
        "\n",
        "##After Adding Right Padding:\n",
        "\n",
        "    [[0, 0, 0, 0, 0],\n",
        "    [0, 1, 2, 3, 0],\n",
        "    [0, 4, 5, 6, 0],\n",
        "    [0, 7, 8, 9, 0],\n",
        "    [0, 0, 0, 0, 0]]\n",
        "\n",
        "##Summary\n",
        "\n",
        "The line\n",
        "\n",
        "    zeroCols = np.zeros((numRows + 2 * p, p))\n",
        "\n",
        "creates a vertical strip of zeros that matches the height of the image after top and bottom padding and has a width of p columns.\n",
        "\n",
        "This allows us to pad the left and right sides of the image, completing the symmetrical padding on all four sides.\n",
        "\n",
        "-----\n",
        "##zeroCols\n",
        "\n",
        "A 2D array of zeros with numRows + 2 * p rows (the height of the image after adding the zero rows) and p columns. This will be used to pad the left and right of the image.\n",
        "\n",
        "Add Zero Columns to the Left and Right of the Image\n",
        "\n",
        "\n",
        "    I = np.hstack((zeroCols, I))\n",
        "    I = np.hstack((I, zeroCols))\n",
        "\n",
        "----\n",
        "##np.hstack: Stacks arrays horizontally (column-wise)\n",
        "\n",
        "The first\n",
        "\n",
        "    np.hstack #adds zeroCols to the left of the image.\n",
        "\n",
        "The second\n",
        "\n",
        "    np.hstack #adds zeroCols to the right of the image.\n",
        "\n",
        "----\n",
        "###Return the Padded Image\n",
        "\n",
        "    return I\n",
        "\n",
        "-----\n",
        "##Summary\n",
        "\n",
        "The function f_padd effectively adds a border of zeros around the input image I with a width specified by p.\n",
        "\n",
        "The result is an image with additional rows and columns of zeros, which can be useful for various image processing tasks. Here's an example to illustrate:\n",
        "\n",
        "---\n",
        "##Example\n",
        "\n",
        "Given an input image I:\n",
        "\n",
        "    I = [[1, 2, 3],\n",
        "        [4, 5, 6],\n",
        "        [7, 8, 9]]\n",
        "\n",
        "If p = 1, the function adds a border of zeros around this image:\n",
        "\n",
        "    I = [[0, 0, 0, 0, 0],\n",
        "        [0, 1, 2, 3, 0],\n",
        "        [0, 4, 5, 6, 0],\n",
        "        [0, 7, 8, 9, 0],\n",
        "        [0, 0, 0, 0, 0]]\n",
        "\n",
        "This padded image has a border of zeros of width p added to the top, bottom, left, and right of the original image."
      ],
      "metadata": {
        "id": "pKctwlNuKBb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "I = np.random.randint(1,5,(5,5))\n",
        "p = 11\n",
        "I2 = f_padd(I,p)\n",
        "print(I2)"
      ],
      "metadata": {
        "id": "diS1kxx9NEFm",
        "outputId": "32bdc947-404b-480e-8007-7353e7654fcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 4. 1. 2. 1. 4. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 1. 1. 4. 4. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 2. 3. 2. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 3. 3. 3. 4. 3. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 4. 3. 2. 4. 4. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Padding\n",
        "This line creates a 5x5 matrix I with random integers between 1 and 4 (inclusive).\n",
        "\n",
        "The np.random.randint function generates random integers, and its parameters are:\n",
        "\n",
        "## 1: The lowest integer (inclusive) to be generated.\n",
        "## 5: The highest integer (exclusive) to be generated.\n",
        "## (5, 5): The shape of the generated matrix."
      ],
      "metadata": {
        "id": "AC_H8xqOOHBy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhF8WB20kuED"
      },
      "outputs": [],
      "source": [
        "def f_conv2d(I,K,p):\n",
        "    fSize = K.shape[0]\n",
        "    I2 = f_padd(I,p)\n",
        "    numRows = I2.shape[0]\n",
        "    numCols = I2.shape[1]\n",
        "\n",
        "    C = np.zeros((numRows-2*p,numCols-2*p))\n",
        "\n",
        "    for i in range(numRows-fSize+1):\n",
        "        for j in range(numCols-fSize+1):\n",
        "            A = I2[i:i+fSize,j:j+fSize]\n",
        "            C[i,j] = (A.flatten()*K.flatten()).sum()\n",
        "    return C\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolution Function\n",
        "This snippet defines a function f_conv2d that performs a 2D convolution operation on an input image I using a kernel K with padding p.\n",
        "\n",
        "The function outputs the result of the convolution.\n",
        "\n",
        "Here's a detailed explanation of each part of the code:\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "## Function Definition\n",
        "\n",
        "    def f_conv2d(I, K, p):\n",
        "\n",
        "This line defines the function f_conv2d which takes three parameters:\n",
        "\n",
        "###I\n",
        "\n",
        "the input image (a 2D numpy array).\n",
        "\n",
        "###K\n",
        "\n",
        "the kernel (a smaller 2D numpy array).\n",
        "\n",
        "###p\n",
        "\n",
        "the padding size (an integer).\n",
        "\n",
        "----\n",
        "## Filter Size and Padded Image\n",
        "\n",
        "    fSize = K.shape[0]\n",
        "    I2 = f_padd(I, p)\n",
        "\n",
        "###fSize = K.shape[0]\n",
        "\n",
        "This line gets the size of the kernel (assuming it's a square matrix) by accessing the number of rows (or columns, since it's square).\n",
        "\n",
        "###I2 = f_padd(I, p)\n",
        "\n",
        "This line calls the f_padd function (assumed to be defined elsewhere) to pad the input image I with p rows and columns of zeros around the border, resulting in a padded image I2.\n",
        "\n",
        "----\n",
        "###Shape of the Padded Image\n",
        "\n",
        "    numRows = I2.shape[0]\n",
        "    numCols = I2.shape[1]\n",
        "\n",
        "numRows and numCols store the dimensions of the padded image I2.\n",
        "\n",
        "----\n",
        "##Initialize Output Matrix\n",
        "\n",
        "    C = np.zeros((numRows-2*p, numCols-2*p))\n",
        "\n",
        "###Explanation\n",
        "\n",
        "    C = np.zeros((numRows-2*p, numCols-2*p))\n",
        "\n",
        "\n",
        "###C = np.zeros((numRows-2*p, numCols-2*p))\n",
        "\n",
        "The line initializes the output matrix C, which will store the results of the convolution operation.\n",
        "\n",
        "####Shape Calculation\n",
        "\n",
        "numRows and numCols are the dimensions of the padded image I2.\n",
        "\n",
        "2*p accounts for the padding added to both sides (top and bottom for rows, left and right for columns).\n",
        "\n",
        "    numRows - 2*p and numCols - 2*p\n",
        "    \n",
        "give the dimensions of the original image I before padding.\n",
        "\n",
        "##Why Subtract 2*p?\n",
        "When you pad an image with p rows/columns of zeros, you add p zeros to each side:\n",
        "\n",
        "The total number of additional rows is\n",
        "\n",
        "    2*p (i.e., p rows at the top + p rows at the bottom).\n",
        "\n",
        "The total number of additional columns is 2*p (i.e., p columns on the left + p columns on the right).\n",
        "\n",
        "\n",
        "###Ensuring the Correct Output Size\n",
        "The original image I has dimensions (originalNumRows, originalNumCols).\n",
        "\n",
        "\n",
        "After padding, the padded image I2 has dimensions (originalNumRows + 2*p, originalNumCols + 2*p).\n",
        "\n",
        "\n",
        "During convolution, the valid output size should match the original image size because the padding ensures the kernel fits within the image boundaries, producing an output of the same dimensions as the input.\n",
        "\n",
        "Thus, the output matrix C is initialized to have the same dimensions as the original image, which is   \n",
        "\n",
        "    (numRows - 2*p, numCols - 2*p).\n",
        "\n",
        "##Visual Example\n",
        "Let's say your original image I is 3x3, and you pad it with p = 1.\n",
        "\n",
        "The padded image I2 will be 5x5 (3 + 2*1).\n",
        "\n",
        "To store the convolution results, you need a matrix C of the same size as the original image (3x3), hence C = np.zeros((5-2*1, 5-2*1)) becomes C = np.zeros((3, 3)).\n",
        "\n",
        "The line\n",
        "\n",
        "    C = np.zeros((numRows-2*p, numCols-2*p))\n",
        "\n",
        "ensures that the output matrix C has the correct dimensions to store the results of the convolution operation, excluding the padded borders.\n",
        "\n",
        "This way, C will have the same dimensions as the original input image I.\n",
        "###C\n",
        "\n",
        "the output matrix (or convolved image) is initialized with zeros. Its size is reduced by 2*p in both dimensions because the padding doesn't contribute to the valid convolution region.\n",
        "\n",
        "----\n",
        "----\n",
        "##Perform Convolution\n",
        "\n",
        "    for i in range(numRows - fSize + 1):\n",
        "        for j in range(numCols - fSize + 1):\n",
        "            A = I2[i:i+fSize, j:j+fSize]\n",
        "            C[i, j] = (A.flatten() * K.flatten()).sum()\n",
        "\n",
        "----\n",
        "##The outer for loop\n",
        "\n",
        "iterates over the rows of the padded image I2, stopping at\n",
        "\n",
        "    numRows - fSize + 1\n",
        "\n",
        "to ensure the kernel K doesn't go out of bounds.\n",
        "\n",
        "------\n",
        "------\n",
        "###Why Add 1?\n",
        "The +1 in numRows - fSize + 1 accounts for the starting position being inclusive.\n",
        "\n",
        "#Without +1, the filter would not cover all valid starting positions.\n",
        "\n",
        " For a 2x2 filter on a 5x5 image, it can start from position 0, 1, 2, or 3, which is 4 valid positions (5 - 2 + 1 = 4).\n",
        "\n",
        "    numRows - fSize + 1\n",
        "\n",
        " ensures the loop covers all valid positions from top-left to bottom-right.\n",
        "\n",
        "The kernel slides across every possible position where it fits entirely within the padded image.\n",
        "\n",
        "The padding ensures the kernel can also slide over the boundary regions of the original image.\n",
        "\n",
        "Adding +1 in numRows - fSize + 1 ensures that we include the last valid position, considering the inclusive nature of Python's range function.\n",
        "\n",
        "##Example\n",
        "Let's go through a practical example with a small matrix and filter to clarify why the +1 is necessary.\n",
        "\n",
        "##Input Matrix and Filter\n",
        "Consider a 4x4 input matrix I and a 2x2 filter K:\n",
        "\n",
        "##Input Matrix I:\n",
        "\n",
        "    1 2 3 4\n",
        "    5 6 7 8\n",
        "    9 10 11 12\n",
        "    13 14 15 16\n",
        "\n",
        "##Filter K:\n",
        "\n",
        "    1 0\n",
        "    0 1\n",
        "\n",
        "##Convolution without Padding\n",
        "When performing a convolution operation with a 2x2 filter on a 4x4 matrix without padding, you need to slide the filter over the matrix.\n",
        "\n",
        "To determine how many valid positions there are for the filter to be placed such that it fits within the matrix boundaries, you use the formula:\n",
        "\n",
        "#Number of valid positions (or output size) in each dimension:\n",
        "\n",
        "    numRows - fSize + 1\n",
        "\n",
        "where numRows is the dimension of the input matrix, and fSize is the dimension of the filter.\n",
        "\n",
        "For our 4x4 matrix and 2x2 filter:\n",
        "\n",
        "    numRows = 4\n",
        "\n",
        "    fSize = 2\n",
        "\n",
        "So:\n",
        "\n",
        "$$numRows - fSize + 1 = 4 - 2 + 1 = 3$$\n",
        "\n",
        "This means there are 3 possible positions for the filter to be placed in each dimension (both horizontally and vertically). Let's visualize this:\n",
        "\n",
        "#Position 1:\n",
        "\n",
        "    1 2\n",
        "    5 6\n",
        "\n",
        "#Position 2:\n",
        "\n",
        "    2 3\n",
        "    6 7\n",
        "\n",
        "#Position 3:\n",
        "\n",
        "    3 4\n",
        "    7 8\n",
        "\n",
        "In each case, the filter is positioned such that it fits within the 4x4 matrix boundaries.\n",
        "\n",
        "#Why Add +1?\n",
        "The +1 in numRows - fSize + 1 is necessary because the filter needs to cover every valid starting position.\n",
        "\n",
        "Without adding +1, the calculation would miss the last possible position where the filter can fully fit.\n",
        "\n",
        "#For instance, without +1:\n",
        "\n",
        "#numRows - fSize = 4 - 2 = 2\n",
        "\n",
        "This suggests there are only 2 valid positions in each dimension.\n",
        "\n",
        "But this is incorrect because it doesn't account for the starting position being inclusive, which means the last position also needs to be considered.\n",
        "\n",
        "By adding +1, the calculation correctly includes the final valid starting position where the filter is still entirely within the matrix boundaries.\n",
        "\n",
        "#Summary\n",
        "In essence, the +1 ensures that the convolution operation covers all positions where the filter can fully overlap with the input matrix, including the edge cases where the filter is placed at the last valid position. This adjustment guarantees that no valid starting position is omitted.\n",
        "\n",
        "----\n",
        "----\n",
        "###The inner for loop\n",
        "\n",
        "iterates over the columns, similarly stopping at numCols - fSize + 1.\n",
        "\n",
        "-----\n",
        "###A = I2[i:i+fSize, j:j+fSize]\n",
        "\n",
        "This line extracts a submatrix A from the padded image I2 starting at (i, j) with the same size as the kernel K.\n",
        "\n",
        "###Variables\n",
        "I2: The padded input matrix.\n",
        "\n",
        "K: The convolution kernel.\n",
        "\n",
        "fSize: The size of the kernel, assumed to be square (e.g., for a 3x3 kernel, fSize would be 3).\n",
        "\n",
        "numRows: The number of rows in the padded input matrix I2.\n",
        "\n",
        "numCols: The number of columns in the padded input matrix I2.\n",
        "\n",
        "C: The output matrix that will store the result of the convolution operation.\n",
        "\n",
        "------\n",
        "###Nested Loops\n",
        "\n",
        "    for i in range(numRows - fSize + 1):\n",
        "       for j in range(numCols - fSize + 1):\n",
        "\n",
        "These nested loops iterate over the padded input matrix I2 to perform the convolution operation.\n",
        "\n",
        "The loops run such that the kernel K can slide over the entire I2 matrix without going out of bounds.\n",
        "\n",
        "-----\n",
        "###Extracting the Submatrix A\n",
        "\n",
        "    A = I2[i:i+fSize, j:j+fSize]\n",
        "\n",
        "Purpose: This line extracts a submatrix A from I2 starting at the position (i, j) and spanning a region of size fSize x fSize.\n",
        "-----\n",
        "Explanation:\n",
        "    i:i+fSize\n",
        "\n",
        "This range selects rows from i to i+fSize-1 (inclusive).\n",
        "\n",
        "j:j+fSize: This range selects columns from j to j+fSize-1 (inclusive).\n",
        "\n",
        "For example, if i = 2, j = 3, and fSize = 3, then:\n",
        "\n",
        "The range 2:2+3 corresponds to rows 2, 3, and 4.\n",
        "\n",
        "The range 3:3+3 corresponds to columns 3, 4, and 5.\n",
        "\n",
        "So, A will be the 3x3 submatrix of I2 starting from row 2 and column 3.\n",
        "\n",
        "-----\n",
        "###Performing the Convolution Operation\n",
        "\n",
        "    C[i, j] = (A.flatten() * K.flatten()).sum()\n",
        "\n",
        "Flattening: A.flatten() and K.flatten() convert the 2D matrices A and K into 1D arrays.\n",
        "\n",
        "Element-wise Multiplication: The element-wise multiplication of these flattened arrays is performed.\n",
        "\n",
        "Summation: The sum() function computes the sum of the resulting array from the element-wise multiplication.\n",
        "\n",
        "The result of this operation is stored in C[i, j]. This effectively computes the dot product of the kernel K with the submatrix A.\n",
        "\n",
        "------\n",
        "###Full Code Explanation\n",
        "\n",
        "    for i in range(numRows - fSize + 1):\n",
        "       for j in range(numCols - fSize + 1):\n",
        "           A = I2[i:i+fSize, j:j+fSize]  # Extract submatrix\n",
        "           C[i, j] = (A.flatten() * K.flatten()).sum()  # Perform convolution\n",
        "\n",
        "###Step-by-Step:\n",
        "Iterate over all possible positions (i, j) in the padded input matrix I2 where the kernel K can be applied.\n",
        "\n",
        "For each position (i, j), extract the submatrix A of the same size as the kernel K.\n",
        "\n",
        "Perform element-wise multiplication between the flattened A and K, and sum the results.\n",
        "\n",
        "Store this sum in the corresponding position (i, j) of the output matrix C.\n",
        "\n",
        "-----\n",
        "###Intuitive Example\n",
        "Assume we have:\n",
        "\n",
        "Padded Input Matrix I2 of size 5x5.\n",
        "Kernel K of size 3x3.\n",
        "\n",
        "For a particular iteration with i = 1 and j = 1:\n",
        "\n",
        "A will be the 3x3 submatrix starting from I2[1, 1]:\n",
        "\n",
        "    [[I2[1, 1], I2[1, 2], I2[1, 3]],\n",
        "    [I2[2, 1], I2[2, 2], I2[2, 3]],\n",
        "    [I2[3, 1], I2[3, 2], I2[3, 3]]]\n",
        "\n",
        "###Flatten A and K:\n",
        "\n",
        "    A.flatten() = [I2[1, 1], I2[1, 2], I2[1, 3], I2[2, 1], I2[2, 2], I2[2, 3], I2[3, 1], I2[3, 2], I2[3, 3]]\n",
        "\n",
        "    K.flatten() = [K[0, 0], K[0, 1], K[0, 2], K[1, 0], K[1, 1], K[1, 2], K[2, 0], K[2, 1], K[2, 2]]\n",
        "\n",
        "-----\n",
        "###Perform element-wise multiplication and sum:\n",
        "\n",
        "   C[1, 1] = sum(A.flatten() * K.flatten())\n",
        "\n",
        "This approach systematically applies the convolution operation to every valid position in the input matrix, producing the output matrix C.\n",
        "---\n",
        "\n",
        "    C[i, j] = (A.flatten() * K.flatten()).sum()\n",
        "\n",
        "This line performs element-wise multiplication of the flattened versions of A and K, then sums the result to get a single scalar value, which is assigned to the corresponding position (i, j) in the output matrix C.\n",
        "\n",
        "#Convolution and Pooling Operations\n",
        "##Convolution (Sum of Element-wise Products)\n",
        "\n",
        "This is used to filter the image or feature map and produce a new feature map based on the kernel’s weights.\n",
        "\n",
        "\n",
        "##Max Pooling\n",
        "\n",
        "Extracts the maximum value from each region and is used to reduce dimensionality.\n",
        "\n",
        "##Average Pooling\n",
        "\n",
        "Computes the average value from each region and is also used for dimensionality reduction.\n",
        "\n",
        "    C[i, j] = (A.flatten() * K.flatten()).sum()\n",
        "\n",
        "is performing a standard convolution operation where:\n",
        "\n",
        "    A.flatten(): Flattens the submatrix to a 1D array.\n",
        "    K.flatten(): Flattens the filter to a 1D array.\n",
        "\n",
        "##(A.flatten() * K.flatten()).sum()\n",
        "\n",
        "Multiplies corresponding elements and sums them up to get a single result.\n",
        "This result reflects how the filter K is applied to the submatrix A, which is standard in convolution operations used in various image processing and neural network tasks.\n",
        "\n",
        "If you need results based on pooling operations like max or average, you would use different operations than convolution.\n",
        "\n",
        "----\n",
        "----\n",
        "##Return the Convolution Result\n",
        "\n",
        "    return C\n",
        "\n",
        "The function returns the convolved image C.\n",
        "\n",
        "-----\n",
        "##Summary\n",
        "The function f_conv2d performs the following steps:\n",
        "\n",
        "Pads the input image I with p zeros on all sides.\n",
        "\n",
        "Initializes an output matrix C to store the results of the convolution.\n",
        "\n",
        "Iterates over the padded image to perform the convolution operation, extracting submatrices of the same size as the kernel, performing element-wise multiplication, summing the results, and storing them in the corresponding positions in C.\n",
        "\n",
        "Returns the convolved image C.\n",
        "\n",
        "----\n",
        "##Example Usage\n",
        "\n",
        "\n",
        "    import numpy as np\n",
        "\n",
        "# Define a simple input image and kernel\n",
        "    I = np.array([[1, 2, 3],\n",
        "                 [4, 5, 6],\n",
        "                 [7, 8, 9]])\n",
        "\n",
        "    K = np.array([[1, 0],\n",
        "                 [0, -1]])\n",
        "\n",
        "# Define padding size\n",
        "    p = 1\n",
        "\n",
        "# Perform the convolution\n",
        "    result = f_conv2d(I, K, p)\n",
        "\n",
        "    print(result)\n",
        "\n",
        "This code snippet should help you understand how the convolution operation is performed using the f_conv2d function."
      ],
      "metadata": {
        "id": "4CK6jLtXbIrL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqMTr-k-kuEF"
      },
      "outputs": [],
      "source": [
        "def f_ReLU(C):\n",
        "    C[C<0] = 0\n",
        "    return C"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RELU Function Definition\n",
        "\n",
        "    def f_ReLU(C):\n",
        "       C[C < 0] = 0\n",
        "       return C\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Purpose of the Function\n",
        "This function implements the ReLU (Rectified Linear Unit) activation function.\n",
        "\n",
        "The ReLU function is commonly used in neural networks, particularly in deep learning, because it introduces non-linearity to the model and helps mitigate the vanishing gradient problem.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## How the Function Works\n",
        "\n",
        "###Input:\n",
        "\n",
        "The function takes a single input parameter C, which is typically a NumPy array.\n",
        "\n",
        "This array represents the output of a layer in a neural network before applying the activation function.\n",
        "\n",
        "----\n",
        "##ReLU Activation\n",
        "\n",
        "The ReLU function sets all negative values in the input array C to zero. This is achieved using the line C[C < 0] = 0.\n",
        "\n",
        "C < 0 creates a boolean array where each element is True if the corresponding element in C is less than zero, and False otherwise.\n",
        "\n",
        "C[C < 0] = 0 uses this boolean array to index into C and set all elements where the condition is True to zero.\n",
        "\n",
        "----\n",
        "##Return\n",
        "\n",
        "The modified array C, with all negative values replaced by zeros, is then returned.\n",
        "\n",
        "----\n",
        "##Example\n",
        "Let's look at a simple example to see how it works:\n",
        "\n",
        "    import numpy as np\n",
        "\n",
        "# Example input array\n",
        "    C = np.array([[1, -2, 3], [-4, 5, -6]])\n",
        "\n",
        "# Applying the ReLU function\n",
        "    C_relu = f_ReLU(C)\n",
        "\n",
        "    print(C_relu)\n",
        "\n",
        "----\n",
        "##Output\n",
        "\n",
        "    [[1 0 3]\n",
        "    [0 5 0]]\n",
        "\n",
        "-----\n",
        "##Explanation of the Example\n",
        "###Input Array C:\n",
        "\n",
        "    [[ 1, -2,  3],\n",
        "    [-4,  5, -6]]\n",
        "\n",
        "-----\n",
        "##Applying ReLU\n",
        "\n",
        "Positive values remain unchanged.\n",
        "\n",
        "Negative values are set to zero.\n",
        "\n",
        "-----\n",
        "##Output Array C_relu\n",
        "\n",
        "    [[1, 0, 3],\n",
        "    [0, 5, 0]]\n",
        "\n",
        "-----\n",
        "##Summary\n",
        "ReLU Activation Function: The function f_ReLU is a straightforward implementation of the ReLU activation function.\n",
        "\n",
        "It modifies the input array C in-place by setting all negative values to zero and returns the modified array.\n",
        "\n",
        "Use in Neural Networks: ReLU is widely used in neural networks because it helps to introduce non-linearity, which is crucial for learning complex patterns, and it mitigates the vanishing gradient problem by ensuring that gradients do not become too small during backpropagation.\n",
        "\n",
        "This function is essential in the context of neural networks and deep learning, where activation functions play a critical role in the performance and convergence of the model."
      ],
      "metadata": {
        "id": "gskiBAa8Hg8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f_pool(C):\n",
        "    r = C.shape[0]\n",
        "    c = C.shape[1]\n",
        "    S = np.zeros((int(r//2), int(c//2)))  # Corrected np.zeros usage\n",
        "    for i in range(0, r, 2):\n",
        "        for j in range(0, c, 2):\n",
        "            # Ensure not to go out of bounds\n",
        "            S[int(i/2), int(j/2)] = C[i:i+2, j:j+2].max()\n",
        "    return S"
      ],
      "metadata": {
        "id": "V0NCTHRsbPUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pooling Function\n",
        "\n",
        "    def f_pool(C):\n",
        "This defines a function f_pool that takes a single argument C.\n",
        "\n",
        "-----\n",
        "##Shape of Input Matrix\n",
        "\n",
        "    r = C.shape[0]\n",
        "    c = C.shape[1]\n",
        "\n",
        "r and c store the number of rows and columns of the input matrix C, respectively.\n",
        "\n",
        "------\n",
        "##Initialize Output Matrix\n",
        "\n",
        "    S = np.zeros((int(r//2), int(c//2)))  # Corrected np.zeros usage\n",
        "\n",
        "S is the output matrix that will store the results of the max pooling operation.\n",
        "\n",
        "It has dimensions (r//2, c//2) because max pooling with a 2x2 filter reduces each dimension by half.\n",
        "\n",
        "------\n",
        "##Iterate Over the Input Matrix\n",
        "\n",
        "    for i in range(0, r, 2):\n",
        "       for j in range(0, c, 2):\n",
        "\n",
        "These nested loops iterate over the input matrix C with a stride of 2, ensuring that each 2x2 block is processed.\n",
        "\n",
        "-----\n",
        "##Max Pooling Operation\n",
        "\n",
        "    S[int(i/2), int(j/2)] = C[i:i+2, j:j+2].max()\n",
        "\n",
        "For each position (i, j) in the input matrix C, a 2x2 block is selected starting at (i, j).\n",
        "\n",
        "C[i:i+2, j:j+2] extracts this 2x2 block.\n",
        ".max() finds the maximum value in this 2x2 block.\n",
        "\n",
        "This maximum value is assigned to the corresponding position in the output matrix S.\n",
        "\n",
        "The position in S is determined by int(i/2) and int(j/2), effectively downsampling the coordinates by a factor of 2.\n",
        "\n",
        "----\n",
        "##Return the Result:\n",
        "\n",
        "    return S\n",
        "\n",
        "The function returns the resulting matrix S, which contains the max-pooled values of the input matrix C.\n",
        "\n",
        "-----\n",
        "##Example to Demonstrate\n",
        "Let's consider a small example to see how it works in practice:\n",
        "\n",
        "    import numpy as np\n",
        "\n",
        "    def f_pool(C):\n",
        "       r = C.shape[0]\n",
        "       c = C.shape[1]\n",
        "       S = np.zeros((int(r//2), int(c//2)))  # Corrected np.zeros usage\n",
        "       for i in range(0, r, 2):\n",
        "           for j in range(0, c, 2):\n",
        "              # Ensure not to go out of bounds\n",
        "             S[int(i/2), int(j/2)] = C[i:i+2, j:j+2].max()\n",
        "    return S\n",
        "\n",
        "# Example input matrix\n",
        "C = np.array([\n",
        "    [1, 3, 2, 4],\n",
        "    [5, 6, 7, 8],\n",
        "    [9, 10, 11, 12],\n",
        "    [13, 14, 15, 16]\n",
        "])\n",
        "\n",
        "# Perform max pooling\n",
        "    S = f_pool(C)\n",
        "    print(S)\n",
        "\n",
        "Output:\n",
        "\n",
        "[[ 6.  8.]\n",
        " [14. 16.]]\n",
        "\n",
        "\n",
        "Explanation of Example:\n",
        "\n",
        "\n",
        "The input matrix C:\n",
        "\n",
        "[[ 1,  3,  2,  4],\n",
        " [ 5,  6,  7,  8],\n",
        " [ 9, 10, 11, 12],\n",
        " [13, 14, 15, 16]]\n",
        "\n",
        "The max pooling operation will divide C into four 2x2 blocks and take the maximum value from each block:\n",
        "\n",
        "Block 1: [[1, 3], [5, 6]] → max value is 6\n",
        "Block 2: [[2, 4], [7, 8]] → max value is 8\n",
        "Block 3: [[9, 10], [13, 14]] → max value is 14\n",
        "Block 4: [[11, 12], [15, 16]] → max value is 16\n",
        "\n",
        "These values are placed in the output matrix S:\n",
        "\n",
        "[[ 6, 8],\n",
        " [14, 16]]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0dkJN4vkdheD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iy0oBac-kuEI"
      },
      "outputs": [],
      "source": [
        "def f_sigmoid(f,w,bf):\n",
        "    x = w.dot(f)+bf\n",
        "    y_hat = 1/(1+np.exp(-x))\n",
        "    return y_hat"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sigmoid Function Definition\n",
        "\n",
        "    def f_sigmoid(f, w, bf):\n",
        "       x = w.dot(f) + bf\n",
        "       y_hat = 1 / (1 + np.exp(-x))\n",
        "       return y_hat\n",
        "\n",
        "------\n",
        "## Purpose of the Function\n",
        "This function calculates the output of a single-layer neural network using the sigmoid activation function.\n",
        "\n",
        "The sigmoid function maps any real-valued number into the range (0, 1), making it useful for binary classification problems.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Parameters\n",
        "###f\n",
        "\n",
        "This is typically a vector representing the input features.\n",
        "\n",
        "###w\n",
        "\n",
        "This is a weight vector that is used to scale the input features.\n",
        "\n",
        "###bf\n",
        "\n",
        "This is a bias term that is added after the weighted sum of the input features.\n",
        "\n",
        "-----\n",
        "##Steps in the Function\n",
        "###Weighted Sum with Bias\n",
        "\n",
        "    x = w.dot(f) + bf\n",
        "\n",
        "###w.dot(f)\n",
        "\n",
        "This performs the dot product between the weight vector w and the input feature vector f. The dot product is a single number that represents the weighted sum of the input features.\n",
        "\n",
        "###+ bf\n",
        "\n",
        "This adds the bias term bf to the weighted sum. The bias helps in adjusting the output along with the weights.\n",
        "\n",
        "-----\n",
        "##Sigmoid Activation\n",
        "\n",
        "    y_hat = 1 / (1 + np.exp(-x))\n",
        "\n",
        "###np.exp(-x)\n",
        "\n",
        "This calculates the exponential of -x.\n",
        "\n",
        "###1 / (1 + np.exp(-x))\n",
        "\n",
        "This applies the sigmoid function to x. The sigmoid function squashes the input x into a value between 0 and 1.\n",
        "\n",
        "----\n",
        "##Return\n",
        "\n",
        "    return y_hat\n",
        "\n",
        "The function returns y_hat, which is the output of the sigmoid activation function.\n",
        "\n",
        "This value is interpreted as the probability or confidence level of the input belonging to a certain class (e.g., class 1 in binary classification).\n",
        "\n",
        "-----\n",
        "##Example\n",
        "Let's look at a simple example to see how it works:\n",
        "\n",
        "    import numpy as np\n",
        "\n",
        "# Example input\n",
        "    f = np.array([0.5, 1.5])  # Input feature vector\n",
        "    w = np.array([0.3, 0.7])  # Weight vector\n",
        "    bf = 0.1                  # Bias term\n",
        "\n",
        "# Applying the sigmoid function\n",
        "    y_hat = f_sigmoid(f, w, bf)\n",
        "\n",
        "    print(y_hat)\n",
        "\n",
        "#Output\n",
        "\n",
        "0.7858349830425586\n",
        "\n",
        "#Explanation of the Example\n",
        "Input Feature Vector f: [0.5, 1.5]\n",
        "\n",
        "Weight Vector w: [0.3, 0.7]\n",
        "\n",
        "Bias Term bf: 0.1\n",
        "\n",
        "Weighted Sum Calculation:\n",
        "\n",
        "w.dot(f) = 0.3*0.5 + 0.7*1.5 = 0.15 + 1.05 = 1.2\n",
        "\n",
        "Adding bias: x = 1.2 + 0.1 = 1.3\n",
        "\n",
        "Sigmoid Activation Calculation:\n",
        "\n",
        "np.exp(-1.3) ≈ 0.27253179\n",
        "\n",
        "y_hat = 1 / (1 + 0.27253179) ≈ 0.7858349830425586\n",
        "\n",
        "Output y_hat: 0.7858349830425586\n",
        "\n",
        "This value indicates a high confidence that the input belongs to the positive class (close to 1).\n",
        "\n",
        "----\n",
        "##Summary\n",
        "Function Purpose: The f_sigmoid function calculates the output of a single-layer neural network using the sigmoid activation function.\n",
        "\n",
        "Weighted Sum with Bias: It first computes the weighted sum of the input features and adds the bias term.\n",
        "\n",
        "Sigmoid Activation: It then applies the sigmoid activation function to squash the result into a range between 0 and 1.\n",
        "\n",
        "Output: The output is the activated value, representing the predicted probability or confidence level for the input."
      ],
      "metadata": {
        "id": "a3q7liHrJ0K1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwf-54rYkuEK"
      },
      "outputs": [],
      "source": [
        "def f_forwardPass(I,K,b,w,bf):\n",
        "    p = int(K.shape[0]/2)\n",
        "    C = f_conv2d(I,K,p)\n",
        "    C = C+b\n",
        "    C = f_ReLU(C)\n",
        "    S = f_pool(C)\n",
        "    f = S.flatten()\n",
        "    y_hat = f_sigmoid(f,w,bf)\n",
        "    return C,f,y_hat"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Function Definition\n",
        "\n",
        "This function performs a forward pass through a convolutional neural network (CNN) layer, followed by a fully connected layer.\n",
        "\n",
        "The forward pass involves convolution, adding bias, applying the ReLU activation function, pooling, flattening the result, and finally using a sigmoid activation to produce the output.\n",
        "\n",
        "----\n",
        "##Parameters\n",
        "\n",
        "I: The input image or feature map.\n",
        "\n",
        "K: The convolutional kernel (filter).\n",
        "\n",
        "b: The bias term for the convolutional layer.\n",
        "\n",
        "w: The weight vector for the fully connected layer.\n",
        "\n",
        "bf: The bias term for the fully connected layer.\n",
        "\n",
        "-----\n",
        "##Steps in the Function\n",
        "###Padding Calculation:\n",
        "\n",
        "    p = int(K.shape[0] / 2)\n",
        "\n",
        "This calculates the amount of padding needed. For a kernel of size fSize, padding of fSize/2 ensures that the output feature map has the same spatial dimensions as the input.\n",
        "\n",
        "----\n",
        "----\n",
        "Let's break down and explain the purpose of this line of code:\n",
        "\n",
        "    p = int(K.shape[0] / 2)\n",
        "\n",
        "##Purpose of the Line\n",
        "This line calculates the amount of padding required to ensure that the output feature map has the same spatial dimensions (height and width) as the input image after the convolution operation.\n",
        "\n",
        "##Explanation\n",
        "###Kernel Shape\n",
        "\n",
        "    K.shape[0]\n",
        "\n",
        "K is the convolutional kernel (or filter), and K.shape[0] gives the height (or number of rows) of the kernel.\n",
        "\n",
        "Assuming the kernel is square, K.shape[0] is equal to K.shape[1].\n",
        "\n",
        "###Padding Calculation:\n",
        "\n",
        "    K.shape[0] / 2\n",
        "\n",
        "This expression calculates half the height of the kernel. For example, if the kernel is of size 3x3, K.shape[0] is 3, and 3 / 2 gives 1.5.\n",
        "Integer Conversion:\n",
        "\n",
        "    int(K.shape[0] / 2)\n",
        "\n",
        "The result of the division is converted to an integer using int(). This rounds down the result to the nearest whole number. So, if the kernel size is 3, 1.5 becomes 1.\n",
        "\n",
        "----\n",
        "-----\n",
        "\n",
        "##Example\n",
        "Consider a few different kernel sizes to see how padding is calculated:\n",
        "\n",
        "###Kernel Size 3x3\n",
        "    K.shape[0] = 3\n",
        "    K.shape[0] / 2 = 1.5\n",
        "    int(3 / 2) = 1\n",
        "    p = 1\n",
        "\n",
        "##Kernel Size 5x5\n",
        "    K.shape[0] = 5\n",
        "    K.shape[0] / 2 = 2.5\n",
        "    int(5 / 2) = 2\n",
        "    p = 2\n",
        "\n",
        "----\n",
        "##Purpose of Padding\n",
        "Padding is used to control the spatial dimensions of the output feature map after applying the convolution operation.\n",
        "\n",
        "By padding the input image appropriately, you ensure that the output feature map has the same height and width as the input image.\n",
        "\n",
        "----\n",
        "##Why Padding?\n",
        "Let's take an example to understand why padding is necessary:\n",
        "\n",
        "###Without Padding\n",
        "Assume you have a 5x5 input image and a 3x3 kernel.\n",
        "\n",
        "If you perform a convolution without padding, the output feature map will be smaller than the input image:\n",
        "\n",
        "###Input Image: 5x5\n",
        "###Kernel: 3x3\n",
        "\n",
        "    Output Feature Map: (5 - 3 + 1) x (5 - 3 + 1) = 3x3\n",
        "\n",
        "This reduction in size occurs because the kernel cannot be applied to the borders of the input image without going out of bounds.\n",
        "\n",
        "##With Padding\n",
        "To maintain the original size of the input image, you can add padding around the borders:\n",
        "\n",
        "    Padding: 1 (for a 3x3 kernel)\n",
        "    Input Image after Padding: 7x7 (5 original + 2 padding)\n",
        "    Kernel: 3x3\n",
        "\n",
        "Output Feature Map: (7 - 3 + 1) x (7 - 3 + 1) = 5x5\n",
        "\n",
        "By adding 1 pixel of padding around the borders, the output feature map retains the same height and width as the input image.\n",
        "----\n",
        "##Conclusion\n",
        "The line p = int(K.shape[0] / 2) calculates the amount of padding needed to ensure that the convolution operation does not reduce the spatial dimensions of the input image.\n",
        "\n",
        "This is particularly useful in convolutional neural networks (CNNs) where maintaining the input size across layers is often desirable.\n",
        "\n",
        "----\n",
        "-----\n",
        "##Convolution\n",
        "\n",
        "    C = f_conv2d(I, K, p)\n",
        "\n",
        "f_conv2d is called to perform a 2D convolution of the input image I with the kernel K, using padding p.\n",
        "\n",
        "##Adding Bias\n",
        "\n",
        "    C = C + b\n",
        "\n",
        "The bias term b is added to the result of the convolution.\n",
        "\n",
        "##ReLU Activation\n",
        "\n",
        "    C = f_ReLU(C)\n",
        "\n",
        "The ReLU activation function is applied to introduce non-linearity by setting all negative values in C to zero.\n",
        "\n",
        "##Pooling\n",
        "\n",
        "    S = f_pool(C)\n",
        "    \n",
        "f_pool is called to perform a pooling operation on the activated feature map C, reducing its spatial dimensions.\n",
        "\n",
        "##Flattening\n",
        "\n",
        "    f = S.flatten()\n",
        "\n",
        "The pooled feature map S is flattened into a one-dimensional vector f, preparing it for the fully connected layer.\n",
        "\n",
        "##Fully Connected Layer and Sigmoid Activation:\n",
        "\n",
        "    y_hat = f_sigmoid(f, w, bf)\n",
        "\n",
        "The sigmoid activation function is applied to the output of the fully connected layer.\n",
        "\n",
        "This layer computes a weighted sum of the input vector f with weights w and adds the bias term bf.\n",
        "\n",
        "----\n",
        "##Return Values\n",
        "\n",
        "    return C, f, y_hat\n",
        "\n",
        "The function returns three values:\n",
        "\n",
        "C: The feature map after convolution, bias addition, and ReLU activation.\n",
        "\n",
        "f: The flattened feature vector after pooling.\n",
        "\n",
        "y_hat: The final output of the forward pass after applying the sigmoid activation function.\n",
        "\n",
        "-----\n",
        "##Example\n",
        "Let's walk through an example to see how it works:\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Define dummy inputs and parameters\n",
        "    I = np.random.rand(5, 5)  # Example input image\n",
        "    K = np.random.rand(3, 3)  # Example kernel\n",
        "    b = 1.0  # Bias term for convolution\n",
        "    w = np.random.rand(9)  # Weights for fully connected layer (assuming 3x3 pooling result flattened)\n",
        "    bf = 0.5  # Bias term for fully connected layer\n",
        "\n",
        "# Define the necessary functions (as assumed to exist in the code)\n",
        "    def f_conv2d(I, K, p):\n",
        "       # Simplified convolution function\n",
        "       I2 = np.pad(I, ((p, p), (p, p)), mode='constant')\n",
        "       C = np.zeros_like(I)\n",
        "\n",
        "       for i in range(I.shape[0]):\n",
        "          for j in range(I.shape[1]):\n",
        "            C[i, j] = np.sum(I2[i:i+K.shape[0], j:j+K.shape[1]] * K)\n",
        "            return C\n",
        "\n",
        "-----\n",
        "    def f_ReLU(C):\n",
        "       C[C < 0] = 0\n",
        "       return C\n",
        "\n",
        "----\n",
        "    def f_pool(C):\n",
        "       # Simplified pooling function (2x2 max pooling)\n",
        "       S = C[::2, ::2]  # Assume downsampling by factor of 2\n",
        "       return S\n",
        "\n",
        "    def f_sigmoid(f, w, bf):\n",
        "       x = w.dot(f) + bf\n",
        "       y_hat = 1 / (1 + np.exp(-x))\n",
        "       return y_hat\n",
        "\n",
        "# Perform the forward pass\n",
        "    C, f, y_hat = f_forwardPass(I, K, b, w, bf)\n",
        "    print(\"Convolved feature map (C):\", C)\n",
        "    print(\"Flattened feature vector (f):\", f)\n",
        "    print(\"Output (y_hat):\", y_hat)\n",
        "\n",
        "##Summary\n",
        "Padding Calculation: Determines how much padding is needed to maintain the input size after convolution.\n",
        "\n",
        "###Convolution\n",
        "\n",
        "Applies the kernel to the input image, adding padding to maintain the spatial dimensions.\n",
        "\n",
        "###Adding Bias\n",
        "\n",
        "Adds a bias term to each element of the convolved feature map.\n",
        "\n",
        "###ReLU Activation\n",
        "\n",
        "Applies the ReLU function to introduce non-linearity.\n",
        "\n",
        "###Pooling\n",
        "\n",
        "Reduces the spatial dimensions of the feature map.\n",
        "\n",
        "###Flattening\n",
        "\n",
        "Converts the pooled feature map into a one-dimensional vector.\n",
        "\n",
        "###Fully Connected Layer and Sigmoid Activation\n",
        "\n",
        "Applies a weighted sum and sigmoid function to produce the final output.\n",
        "\n",
        "###Return Values\n",
        "\n",
        "Provides the convolved feature map, flattened feature vector, and final output.\n",
        "\n",
        "This function demonstrates the key steps involved in a forward pass through a simple CNN and fully connected layer."
      ],
      "metadata": {
        "id": "d8Gaj72TMr3w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDGXUmuMkuEL"
      },
      "outputs": [],
      "source": [
        "def f_getGradient_w(y_hat,y,f):\n",
        "    Dw = np.squeeze(np.zeros((1,len(f))))\n",
        "    a = (y_hat-y)*y_hat*(1-y_hat)\n",
        "    for i in range(len(f)):\n",
        "        Dw[i] = a*f[i]\n",
        "    return Dw"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1st Backpropagation Function -Calculates the Gradient (Single Neuron Backpropagation)\n",
        "\n",
        "This function f_getGradient_w calculates the gradient of the weights (denoted as Dw) for a single neuron in a neural network during the backpropagation process.\n",
        "\n",
        "The gradient is used to update the weights in order to minimize the loss function.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#Parameters\n",
        "\n",
        "##y_hat\n",
        "\n",
        "The predicted output of the neuron (a single value).\n",
        "\n",
        "##y\n",
        "\n",
        "The true label or target value (a single value).\n",
        "\n",
        "##f\n",
        "\n",
        "The flattened feature vector (input to the neuron).\n",
        "\n",
        "----\n",
        "#Steps in the Function\n",
        "\n",
        "##Initialize the Gradient Array\n",
        "\n",
        "    Dw = np.squeeze(np.zeros((1, len(f))))\n",
        "    np.zeros((1, len(f)))\n",
        "    \n",
        "creates a 1xN array filled with zeros, where N is the length of the feature vector f.\n",
        "\n",
        "=========================================\n",
        "##np.squeeze\n",
        "\n",
        "removes single-dimensional entries from the shape of the array, converting it to a 1D array of length N.\n",
        "\n",
        "Let's create an array with a shape that includes a single-dimensional entry:\n",
        "\n",
        "    import numpy as np\n",
        "    array = np.zeros((1, 5))\n",
        "    print(array.shape)  # Output: (1, 5)\n",
        "\n",
        "This creates a 2D array with one row and five columns.\n",
        "\n",
        "The shape of the array is (1, 5), meaning it has one row and five columns.\n",
        "\n",
        "##Applying np.squeeze:\n",
        "\n",
        "Now, let's apply np.squeeze to this array:\n",
        "\n",
        "    squeezed_array = np.squeeze(array)\n",
        "    print(squeezed_array.shape)  # Output: (5,)\n",
        "    \n",
        "np.squeeze removes the single-dimensional entry from the shape.\n",
        "\n",
        "The shape of the array after squeezing is (5,), meaning it's now a 1D array with five elements.\n",
        "\n",
        "##Explanation\n",
        "The original array array has a shape of (1, 5).\n",
        "\n",
        "This means it has one row and five columns.\n",
        "\n",
        "When we apply np.squeeze, it removes the dimension of size 1 (the single row), resulting in a 1D array with the shape (5,).\n",
        "\n",
        "##Applying to the Code Snippet\n",
        "In the context of your code snippet:\n",
        "\n",
        "    Dw = np.squeeze(np.zeros((1, len(f))))\n",
        "\n",
        "##Creating a Zero Array\n",
        "\n",
        "###np.zeros((1, len(f)))\n",
        "\n",
        "creates a 2D array with shape (1, len(f)), where len(f) is the number of features.\n",
        "\n",
        "This means it creates a single row with len(f) columns, all initialized to zero.\n",
        "\n",
        "##Applying np.squeeze\n",
        "\n",
        "np.squeeze removes the single-dimensional entry (the single row), converting it to a 1D array.\n",
        "\n",
        "After squeezing, the shape of Dw is (len(f),), which is a 1D array with len(f) elements.\n",
        "\n",
        "##Why Use np.squeeze?\n",
        "\n",
        "In many machine learning and numerical computing tasks, we often need to ensure that arrays have the correct dimensions.\n",
        "\n",
        "np.squeeze is used here to convert a 2D array with one row into a 1D array, which simplifies further calculations and operations.\n",
        "\n",
        "##Conclusion\n",
        "np.squeeze is a useful function in NumPy to remove single-dimensional entries from the shape of an array.\n",
        "\n",
        "In the provided code, it ensures that the gradient array Dw has the correct shape (1D array with length equal to the number of features) for further processing.\n",
        "\n",
        "-----\n",
        "##Compute the Error Term\n",
        "\n",
        "    a = (y_hat - y) * y_hat * (1 - y_hat)\n",
        "\n",
        "This term a represents the error gradient with respect to the neuron's output.\n",
        "\n",
        "y_hat - y is the difference between the predicted output and the true output.\n",
        "\n",
        "y_hat * (1 - y_hat) is the derivative of the sigmoid activation function (which is commonly used in neural networks).\n",
        "\n",
        "The product of these terms gives the gradient of the loss with respect to the neuron's output.\n",
        "\n",
        "-----\n",
        "##Calculate the Gradient for Each Weight\n",
        "\n",
        "    for i in range(len(f)):\n",
        "       Dw[i] = a * f[i]\n",
        "\n",
        "Iterate over each element in the feature vector f.\n",
        "\n",
        "For each feature f[i], calculate the gradient Dw[i] by multiplying the error term a with the corresponding feature value f[i].\n",
        "\n",
        "This follows the chain rule in calculus, where the gradient of the weight is the product of the gradient of the output and the input feature.\n",
        "\n",
        "----\n",
        "##Return the Gradient\n",
        "\n",
        "    return Dw\n",
        "\n",
        "The function returns the calculated gradient array Dw.\n",
        "\n",
        "------\n",
        "#Example\n",
        "Suppose y_hat = 0.8, y = 1, and f = [0.5, 0.3, 0.2].\n",
        "\n",
        "##Initialize the Gradient Array\n",
        "\n",
        "    Dw = np.squeeze(np.zeros((1, 3)))  # Dw = [0.0, 0.0, 0.0]\n",
        "\n",
        "##Compute the Error Term\n",
        "\n",
        "    a = (0.8 - 1) * 0.8 * (1 - 0.8)  # a = -0.16\n",
        "\n",
        "##Calculate the Gradient for Each Weight\n",
        "\n",
        "    Dw[0] = -0.16 * 0.5  # Dw[0] = -0.08\n",
        "    Dw[1] = -0.16 * 0.3  # Dw[1] = -0.048\n",
        "    Dw[2] = -0.16 * 0.2  # Dw[2] = -0.032\n",
        "\n",
        "##Return the Gradient\n",
        "\n",
        "    return Dw  # Dw = [-0.08, -0.048, -0.032]\n",
        "\n",
        "----   \n",
        "#Conclusion\n",
        "This function computes the gradient of the loss function with respect to the weights of a neuron.\n",
        "\n",
        "The gradients are essential for updating the weights during the training process to minimize the loss function and improve the model's predictions."
      ],
      "metadata": {
        "id": "GupTY2onK57A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjR5w2UhkuEM"
      },
      "outputs": [],
      "source": [
        "def f_getGradient_f(y_hat,y,w):\n",
        "    Df = np.squeeze(np.zeros((1,len(w))))\n",
        "    a = (y_hat-y)*y_hat*(1-y_hat)\n",
        "    for i in range(len(w)):\n",
        "        Df[i] = a*w[i]\n",
        "    return Df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2nd Backpropagation Function -Calculates the gradient of the loss with respect to the features\n",
        "\n",
        "both calculate gradients, but they differ in the parameters they use to compute these gradients and their intended use in the context of training a neural network.\n",
        "\n",
        "--------------\n",
        "    f_getGradient_f:\n",
        "\n",
        "##Inputs\n",
        "\n",
        "    y_hat (predicted value), y (true value), w (weight vector)\n",
        "\n",
        "##Outputs: Df (gradient with respect to features)\n",
        "\n",
        "    def f_getGradient_f(y_hat, y, w):\n",
        "      # Initialize gradient vector with zeros\n",
        "     Df = np.squeeze(np.zeros((1, len(w))))\n",
        "    \n",
        "     # Calculate the error term 'a'\n",
        "     a = (y_hat - y) * y_hat * (1 - y_hat)\n",
        "    \n",
        "     # Compute the gradient for each weight\n",
        "     for i in range(len(w)):\n",
        "        Df[i] = a * w[i]\n",
        "    \n",
        "     return Df\n",
        "\n",
        "This function computes the gradient of the loss with respect to each feature by considering the contribution of each weight. Again, the error term a adjusts the gradient based on the difference between the predicted and true values.\n",
        "\n",
        "-----\n",
        "#Context in Neural Network Training\n",
        "f_getGradient_w: This function is used during the backpropagation step to update the weights of the network.\n",
        "\n",
        "The gradients calculated here indicate how much each weight should be adjusted to minimize the loss.\n",
        "\n",
        "    f_getGradient_f:\n",
        "\n",
        "This function calculates how much the input features (or activations from a previous layer) influence the loss, which can be used to update the features in certain types of neural networks or for analysis purposes.\n",
        "\n",
        "----\n",
        "#Example:\n",
        "Assume we have:\n",
        "\n",
        "Predicted value y_hat = 0.8\n",
        "\n",
        "True value y = 1\n",
        "\n",
        "Feature vector f = [0.5, 1.2, -0.7]\n",
        "\n",
        "Weight vector w = [0.3, -0.8, 0.5]\n",
        "\n",
        "    Using f_getGradient_w:\n",
        "    gradients_w = f_getGradient_w(0.8, 1, [0.5, 1.2, -0.7])\n",
        "    print(\"Gradients with respect to weights:\", gradients_w)\n",
        "\n",
        "##Using f_getGradient_f:\n",
        "\n",
        "    gradients_f = f_getGradient_f(0.8, 1, [0.3, -0.8, 0.5])\n",
        "    print(\"Gradients with respect to features:\", gradients_f)\n",
        "\n",
        "Both functions will provide different gradient vectors, reflecting their respective influences on the loss function.\n",
        "\n",
        "----\n",
        "#Conclusion\n",
        "The main difference between the two functions lies in their targets for gradient computation:\n",
        "\n",
        "f_getGradient_w focuses on the gradients of the weights using the feature vector.\n",
        "\n",
        "f_getGradient_f focuses on the gradients of the features using the weight vector.\n",
        "\n",
        "In summary, f_getGradient_w and f_getGradient_f are integral steps in the backpropagation process. They calculate the necessary gradients for updating weights and features to minimize the loss function, thereby improving the model's performance during training.\n",
        "\n",
        "\n",
        "The first function in the backpropagation process is\n",
        "\n",
        "##1. f_getGradient_w.\n",
        "\n",
        "This is followed by\n",
        "\n",
        "##f_getGradient_f.\n",
        "\n",
        "By computing the gradients in this order, you ensure that the error term is propagated correctly from the output layer back through the network, allowing for proper weight updates during the training process.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LT5vJC8lWem5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ooLYTUZkuEO"
      },
      "outputs": [],
      "source": [
        "def f_getGradient_bf(y_hat,y):\n",
        "    Dbf = (y_hat-y)*y_hat*(1-y_hat)\n",
        "    return Dbf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3rd Backpropagation Function - Calculates the gradient of the loss with respect to the bias term in the output layer\n",
        "\n",
        "This function calculates the gradient of the loss with respect to the bias term in the output layer. In the context of a neural network, the bias term affects the activation function and needs to be adjusted to minimize the loss.\n",
        "\n",
        "----\n",
        "## Calculation\n",
        "\n",
        "It uses the error term, computed as the difference between the predicted output (y_hat) and the actual output (y), multiplied by the derivative of the sigmoid activation function (since y_hat is the output of a sigmoid function).\n",
        "\n",
        "This function calculates the gradient of the loss with respect to the bias term in the output layer. In the context of a neural network, the bias term affects the activation function and needs to be adjusted to minimize the loss.\n",
        "\n",
        "Calculation: It uses the error term, computed as the difference between the predicted output (y_hat) and the actual output (y), multiplied by the derivative of the sigmoid activation function (since y_hat is the output of a sigmoid function).\n",
        "\n"
      ],
      "metadata": {
        "id": "dI0Y_sVokvpq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uJP-8lRkuEQ"
      },
      "outputs": [],
      "source": [
        "def f_getGradient_S(Df):\n",
        "    n = int(len(Df)**0.5)\n",
        "    DS = Df.reshape((n,n))\n",
        "    return DS"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4th Backpropagation Function -Df is a 1D array (vector) that represents the gradient of some feature or activation, which was flattened from its original 2D shape\n",
        "\n",
        "---\n",
        "##Input Gradient Vector (Df):\n",
        "\n",
        "Df is assumed to be a 1D array (vector) that represents the gradient of some feature or activation, which was flattened from its original 2D shape.\n",
        "Calculate the Dimension (n):\n",
        "\n",
        "    n = int(len(Df) ** 0.5)\n",
        "\n",
        "This calculates the size of one dimension of the original 2D shape. It assumes that Df was flattened from a square matrix, so the number of elements in Df should be a perfect square.\n",
        "\n",
        "len(Df) gives the total number of elements in the 1D gradient vector.\n",
        "\n",
        "Taking the square root of len(Df) gives the dimension of the original 2D matrix, n.\n",
        "\n",
        "int() converts the result to an integer. This is necessary because the square root could result in a floating-point number.\n",
        "\n",
        "-----\n",
        "##Reshape the Vector (DS):\n",
        "\n",
        "    DS = Df.reshape((n, n))\n",
        "\n",
        "This reshapes the 1D gradient vector Df into a 2D matrix with shape (n, n).\n",
        "The reshape method transforms the vector back into the 2D shape it was before being flattened.\n",
        "\n",
        "----\n",
        "##Return the Reshaped Matrix\n",
        "\n",
        "The function returns the reshaped matrix DS, which now matches the original 2D dimensions from which Df was flattened.\n",
        "\n",
        "-----\n",
        "Example\n",
        "Assume Df is a gradient vector with 16 elements. This suggests that the original matrix was of size 4x4. Here’s what happens:\n",
        "\n",
        "    Df = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16])\n",
        "    n = int(len(Df) ** 0.5)  # n = int(16 ** 0.5) = 4\n",
        "    DS = Df.reshape((n, n))\n",
        "\n",
        "The reshaped matrix DS would be:\n",
        "\n",
        "[[ 1,  2,  3,  4],\n",
        " [ 5,  6,  7,  8],\n",
        " [ 9, 10, 11, 12],\n",
        " [13, 14, 15, 16]]\n",
        "\n",
        " -----\n",
        "##Summary\n",
        "The function f_getGradient_S is used to convert a flattened 1D gradient vector back into its original 2D form.\n",
        "\n",
        "This is useful in neural networks for visualizing or updating the gradients associated with a particular feature or layer in its spatial dimensions."
      ],
      "metadata": {
        "id": "hBtTkX8hi-BK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TE7E_fXHkuER"
      },
      "outputs": [],
      "source": [
        "def f_getGradient_C(DS,C):\n",
        "    r = C.shape[0]\n",
        "    c = C.shape[1]\n",
        "    DC = np.zeros((r,c))\n",
        "    for i in range(0,r,2):\n",
        "        for j in range(0,c,2):\n",
        "            C_block = C[i:i+2,j:j+2]\n",
        "            ind = np.unravel_index(np.argmax(C_block,axis=None),C_block.shape)\n",
        "            DC[i+ind[0],j+ind[1]] = DS[int(i/2),int(j/2)]\n",
        "    return DC"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Function computes the gradient with respect to the convolutional layer's output (C)\n",
        "\n",
        "The function f_getGradient_C computes the gradient with respect to the convolutional layer's output (C) using the gradient from the pooling layer (DS).\n",
        "\n",
        "This function backpropagates the gradients through a max-pooling layer.\n",
        "\n",
        "-----\n",
        "##Function Explanation\n",
        "\n",
        "    def f_getGradient_C(DS, C):\n",
        "       r = C.shape[0]\n",
        "       c = C.shape[1]\n",
        "       DC = np.zeros((r, c))\n",
        "       for i in range(0, r, 2):\n",
        "         for j in range(0, c, 2):\n",
        "            C_block = C[i:i+2, j:j+2]\n",
        "            ind = np.unravel_index(np.argmax(C_block, axis=None), C_block.shape)\n",
        "            DC[i+ind[0], j+ind[1]] = DS[int(i/2), int(j/2)]\n",
        "        return DC\n",
        "\n",
        "----\n",
        "##Detailed Explanation\n",
        "###Input and Initialization\n",
        "\n",
        "DS: This is the gradient coming from the subsequent layer (the pooling layer in this case). It has the same shape as the output of the pooling layer.\n",
        "\n",
        "C: This is the output of the convolutional layer before pooling.\n",
        "\n",
        "r and c: These are the number of rows and columns of the matrix C.\n",
        "\n",
        "DC: This is the gradient of the loss with respect to the output of the convolutional layer, initialized to a zero matrix of the same shape as C.\n",
        "\n",
        "###Loop Over the Convolution Output\n",
        "\n",
        "The function iterates over the convolutional output C in steps of 2 (since it's assumed that the pooling window is 2x2 and stride is 2).\n",
        "\n",
        "###Pooling Window and Gradient Assignment\n",
        "\n",
        "C_block: This extracts a 2x2 block from the matrix C.\n",
        "\n",
        "ind: This finds the index of the maximum value in the C_block. The max-pooling operation records which index within the 2x2 block had the maximum value.\n",
        "\n",
        "The gradient DS for the corresponding pooled region is then assigned to the location in DC that corresponds to the maximum value in C_block.\n",
        "\n",
        "###Returning the Gradient:\n",
        "\n",
        "The function returns DC, which is the gradient with respect to the convolutional layer's output.\n",
        "\n",
        "-----\n",
        "#Example to Illustrate\n",
        "Let's assume C is the output from a convolutional layer, and DS is the gradient from the pooling layer:\n",
        "\n",
        "    import numpy as np\n",
        "\n",
        "-----\n",
        "# Example convolutional layer output (4x4 matrix)\n",
        "    C = np.array([\n",
        "      [1, 3, 2, 4],\n",
        "      [5, 6, 7, 8],\n",
        "      [9, 2, 4, 1],\n",
        "      [3, 7, 5, 6]\n",
        "    ])\n",
        "\n",
        "-----\n",
        "# Example pooling layer gradient (2x2 matrix)\n",
        "    DS = np.array([\n",
        "      [1, 2],\n",
        "      [3, 4]\n",
        "    ])\n",
        "\n",
        "    def f_getGradient_C(DS, C):\n",
        "       r = C.shape[0]\n",
        "       c = C.shape[1]\n",
        "       DC = np.zeros((r, c))\n",
        "       for i in range(0, r, 2):\n",
        "          for j in range(0, c, 2):\n",
        "             C_block = C[i:i+2, j:j+2]\n",
        "             ind = np.unravel_index(np.argmax(C_block, axis=None), C_block.shape)\n",
        "             DC[i+ind[0], j+ind[1]] = DS[int(i/2), int(j/2)]\n",
        "      return DC\n",
        "\n",
        "###Explanation\n",
        "Looping Over Blocks in the Convolutional Output:\n",
        "    for i in range(0, r, 2):\n",
        "       for j in range(0, c, 2):\n",
        "\n",
        "These loops iterate over the convolutional output matrix C in steps of 2.\n",
        "r and c are the dimensions (number of rows and columns) of the matrix C.\n",
        "\n",
        "The step of 2 is used because the max-pooling operation typically uses a 2x2 window with a stride of 2.\n",
        "\n",
        "###Extracting a 2x2 Block:\n",
        "\n",
        "    C_block = C[i:i+2, j:j+2]\n",
        "\n",
        "This line extracts a 2x2 block from C, starting at position (i, j).\n",
        "\n",
        "C_block is a small 2x2 matrix containing elements from C.\n",
        "\n",
        "###Finding the Index of the Maximum Value in the Block:\n",
        "\n",
        "    ind = np.unravel_index(np.argmax(C_block, axis=None), C_block.shape)\n",
        "    np.argmax(C_block, axis=None)\n",
        "    \n",
        "finds the index of the maximum value in C_block as if C_block were a flattened array.\n",
        "\n",
        "    np.unravel_index\n",
        "\n",
        "then converts this flat index back into a tuple of coordinates within the 2x2 block.\n",
        "\n",
        "ind is a tuple representing the row and column indices of the maximum value within the C_block.\n",
        "\n",
        "###Assigning the Gradient to the Corresponding Position in DC:\n",
        "\n",
        "    DC[i+ind[0], j+ind[1]] = DS[int(i/2), int(j/2)]\n",
        "\n",
        "This line assigns the gradient value from DS to the corresponding position in DC.\n",
        "\n",
        "    DS[int(i/2), int(j/2)]\n",
        "\n",
        "retrieves the gradient from the down-sampled gradient matrix DS. The indices int(i/2) and int(j/2) map the current 2x2 block position to the corresponding position in the smaller DS matrix.\n",
        "\n",
        "    DC[i+ind[0], j+ind[1]]\n",
        "\n",
        "places the retrieved gradient value into DC at the position corresponding to the maximum value within the 2x2 block.\n",
        "\n",
        "-----\n",
        "#Summary\n",
        "The loop iterates over each 2x2 block in the convolutional output matrix C.\n",
        "For each block, it identifies the position of the maximum value (which was selected during max-pooling).\n",
        "\n",
        "It then assigns the corresponding gradient value from the pooled gradient matrix DS back to the original position in DC.\n",
        "\n",
        "-----\n",
        "#Example\n",
        "To clarify, let's consider a small example:\n",
        "\n",
        "##Convolutional Output C:\n",
        "\n",
        "   [[1, 3, 2, 4],\n",
        "    [5, 6, 7, 8],\n",
        "    [9, 2, 4, 1],\n",
        "    [3, 7, 5, 6]]\n",
        "\n",
        "##Pooled Gradient DS:\n",
        "\n",
        "    [[1, 2],\n",
        "    [3, 4]]\n",
        "\n",
        "The function will perform the following steps for each 2x2 block:\n",
        "\n",
        "For the block at (0,0):\n",
        "\n",
        "    [[1, 3],\n",
        "    [5, 6]]\n",
        "\n",
        "Maximum value is 6 at (1,1).\n",
        "\n",
        "Assign DS[0,0] (which is 1) to DC[0+1, 0+1], so DC[1,1] = 1.\n",
        "\n",
        "For the block at (0,2):\n",
        "\n",
        "    [[2, 4],\n",
        "    [7, 8]]\n",
        "\n",
        "Maximum value is 8 at (1,1).\n",
        "\n",
        "Assign DS[0,1] (which is 2) to DC[1,3], so DC[1,3] = 2.\n",
        "\n",
        "For the block at (2,0):\n",
        "\n",
        "    [[9, 2],\n",
        "    [3, 7]]\n",
        "\n",
        "Maximum value is 9 at (0,0).\n",
        "\n",
        "Assign DS[1,0] (which is 3) to DC[2,0], so DC[2,0] = 3.\n",
        "\n",
        "For the block at (2,2):\n",
        "\n",
        "    [[4, 1],\n",
        "    [5, 6]]\n",
        "Maximum value is 6 at (1,1).\n",
        "\n",
        "Assign DS[1,1] (which is 4) to DC[3,3], so DC[3,3] = 4.\n",
        "\n",
        "Resulting DC:\n",
        "\n",
        "   [[0, 0, 0, 0],\n",
        "    [0, 1, 0, 2],\n",
        "    [3, 0, 0, 0],\n",
        "    [0, 0, 0, 4]]\n",
        "\n",
        "This shows how the gradients are backpropagated from the pooled output to the original convolutional output, assigning them to the positions of the maximum values within each pooling block.\n",
        "------\n",
        "# Call the function\n",
        "    DC = f_getGradient_C(DS, C)\n",
        "    print(DC)\n",
        "\n",
        "------\n",
        "#Result\n",
        "\n",
        "[[0. 0. 0. 2.]\n",
        " [0. 1. 0. 0.]\n",
        " [3. 0. 0. 0.]\n",
        " [0. 0. 0. 4.]]\n",
        "\n",
        "Here, the DC matrix shows the gradient assigned to the positions of the maximum values in each 2x2 block of C, as determined by the DS matrix.\n",
        "\n",
        "------------------\n",
        "#Summary\n",
        "The f_getGradient_C function essentially backpropagates the gradients through a max-pooling layer by distributing the gradient from the pooled output back to the positions of the maximum values in the convolutional output.\n",
        "\n",
        "This is an essential step in implementing backpropagation in convolutional neural networks (CNNs), ensuring the correct gradients are computed for the convolutional layer parameters."
      ],
      "metadata": {
        "id": "YpOHwTAadqsC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pio8_YRpkuES"
      },
      "outputs": [],
      "source": [
        "def f_getChainRuleGradients(C,DC,I,u,v):\n",
        "    DKuv = 0\n",
        "    for i in range(C.shape[0]):\n",
        "        for j in range(C.shape[1]):\n",
        "            if C[i,j]>0 and i-u>=0 and j-v>=0 and i-u<C.shape[0] and j-v<C.shape[1]:\n",
        "                DKuv = DKuv + (I[i-u,j-v]*DC[i,j])\n",
        "    return DKuv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The gradient of the loss with respect to 𝐶\n",
        "###C\n",
        "\n",
        "The output of the convolutional layer after applying the activation function (in this case, ReLU).\n",
        "\n",
        "###DC\n",
        "\n",
        "The gradient of the loss with respect to\n",
        "$$𝐶$$\n",
        "which is propagated back from the pooling layer.\n",
        "\n",
        "I: The original input image to the convolutional layer.\n",
        "\n",
        "u, v: The coordinates of the element in the convolutional kernel\n",
        "$$𝐾$$\n",
        "for which we are calculating the gradient.\n",
        "\n",
        "------\n",
        "##Function Logic\n",
        "\n",
        "###Initialization:\n",
        "\n",
        "    DKuv = 0\n",
        "This initializes the gradient\n",
        "\n",
        "$$𝐷\n",
        "𝐾\n",
        "𝑢\n",
        "𝑣$$\n",
        "\n",
        "DKuv of the kernel element at position\n",
        "$$(\n",
        "𝑢\n",
        ",\n",
        "𝑣\n",
        ")$$\n",
        "\n",
        "(u,v) to zero.\n",
        "\n",
        "Loop Over Each Element in\n",
        "$$𝐶$$\n",
        "\n",
        "    for i in range(C.shape[0]):\n",
        "       for j in range(C.shape[1]):\n",
        "\n",
        "This loops over each element in the convolutional output 𝐶.\n",
        "\n",
        "Conditions to Ensure Valid Indices:\n",
        "\n",
        "\n",
        "    if C[i,j] > 0 and i-u >= 0 and j-v >= 0 and i-u < C.shape[0] and j-v < C.shape[1]:\n",
        "\n",
        "C[i, j] > 0: This checks if the element\n",
        "𝐶\n",
        "[\n",
        "𝑖\n",
        ",\n",
        "𝑗\n",
        "]\n",
        "C[i,j]\n",
        "\n",
        "is part of the region that was activated by the ReLU function.\n",
        "\n",
        "Since ReLU sets negative values to zero, we only consider positive values.\n",
        "i-u >= 0 and j-v >= 0 and i-u < C.shape[0] and j-v < C.shape[1]: These conditions ensure that the indices\n",
        "$$(\n",
        "𝑖\n",
        "−\n",
        "𝑢\n",
        ",\n",
        "𝑗\n",
        "−\n",
        "𝑣\n",
        ")$$\n",
        "(i−u,j−v) are within the bounds of the input image\n",
        "𝐼\n",
        "\n",
        "-----\n",
        "##Accumulate Gradient\n",
        "\n",
        "    DKuv = DKuv + (I[i-u, j-v] * DC[i, j])\n",
        "If the conditions are satisfied, the gradient\n",
        "$$𝐷\n",
        "𝐾\n",
        "𝑢\n",
        "𝑣$$\n",
        "\n",
        "DKuv is updated by adding the product of:\n",
        "\n",
        "I[i-u, j-v]: The corresponding element in the input image.\n",
        "\n",
        "DC[i, j]: The gradient of the loss with respect to\n",
        "𝐶\n",
        "[\n",
        "𝑖\n",
        ",\n",
        "𝑗\n",
        "]\n",
        "C[i,j].\n",
        "Return the Gradient:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "return DKuv\n",
        "After looping through all elements, the function returns the calculated gradient\n",
        "𝐷\n",
        "𝐾\n",
        "𝑢\n",
        "𝑣\n",
        "DKuv.\n",
        "\n",
        "Example\n",
        "Let's consider a small example to illustrate the calculation:\n",
        "\n",
        "Input Matrices\n",
        "Input Image\n",
        "𝐼\n",
        "I:\n",
        "\n",
        "plaintext\n",
        "Copy code\n",
        "[[1, 2, 3],\n",
        " [4, 5, 6],\n",
        " [7, 8, 9]]\n",
        "Output of Convolution\n",
        "𝐶\n",
        "C:\n",
        "\n",
        "plaintext\n",
        "Copy code\n",
        "[[1, 3],\n",
        " [5, 6]]\n",
        "Gradient with Respect to\n",
        "𝐶\n",
        "C (DC):\n",
        "\n",
        "plaintext\n",
        "Copy code\n",
        "[[0.1, 0.2],\n",
        " [0.3, 0.4]]\n",
        "Kernel Element Position\n",
        "(\n",
        "𝑢\n",
        ",\n",
        "𝑣\n",
        ")\n",
        "=\n",
        "(\n",
        "0\n",
        ",\n",
        "0\n",
        ")\n",
        "(u,v)=(0,0)\n",
        "\n",
        "Calculation\n",
        "Initialize\n",
        "𝐷\n",
        "𝐾\n",
        "𝑢\n",
        "𝑣\n",
        "DKuv:\n",
        "\n",
        "plaintext\n",
        "Copy code\n",
        "DKuv = 0\n",
        "Loop through each element in\n",
        "𝐶\n",
        "C:\n",
        "\n",
        "For\n",
        "𝑖\n",
        "=\n",
        "0\n",
        ",\n",
        "𝑗\n",
        "=\n",
        "0\n",
        "i=0,j=0:\n",
        "\n",
        "plaintext\n",
        "Copy code\n",
        "DKuv += I[0-0, 0-0] * DC[0, 0]\n",
        "DKuv += 1 * 0.1\n",
        "DKuv = 0.1\n",
        "For\n",
        "𝑖\n",
        "=\n",
        "0\n",
        ",\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "i=0,j=1:\n",
        "\n",
        "plaintext\n",
        "Copy code\n",
        "DKuv += I[0-0, 1-0] * DC[0, 1]\n",
        "DKuv += 2 * 0.2\n",
        "DKuv = 0.5\n",
        "For\n",
        "𝑖\n",
        "=\n",
        "1\n",
        ",\n",
        "𝑗\n",
        "=\n",
        "0\n",
        "i=1,j=0:\n",
        "\n",
        "plaintext\n",
        "Copy code\n",
        "DKuv += I[1-0, 0-0] * DC[1, 0]\n",
        "DKuv += 4 * 0.3\n",
        "DKuv = 1.7\n",
        "For\n",
        "𝑖\n",
        "=\n",
        "1\n",
        ",\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "i=1,j=1:\n",
        "\n",
        "plaintext\n",
        "Copy code\n",
        "DKuv += I[1-0, 1-0] * DC[1, 1]\n",
        "DKuv += 5 * 0.4\n",
        "DKuv = 3.7\n",
        "Return\n",
        "𝐷\n",
        "𝐾\n",
        "𝑢\n",
        "𝑣\n",
        "DKuv:\n",
        "\n",
        "plaintext\n",
        "Copy code\n",
        "return 3.7\n",
        "The function f_getChainRuleGradients accumulates the gradient for the specific kernel element\n",
        "(\n",
        "𝑢\n",
        ",\n",
        "𝑣\n",
        ")\n",
        "(u,v) by considering the influence of each activated element in\n",
        "𝐶\n",
        "C and the corresponding elements in the input image\n",
        "𝐼\n",
        "I. This approach follows the chain rule to propagate the gradients back through the layers."
      ],
      "metadata": {
        "id": "kZxKSydL8tSG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ydf8JGOYkuET"
      },
      "outputs": [],
      "source": [
        "def f_getGradient_K(C,I,y_hat,y,w):\n",
        "    Df = f_getGradient_f(y_hat,y,w)\n",
        "    DS = f_getGradient_S(Df)\n",
        "    DC = f_getGradient_C(DS,C)\n",
        "    DK = np.zeros((5,5))\n",
        "    for u in range(-2,3):\n",
        "        for v in range(-2,3):\n",
        "            DK[u+2,v+2] = f_getChainRuleGradients(C,DC,I,u,v)\n",
        "    return DK,DC"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# f_getGradient_K function\n",
        "\n",
        "This function computes the gradient of the convolutional kernel\n",
        "$$𝐾$$\n",
        "during the backpropagation process in a convolutional neural network (CNN).\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Calculate Gradient with Respect to Flattened Output\n",
        "\n",
        "    Df = f_getGradient_f(y_hat, y, w)\n",
        "\n",
        "###f_getGradient_f(y_hat, y, w):\n",
        "\n",
        "This function calculates the gradient of the loss with respect to the flattened feature map output from the pooling layer.\n",
        "\n",
        "----\n",
        "##Parameters\n",
        "y_hat: The predicted output.\n",
        "\n",
        "y: The true label.\n",
        "\n",
        "w: The weights of the fully connected layer.\n",
        "\n",
        "------\n",
        "##Reshape Gradient to Match Pooled\n",
        " Output Dimensions\n",
        "\n",
        "    DS = f_getGradient_S(Df)\n",
        "\n",
        "###f_getGradient_S(Df)\n",
        "\n",
        "This function reshapes the gradient Df from a flattened vector back into a 2D shape that matches the pooled output dimensions.\n",
        "\n",
        "------\n",
        "##Calculate Gradient with Respect to Convolutional Output:\n",
        "\n",
        "    DC = f_getGradient_C(DS, C)\n",
        "\n",
        "-----\n",
        "\n",
        "    f_getGradient_C(DS, C)\n",
        "\n",
        "This function calculates the gradient of the loss with respect to the output of the convolutional layer (before pooling).\n",
        "\n",
        "\n",
        "###Parameters\n",
        "DS: The gradient with respect to the pooled output.\n",
        "\n",
        "C: The output of the convolutional layer.\n",
        "\n",
        "----\n",
        "\n",
        "Initialize Kernel Gradient Matrix:\n",
        "\n",
        "    DK = np.zeros((5,5))\n",
        "\n",
        "DK: This matrix will hold the gradients of the kernel 𝐾.\n",
        "\n",
        "Assuming a 5x5 kernel, it's initialized to zeros.\n",
        "\n",
        "------\n",
        "###Calculate Gradients for Each Element in the Kernel:\n",
        "\n",
        "    for u in range(-2, 3):\n",
        "       for v in range(-2, 3):\n",
        "          DK[u+2, v+2] = f_getChainRuleGradients(C, DC, I, u, v)\n",
        "\n",
        "This double loop iterates over the kernel's dimensions.\n",
        "\n",
        "For a 5x5 kernel, u and v range from -2 to 2.\n",
        "\n",
        "----\n",
        "###f_getChainRuleGradients(C, DC, I, u, v):\n",
        "\n",
        "This function calculates the gradient for a specific element in the kernel using the chain rule.\n",
        "\n",
        "###Parameters:\n",
        "C: The output of the convolutional layer.\n",
        "\n",
        "DC: The gradient with respect to the\n",
        "convolutional layer's output.\n",
        "\n",
        "I: The input image.\n",
        "\n",
        "u, v: The current indices in the kernel.\n",
        "\n",
        "Note: u+2 and v+2 adjust the indices to correctly map into the 5x5 kernel matrix DK.\n",
        "\n",
        "-----\n",
        "###Return the Gradient Matrices:\n",
        "\n",
        "    return DK, DC\n",
        "\n",
        "-----\n",
        "##Summary of the Gradient Calculation Process\n",
        "Gradient with Respect to Fully Connected Layer Output (Df):\n",
        "\n",
        "1. Calculate the gradient of the loss with respect to the output of the fully connected layer.\n",
        "\n",
        "2. Gradient with Respect to Pooled Output (DS):\n",
        "\n",
        "3. Reshape the gradient Df to match the dimensions of the pooled output.\n",
        "\n",
        "4. Gradient with Respect to Convolutional Layer Output (DC):\n",
        "\n",
        "5. Calculate the gradient of the loss with respect to the output of the convolutional layer.\n",
        "\n",
        "6. Gradient with Respect to Kernel (DK):\n",
        "\n",
        "Iterate over each element in the kernel and use the chain rule to compute its gradient.\n",
        "\n",
        "This ensures that gradients are propagated back correctly through each layer, allowing for the correct adjustment of parameters (weights and biases) during the training process of the neural network."
      ],
      "metadata": {
        "id": "ZC_BT9cYs3w7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYh-rYRCkuET"
      },
      "outputs": [],
      "source": [
        "def f_getGradient_b(C,DC):\n",
        "    Db = DC[C>0].sum()\n",
        "    return Db"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function Calculates the gradient of the bias term in the convolutional layer during the backpropagation process in a convolutional neural network (CNN).\n",
        "\n",
        "------\n",
        "1. Extract the Gradient Values Where the Convolutional Output is Positive:\n",
        "\n",
        "    Db = DC[C > 0].sum()\n",
        "\n",
        "##C > 0\n",
        "\n",
        "This creates a boolean mask of the same shape as the convolutional output C, where each element is True if the corresponding element in C is greater than 0, and False otherwise.\n",
        "\n",
        "##DC[C > 0]\n",
        "\n",
        " This uses the boolean mask to extract elements from DC (the gradient of the loss with respect to the convolutional layer's output) where the corresponding elements in C are positive. This is due to the ReLU activation function, which only propagates gradients for positive values.\n",
        "\n",
        "###.sum()\n",
        "\n",
        "This sums up all the extracted gradient values to compute the total gradient of the bias term.\n",
        "\n",
        "----------------\n",
        "##Return the Computed Gradient\n",
        "\n",
        "    return Db\n",
        "The function returns the computed gradient of the bias term Db.\n",
        "\n",
        "-----\n",
        "#Summary of the Gradient Calculation Process\n",
        "1. Gradient Masking:\n",
        "\n",
        "The function first identifies where the convolutional layer's output C is positive.\n",
        "\n",
        "This is crucial because the ReLU activation function zeroes out negative values, meaning only positive values contribute to the backpropagated gradients.\n",
        "\n",
        "2. Extract Relevant Gradients:\n",
        "\n",
        "Using the mask, it extracts the corresponding elements from DC, which contains the gradients of the loss with respect to each element in C.\n",
        "Sum the Gradients:\n",
        "\n",
        "It then sums these gradients to get the total gradient with respect to the bias term Db.\n",
        "\n",
        "-------\n",
        "#Why This Approach?\n",
        "###ReLU Activation\n",
        "\n",
        "The ReLU activation function sets all negative values to zero and only allows positive values to pass through.\n",
        "\n",
        "During backpropagation, this means that only the gradients corresponding to positive values in C will be non-zero.\n",
        "\n",
        "Hence, DC[C > 0] ensures that we only consider these non-zero gradients.\n",
        "\n",
        "2. Bias Term\n",
        "\n",
        "In a convolutional layer, the bias term is added to each element of the output feature map.\n",
        "\n",
        "The gradient of the loss with respect to the bias term is therefore the sum of the gradients of the loss with respect to each element in the output feature map.\n",
        "\n",
        "This method ensures that the gradients are correctly calculated for the bias term, allowing it to be updated properly during the training process."
      ],
      "metadata": {
        "id": "00R8W2xQ4iI0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuTD007EkuEU"
      },
      "outputs": [],
      "source": [
        "def f_backwardPass(I,C,f,w,y_hat,y):\n",
        "    Dw = f_getGradient_w(y_hat,y,f)\n",
        "    Dbf = f_getGradient_bf(y_hat,y)\n",
        "    DK,DC = f_getGradient_K(C,I,y_hat,y,w)\n",
        "    Db = f_getGradient_b(C,DC)\n",
        "    return DK,Db,Dw,Dbf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The f_backwardPass function orchestrates the process of calculating gradients for different parameters in a neural network during backpropagation.\n",
        "\n",
        "-------\n",
        "1. Calculate Gradient of the Weights in the Fully Connected Layer (Dw):\n",
        "\n",
        "    Dw = f_getGradient_w(y_hat, y, f)\n",
        "\n",
        "This line calls f_getGradient_w, which computes the gradient of the loss with respect to the weights w in the fully connected layer.\n",
        "\n",
        "Inputs\n",
        "\n",
        "The predicted output y_hat, the true label y, and the flattened output of the pooling layer f.\n",
        "\n",
        "2. Calculate Gradient of the Bias in the Fully Connected Layer (Dbf):\n",
        "\n",
        "    Dbf = f_getGradient_bf(y_hat, y)\n",
        "\n",
        "This line calls f_getGradient_bf, which computes the gradient of the loss with respect to the bias bf in the fully connected layer.\n",
        "\n",
        "Inputs\n",
        "\n",
        "The predicted output y_hat and the true label y.\n",
        "\n",
        "3. Calculate Gradients for the Convolutional Layer (DK and DC):\n",
        "\n",
        "    DK, DC = f_getGradient_K(C, I, y_hat, y, w)\n",
        "\n",
        "This line calls f_getGradient_K, which computes the gradient of the loss with respect to the convolutional kernel K and also returns the intermediate gradient DC for further use.\n",
        "\n",
        "Inputs\n",
        "\n",
        "The output of the convolutional layer C, the input image I, the predicted output y_hat, the true label y, and the weights w.\n",
        "\n",
        "4. Calculate Gradient of the Bias in the Convolutional Layer (Db):\n",
        "\n",
        "    Db = f_getGradient_b(C, DC)\n",
        "\n",
        "This line calls f_getGradient_b, which computes the gradient of the loss with respect to the bias b in the convolutional layer.\n",
        "\n",
        "Inputs\n",
        "\n",
        "The output of the convolutional layer C and the intermediate gradient DC.\n",
        "\n",
        "4. Return All Gradients:\n",
        "\n",
        "    return DK, Db, Dw, Dbf\n",
        "\n",
        "The function returns the gradients for the convolutional kernel DK, the convolutional layer bias Db, the fully connected layer weights Dw, and the fully connected layer bias Dbf.\n",
        "\n",
        "------\n",
        "#Summary of Gradient Calculation Process\n",
        "\n",
        "Fully Connected Layer Gradients:\n",
        "\n",
        "f_getGradient_w: Computes the gradient with respect to the weights of the fully connected layer.\n",
        "\n",
        "f_getGradient_bf:\n",
        "\n",
        " Computes the gradient with respect to the bias of the fully connected layer.\n",
        "Convolutional Layer Gradients:\n",
        "\n",
        "f_getGradient_K:\n",
        "\n",
        "Computes the gradient with respect to the convolutional kernel and returns an intermediate gradient for the convolutional layer.\n",
        "\n",
        "f_getGradient_b\n",
        "\n",
        "Computes the gradient with respect to the bias of the convolutional layer using the intermediate gradient.\n",
        "\n",
        "------\n",
        "#Why This Approach?\n",
        "Separation of Concerns: Each function is responsible for computing the gradient of a specific parameter, making the code modular and easier to understand and debug.\n",
        "\n",
        "##Backpropagation Workflow\n",
        "\n",
        "The function follows the natural flow of backpropagation, starting from the output layer and working its way backward through the network, calculating gradients at each layer.\n",
        "\n",
        "##Intermediate Gradients\n",
        "\n",
        "The use of intermediate gradients (DC) allows for the correct propagation of errors through layers, ensuring accurate gradient computation for earlier layers in the network.\n",
        "\n",
        "-----------\n",
        "##Example Scenario\n",
        "Assume we have a simple CNN with one convolutional layer followed by a fully connected layer.\n",
        "\n",
        "During training, the f_backwardPass function is called to compute the gradients needed to update the network's parameters:\n",
        "\n",
        "1. The function first computes the gradient of the loss with respect to the weights and biases in the fully connected layer.\n",
        "\n",
        "2. It then computes the gradient of the loss with respect to the convolutional kernel and the bias in the convolutional layer.\n",
        "\n",
        "These gradients are then used to update the network's parameters to minimize the loss during training.\n",
        "\n",
        "3. By organizing the gradient computations in this manner, the f_backwardPass function ensures that all necessary gradients are correctly computed and returned for use in the optimization step."
      ],
      "metadata": {
        "id": "h9Yrc5YR6i9L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Lq2P0hOkuEU"
      },
      "outputs": [],
      "source": [
        "def f_initParams():\n",
        "    K = 0.01*np.random.randn(5,5)\n",
        "    b = 0.01*np.random.randn()\n",
        "    w = np.squeeze(0.01*np.random.randn(1,256))\n",
        "    bf = 0.01*np.random.randn()\n",
        "    return K,b,w,bf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize the Convolutional Kernel (K):\n",
        "------\n",
        "    K = 0.01 * np.random.randn(5, 5)\n",
        "\n",
        "This line initializes a 5x5 convolutional kernel K.\n",
        "\n",
        "##np.random.randn(5, 5)\n",
        "\n",
        "generates a 5x5 matrix with values drawn from a standard normal distribution (mean = 0, standard deviation = 1).\n",
        "\n",
        "Multiplying by 0.01 scales the values to have a smaller variance, which can help with the convergence of the network.\n",
        "\n",
        "-----------\n",
        "##Initialize the Bias for the Convolutional Layer (b):\n",
        "\n",
        "    b = 0.01 * np.random.randn()\n",
        "\n",
        "This line initializes a scalar bias b for the convolutional layer.\n",
        "\n",
        "-----------\n",
        "##np.random.randn()\n",
        "\n",
        " generates a single value from a standard normal distribution, and multiplying by 0.01 scales it.\n",
        "\n",
        "---------\n",
        "##Initialize the Weights for the Fully Connected Layer (w):\n",
        "\n",
        "    w = np.squeeze(0.01 * np.random.randn(1, 256))\n",
        "\n",
        "This line initializes the weights w for the fully connected layer.\n",
        "\n",
        "##np.random.randn(1, 256)\n",
        "\n",
        "generates a 1x256 matrix with values drawn from a standard normal distribution.\n",
        "\n",
        "##np.squeeze()\n",
        "\n",
        "removes the single-dimensional entry from the shape of the array, converting it to a 1D array of length 256.\n",
        "\n",
        "Multiplying by 0.01 scales the values.\n",
        "\n",
        "-----\n",
        "##Initialize the Bias for the Fully Connected Layer (bf):\n",
        "\n",
        "    bf = 0.01 * np.random.randn()\n",
        "\n",
        "This line initializes a scalar bias bf for the fully connected layer.\n",
        "\n",
        "##np.random.randn()\n",
        "\n",
        " generates a single value from a standard normal distribution, and multiplying by 0.01 scales it.\n",
        "\n",
        "-----\n",
        "##Return the Initialized Parameters:\n",
        "\n",
        "    return K, b, w, bf\n",
        "\n",
        "The function returns the initialized parameters\n",
        "\n",
        "the convolutional kernel K, the convolutional bias b, the fully connected layer weights w, and the fully connected layer bias bf.\n",
        "\n",
        "------\n",
        "##Summary\n",
        "\n",
        "1. K: A 5x5 matrix representing the convolutional kernel, initialized with small random values.\n",
        "\n",
        "2. b: A scalar bias for the convolutional layer, initialized with a small random value.\n",
        "\n",
        "3. w: A 1D array of length 256 representing the weights for the fully connected layer, initialized with small random values.\n",
        "\n",
        "4. bf: A scalar bias for the fully connected layer, initialized with a small random value.\n",
        "\n",
        "------\n",
        "#Why This Initialization?\n",
        "##Small Random Values\n",
        "\n",
        "Initializing weights and biases with small random values helps prevent large initial activations and gradients, which can lead to issues like exploding or vanishing gradients.\n",
        "\n",
        "##Standard Normal Distribution\n",
        "\n",
        "Using values drawn from a standard normal distribution ensures that the parameters are spread out around zero, promoting diversity in the initial values.\n",
        "\n",
        "##Scaling\n",
        "\n",
        "Multiplying by 0.01 further reduces the variance, helping to stabilize the training process in the initial stages.\n",
        "\n",
        "Proper initialization is essential for efficient and effective training of neural networks.\n",
        "\n",
        "This function provides a simple yet effective way to initialize the parameters of a basic convolutional neural network.\n"
      ],
      "metadata": {
        "id": "Jp6q2J00-AYt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEhPBI4bkuEV",
        "outputId": "56fa93bf-6df6-4661-f7ae-7bfa7c74e02c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.4920229785037398\n",
            "0.243209155495617\n",
            "0.15288964553136547\n",
            "0.11512223380183137\n",
            "0.09087306899243046\n",
            "0.07531031211479094\n",
            "0.06470764726937782\n",
            "0.05729752408381704\n",
            "0.051654478064740786\n",
            "0.04719946859083668\n",
            "0.04359662849889232\n",
            "0.04056794188721934\n",
            "0.03802546596762972\n",
            "0.03587733059268647\n",
            "0.03405203009207826\n",
            "0.03244757526677582\n",
            "0.031024403919923484\n",
            "0.029752018091007512\n",
            "0.028606511851509826\n",
            "0.027568868555206745\n",
            "0.026623761696283544\n",
            "0.025758693538737613\n",
            "0.02496336530069008\n",
            "0.02422920923141936\n",
            "0.023549035905569685\n",
            "0.022915914406668098\n",
            "0.022323590582331607\n",
            "0.021769937027563663\n",
            "0.021251060744915732\n",
            "0.0207635869665655\n",
            "0.020304574630617984\n",
            "0.019871447996215756\n",
            "0.01946194089732216\n",
            "0.019074050978987237\n",
            "0.018706001881194704\n",
            "0.018355699463212194\n",
            "0.018021189144092755\n",
            "0.01770243160950251\n",
            "0.017398265894446933\n",
            "0.01710764556369455\n",
            "0.0168296247186427\n",
            "0.016563346025295723\n",
            "0.016308030428783798\n",
            "0.01606296828194199\n",
            "0.015827511664852217\n",
            "0.01560106771175928\n",
            "0.015383092793555568\n",
            "0.015173087429734881\n",
            "0.014970591824611076\n",
            "0.0147751819396627\n"
          ]
        }
      ],
      "source": [
        "I = np.random.randint(1,255,(32,32))\n",
        "y = 0\n",
        "K,b,w,bf = f_initParams()\n",
        "for i in range(50):\n",
        "    C,f,y_hat = f_forwardPass(I,K,b,w,bf)\n",
        "    print(y_hat)\n",
        "    DK,Db,Dw,Dbf = f_backwardPass(I,C,f,w,y_hat,y)\n",
        "    alpha = 0.001\n",
        "    K = K - alpha*DK\n",
        "    b = b - alpha*Db\n",
        "    w = w - alpha*Dw\n",
        "    bf = bf - alpha*Dbf\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This snippet demonstrates a simple training loop for a convolutional neural network (CNN).\n",
        "\n",
        "------\n",
        "## Initialization\n",
        "\n",
        "###Random Image and Label:\n",
        "\n",
        "    I = np.random.randint(1, 255, (32, 32))\n",
        "    y = 0\n",
        "\n",
        "I is a 32x32 matrix representing a grayscale image with pixel values randomly chosen between 1 and 254.\n",
        "\n",
        "y is the true label for the image, set to 0. This is a simplistic example, and typically y would be the actual label for the input image.\n",
        "\n",
        "-----\n",
        "###Initialize Parameters\n",
        "\n",
        "    K, b, w, bf = f_initParams()\n",
        "\n",
        "The function f_initParams initializes the parameters of the network:\n",
        "\n",
        "1. the convolutional kernel K,\n",
        "2. the bias for the convolutional layer b\n",
        "3. the weights for the fully connected layer w, and\n",
        "4. the bias for the fully connected layer bf.\n",
        "\n",
        "------\n",
        "###Training Loop\n",
        "The training loop runs for 50 iterations, where in each iteration, a forward pass and a backward pass are performed, followed by parameter updates.\n",
        "\n",
        "-----\n",
        "###Forward Pass:\n",
        "\n",
        "    C, f, y_hat = f_forwardPass(I, K, b, w, bf)\n",
        "\n",
        "The function f_forwardPass performs a forward pass through the network, computing the convolutional output C, the flattened pooled features f, and the predicted output y_hat.\n",
        "\n",
        "-----\n",
        "###Print Prediction:\n",
        "\n",
        "    print(y_hat)\n",
        "This prints the predicted output y_hat for each iteration.\n",
        "\n",
        "This can be useful for monitoring the model's predictions during training.\n",
        "\n",
        "------\n",
        "###Backward Pass:\n",
        "\n",
        "    DK, Db, Dw, Dbf = f_backwardPass(I, C, f, w, y_hat, y)\n",
        "The function f_backwardPass computes the gradients of the loss with respect to the parameters K, b, w, and bf.\n",
        "\n",
        "------\n",
        "###Parameter Updates:\n",
        "\n",
        "    alpha = 0.001\n",
        "    K = K - alpha * DK\n",
        "    b = b - alpha * Db\n",
        "    w = w - alpha * Dw\n",
        "    bf = bf - alpha * Dbf\n",
        "\n",
        "The learning rate alpha is set to 0.001.\n",
        "\n",
        "\n",
        "The parameters are updated using gradient descent:\n",
        "1. The convolutional kernel K is updated by subtracting alpha * DK from K.\n",
        "\n",
        "2. The bias for the convolutional layer b is updated by subtracting alpha * Db from b.\n",
        "\n",
        "3. The weights for the fully connected layer w are updated by subtracting alpha * Dw from w.\n",
        "\n",
        "4. The bias for the fully connected layer bf is updated by subtracting alpha * Dbf from bf.\n",
        "\n",
        "-----\n",
        "#Summary\n",
        "1. Random Initialization: The image I and label y are randomly initialized.\n",
        "\n",
        "2. Parameter Initialization: The network parameters (K, b, w, bf) are initialized.\n",
        "\n",
        "3. Training Loop:\n",
        "Forward Pass: Compute the predicted output y_hat.\n",
        "\n",
        "4. Print Prediction: Print the predicted output.\n",
        "\n",
        "5. Backward Pass: Compute the gradients of the loss with respect to the parameters.\n",
        "\n",
        "6. Parameter Updates: Update the parameters using gradient descent.\n",
        "\n",
        "\n",
        "This simple training loop demonstrates the basic workflow of training a neural network, where each iteration involves computing predictions, calculating gradients, and updating parameters to minimize the loss function."
      ],
      "metadata": {
        "id": "RRegy4wgAYZ0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYwl0A0EkuEZ",
        "outputId": "25e9785b-5e1f-41fe-eeb7-fd2aaef6a62f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.895626736610168"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_hat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIKjwpi4kuEZ",
        "outputId": "a616a6dd-d631-425a-a047-b5441f23ea8d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "256"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZ2oMuMBkuEa",
        "outputId": "c0a88c1e-6565-4ab8-ddeb-e88e30c095d1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.4961399908819838"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_hat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXdJBSpxkuEb",
        "outputId": "e0dfb6cc-7591-4c62-b880-fcbc51d718b6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-2.97547376, -2.61208367, -1.25452159, -0.10679289, -1.12689902],\n",
              "       [-2.60374285, -2.91080737, -2.63396244, -3.35799435, -3.24738384],\n",
              "       [-3.56845218, -3.09257297, -1.02921786, -4.64212009, -2.74420624],\n",
              "       [-3.32659253, -2.67697302, -3.12026933, -3.79688228, -1.79560188],\n",
              "       [-2.45224328, -3.83832392, -1.55289401, -2.82055551, -2.72611557]])"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "DK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AS1G-OTkuEc",
        "outputId": "8cd3d052-4bd3-40b4-ab8f-c4e9f732ebdf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([-0.34803887, -0.81728514, -0.87373002, -1.14537418, -0.59310609,\n",
              "       -0.20351208, -0.72914047, -0.88524479, -0.41872099, -0.41498201,\n",
              "       -0.37638551, -0.73271683, -0.30267096, -0.51834231, -0.77523831,\n",
              "       -0.30602767, -0.47541978, -0.5287802 , -0.39389314, -0.2601277 ,\n",
              "       -1.15820767, -0.16973692, -0.11222836, -0.        , -0.46965639,\n",
              "       -0.75465536, -0.47732463, -0.34511674, -0.30446064, -0.49239171,\n",
              "       -0.40545826, -0.71765816, -0.23753064, -0.23499351, -1.42790353,\n",
              "       -0.17978169, -0.51620326, -1.13837197, -0.25103392, -0.66151841,\n",
              "       -1.21015924, -0.46574599, -0.43922686, -0.37903103, -0.18672564,\n",
              "       -0.43357395, -1.08880907, -0.        , -0.6861942 , -0.58266859,\n",
              "       -0.49492594, -0.40179354, -0.52538117, -0.50736274, -1.15901314,\n",
              "       -0.39998735, -0.22748738, -0.18657371, -0.68763762, -0.        ,\n",
              "       -0.54415166, -0.26445015, -0.66209606, -0.31885034, -0.18546286,\n",
              "       -0.31396223, -0.89413688, -0.55533751, -0.40425145, -0.93037689,\n",
              "       -0.79868628, -0.48441011, -0.35975779, -0.6919888 , -0.4964582 ,\n",
              "       -0.62925026, -1.08212084, -0.58229601, -0.08226466, -0.40732064,\n",
              "       -0.49967268, -0.50396907, -0.48534156, -0.32748795, -0.49214408,\n",
              "       -0.0388831 , -0.47729357, -0.423376  , -0.4703694 , -0.15264267,\n",
              "       -0.75282001, -0.88297674, -0.22543503, -0.60561293, -0.62865227,\n",
              "       -0.04793328, -0.34216996, -1.13817544, -0.56775143, -0.56024257,\n",
              "       -0.73312023, -0.33155966, -0.36223231, -0.7307187 , -0.96041906,\n",
              "       -0.23559464, -0.67419153, -0.75881377, -0.27572134, -0.66503538,\n",
              "       -0.89855134, -0.22327997, -0.19686894, -0.59053428, -0.        ,\n",
              "       -1.00811868, -0.78952826, -0.515134  , -0.46587462, -0.96329019,\n",
              "       -1.02510528, -0.52449331, -0.71891822, -0.19767294, -0.29167654,\n",
              "       -0.08972099, -0.59508678, -0.57014449, -0.73343748, -0.85330007,\n",
              "       -0.50942077, -0.44487296, -0.44616715, -0.53951918, -0.24028061,\n",
              "       -0.26998891, -0.24709156, -1.07668906, -0.29269902, -0.19029594,\n",
              "       -1.04705006, -0.60853525, -0.22220069, -0.41877886, -0.51752949,\n",
              "       -0.9588315 , -0.81089911, -0.7164498 , -0.71108204, -0.20518753,\n",
              "       -0.85248239, -0.214624  , -0.69954589, -0.76850704, -0.64107039,\n",
              "       -0.52414974, -0.19134866, -0.31759293, -0.66596614, -0.16377858,\n",
              "       -0.63648681, -0.17859177, -1.2117078 , -1.37772717, -0.53347928,\n",
              "       -0.75040356, -0.53495596, -0.61044522, -0.74508245, -1.07874703,\n",
              "       -0.75113656, -0.18402508, -0.66717564, -0.21747083, -0.67363453,\n",
              "       -0.77430225, -0.08561174, -0.3662409 , -0.20107899, -0.76480246,\n",
              "       -0.47707945, -0.26708987, -0.71377712, -0.62721932, -0.81756436,\n",
              "       -0.61106867, -0.71590925, -0.31782441, -0.19976453, -0.95697408,\n",
              "       -0.62843286, -0.48071353, -0.09375682, -0.69239711, -0.41839629,\n",
              "       -0.45638528, -0.32304649, -1.11465096, -0.2974555 , -0.21426432,\n",
              "       -0.53033058, -0.37012502, -0.55715697, -0.41565909, -0.28591755,\n",
              "       -0.83378255, -0.55600405, -0.09982022, -0.50985042, -0.83994837,\n",
              "       -0.36780444, -0.64577747, -0.20603587, -0.3443035 , -0.46436854,\n",
              "       -0.87636926, -0.98491447, -0.2983588 , -0.70240831, -0.6073161 ,\n",
              "       -0.66639739, -0.        , -0.        , -0.37948339, -0.23467308,\n",
              "       -0.49169168, -0.60721213, -0.61521733, -0.25792916, -0.50436017,\n",
              "       -0.39641599, -0.84554512, -0.96357743, -0.58247333, -0.97141872,\n",
              "       -0.50868668, -0.53447164, -0.69337092, -0.23469518, -0.62994825,\n",
              "       -0.08568491, -0.24542768, -0.20447445, -0.12108686, -0.14770146,\n",
              "       -0.66341001, -0.6729699 , -0.76349779, -0.51623553, -0.        ,\n",
              "       -0.01641897, -0.78097847, -0.46456612, -0.1752207 , -0.07035359,\n",
              "       -0.        ])"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Dw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3ifXK4ykuEc",
        "outputId": "e4dca400-178f-49b8-9c2c-23a3990d6c4d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-0.020351432784903194"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anNCXOLdkuEd",
        "outputId": "752df789-9133-494a-88d0-9a802c64a961"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-0.1060162056883096"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Dbf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary of the CNN code\n",
        "The provided code snippet is a simplified implementation of a Convolutional Neural Network (CNN) for training with backpropagation.\n",
        "\n",
        "It includes functions for forward propagation, backpropagation, and parameter updates.\n",
        "\n",
        "Here’s a detailed explanation of what each part of the code does:\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "#1. Function Definitions\n",
        "##1.1 f_padd(I, p)\n",
        "\n",
        "###Purpose\n",
        "\n",
        "Adds padding to the input image I.\n",
        "\n",
        "##Parameters\n",
        "I: Input image.\n",
        "\n",
        "p: Padding size.\n",
        "\n",
        "##Operation\n",
        "\n",
        "Adds p rows of zeros to the top and bottom.\n",
        "\n",
        "Adds p columns of zeros to the left and right.\n",
        "\n",
        "Returns the padded image.\n",
        "\n",
        "...............................................................................\n",
        "\n",
        "##1.2 f_conv2d(I, K, p)\n",
        "\n",
        "Performs 2D convolution on the input image I with kernel K and padding p.\n",
        "\n",
        "###Parameters\n",
        "I: Input image.\n",
        "\n",
        "K: Convolutional kernel.\n",
        "\n",
        "p: Padding size.\n",
        "\n",
        "###Operation\n",
        "Pads the input image I.\n",
        "\n",
        "Applies the convolution operation using kernel K.\n",
        "\n",
        "Returns the feature map C.\n",
        "\n",
        "------\n",
        "##1.3 f_ReLU(C)\n",
        "\n",
        "###Purpose\n",
        "\n",
        "Applies the ReLU activation function.\n",
        "\n",
        "###Parameters\n",
        "\n",
        "C: Input feature map.\n",
        "\n",
        "###Operation:\n",
        "Sets all negative values in C to zero.\n",
        "\n",
        "Returns the activated feature map.\n",
        "\n",
        "------\n",
        "##1.4 f_pool(C)\n",
        "\n",
        "###Purpose\n",
        "\n",
        "Performs max pooling on the feature map C.\n",
        "\n",
        "###Parameters\n",
        "\n",
        "C: Input feature map.\n",
        "\n",
        "###Operation\n",
        "Applies 2x2 max pooling.\n",
        "\n",
        "Reduces the dimensions of C by half.\n",
        "\n",
        "Returns the pooled feature map S.\n",
        "\n",
        "------\n",
        "##1.5 f_sigmoid(f, w, bf)\n",
        "\n",
        "###Purpose\n",
        "\n",
        "Computes the output of a sigmoid activation function.\n",
        "\n",
        "###Parameters\n",
        "\n",
        "f: Flattened pooled feature map.\n",
        "\n",
        "w: Weights.\n",
        "\n",
        "bf: Bias for the sigmoid function.\n",
        "\n",
        "###Operation\n",
        "Computes the weighted sum x = w.dot(f) + bf.\n",
        "\n",
        "Applies the sigmoid function to x.\n",
        "\n",
        "Returns the sigmoid output y_hat.\n",
        "\n",
        "------\n",
        "#1.6 f_forwardPass(I, K, b, w, bf)\n",
        "\n",
        "###Purpose\n",
        "\n",
        "Performs forward propagation through the network.\n",
        "\n",
        "###Parameters\n",
        "I: Input image.\n",
        "\n",
        "K: Convolutional kernel.\n",
        "\n",
        "b: Bias for the convolution layer.\n",
        "\n",
        "w: Weights for the sigmoid function.\n",
        "\n",
        "bf: Bias for the sigmoid function.\n",
        "\n",
        "####Operation\n",
        "Applies convolution, adds bias, applies ReLU, and performs pooling.\n",
        "\n",
        "Flattens the pooled feature map and computes the sigmoid activation.\n",
        "\n",
        "Returns the intermediate and final outputs.\n",
        "\n",
        "------\n",
        "##1.7 f_getGradient_w(y_hat, y, f)\n",
        "\n",
        "###Purpose\n",
        "\n",
        "Computes the gradient with respect to weights w.\n",
        "\n",
        "###Parameters\n",
        "y_hat: Predicted output.\n",
        "\n",
        "y: Actual target.\n",
        "\n",
        "f: Flattened pooled feature map.\n",
        "\n",
        "###Operation\n",
        "Calculates the gradient using the chain rule.\n",
        "\n",
        "Returns the gradient with respect to w.\n",
        "\n",
        "------\n",
        "##1.8 f_getGradient_f(y_hat, y, w)\n",
        "###Purpose\n",
        "\n",
        "Computes the gradient with respect to the flattened feature map f.\n",
        "\n",
        "###Parameters\n",
        "y_hat: Predicted output.\n",
        "\n",
        "y: Actual target.\n",
        "\n",
        "w: Weights.\n",
        "\n",
        "###Operation\n",
        "Calculates the gradient using the chain rule.\n",
        "Returns the gradient with respect to f.\n",
        "\n",
        "-----\n",
        "##1.9 f_getGradient_bf(y_hat, y)\n",
        "\n",
        "###Purpose\n",
        "\n",
        "Computes the gradient with respect to the bias bf in the sigmoid function.\n",
        "\n",
        "###Parameters\n",
        "y_hat: Predicted output.\n",
        "\n",
        "y: Actual target.\n",
        "\n",
        "####Operation\n",
        "Calculates the gradient with respect to bf.\n",
        "\n",
        "Returns the gradient.\n",
        "\n",
        "-----\n",
        "##1.10 f_getGradient_S(Df)\n",
        "\n",
        "###Purpose\n",
        "\n",
        "Reshapes the gradient vector Df into the shape of the pooled feature map S.\n",
        "\n",
        "###Parameters\n",
        "Df: Gradient vector.\n",
        "\n",
        "###Operation\n",
        "Reshapes Df into a square matrix.\n",
        "\n",
        "Returns the reshaped gradient matrix DS.\n",
        "\n",
        "-----\n",
        "##1.11 f_getGradient_C(DS, C)\n",
        "###Purpose\n",
        "\n",
        "Computes the gradient with respect to the convolutional layer C from the pooled gradients DS.\n",
        "\n",
        "###Parameters\n",
        "DS: Gradient matrix for the pooled feature map.\n",
        "\n",
        "C: Feature map before pooling.\n",
        "\n",
        "###Operation:\n",
        "Maps gradients from DS back to the locations in C that contributed to the pooling.\n",
        "\n",
        "Returns the gradient with respect to C.\n",
        "\n",
        "------\n",
        "##1.12 f_getChainRuleGradients(C, DC, I, u, v)\n",
        "###Purpose\n",
        "\n",
        "Computes the gradient for each kernel weight using the chain rule.\n",
        "\n",
        "###Parameters\n",
        "C: Feature map before pooling.\n",
        "\n",
        "DC: Gradient matrix for the pooled feature map.\n",
        "\n",
        "I: Input image.\n",
        "\n",
        "u, v: Offsets for the kernel position.\n",
        "\n",
        "###Operation\n",
        "Calculates the gradient contribution from each input pixel to each kernel weight.\n",
        "\n",
        "Returns the gradient for a particular kernel weight.\n",
        "\n",
        "------\n",
        "##1.13 f_getGradient_K(C, I, y_hat, y, w)\n",
        "###Purpose\n",
        "\n",
        "Computes the gradient with respect to the convolutional kernel K.\n",
        "\n",
        "###Parameters\n",
        "C: Feature map before pooling.\n",
        "\n",
        "I: Input image.\n",
        "\n",
        "y_hat: Predicted output.\n",
        "\n",
        "y: Actual target.\n",
        "\n",
        "w: Weights.\n",
        "\n",
        "###Operation\n",
        "Calculates gradients for the kernel using the chain rule.\n",
        "Returns the gradient with respect to K and the gradient matrix DC.\n",
        "\n",
        "-----\n",
        "##1.14 f_getGradient_b(C, DC)\n",
        "###Purpose\n",
        "\n",
        "Computes the gradient with respect to the bias b.\n",
        "\n",
        "###Parameters\n",
        "C: Feature map before pooling.\n",
        "\n",
        "DC: Gradient matrix for the convolutional layer.\n",
        "\n",
        "###Operation:\n",
        "Sums up gradients where C is positive.\n",
        "\n",
        "Returns the gradient with respect to b.\n",
        "\n",
        "------\n",
        "##1.15 f_backwardPass(I, C, f, w, y_hat, y)\n",
        "###Purpose\n",
        "\n",
        "Performs backpropagation to compute gradients for all parameters.\n",
        "\n",
        "###Parameters\n",
        "I: Input image.\n",
        "\n",
        "C: Feature map before pooling.\n",
        "\n",
        "f: Flattened pooled feature map.\n",
        "\n",
        "w: Weights.\n",
        "\n",
        "y_hat: Predicted output.\n",
        "\n",
        "y: Actual target.\n",
        "\n",
        "###Operation:\n",
        "Calls gradient functions to compute gradients for weights, biases, and kernel.\n",
        "\n",
        "Returns gradients for K, b, w, and bf.\n",
        "\n",
        "-------\n",
        "##1.16 f_initParams()\n",
        "###Purpose\n",
        "\n",
        "Initializes parameters for the convolutional layer.\n",
        "\n",
        "###Operation\n",
        "Initializes the convolution kernel K, bias b, weights w, and bias for sigmoid bf with small random values.\n",
        "\n",
        "Returns initialized parameters.\n",
        "\n",
        "-----\n",
        "-----\n",
        "#2. Main Code Execution\n",
        "###Initialization\n",
        "\n",
        "An input image I is randomly generated.\n",
        "\n",
        "Parameters K, b, w, and bf are initialized.\n",
        "\n",
        "###Training Loop\n",
        "\n",
        "Forward Pass: Computes the forward pass through the network, producing output y_hat.\n",
        "\n",
        "Backward Pass: Computes gradients for all parameters using the backward pass.\n",
        "\n",
        "Parameter Update: Updates parameters (K, b, w, bf) using gradient descent with a learning rate alpha.\n",
        "\n",
        "-----\n",
        "-----\n",
        "#Summary\n",
        "\n",
        "This program demonstrates a basic implementation of forward and backward propagation in a simple CNN-like structure. It includes padding, convolution, ReLU activation, pooling, and sigmoid activation.\n",
        "\n",
        "The backward propagation functions calculate gradients needed to update the weights and biases in the network, allowing the model to learn from the data over multiple iterations."
      ],
      "metadata": {
        "id": "--7Mt6PEthYm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7zVnJSdkuEe"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}